<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8" />
<title>PRML读书会第六章   Kernel Methods | 我爱自然语言处理</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="stylesheet" type="text/css" media="all" href="http://www.52nlp.cn/wp-content/themes/twentytenorg/style.css" />
<link rel="pingback" href="http://www.52nlp.cn/xmlrpc.php" />
<link rel="alternate" type="application/rss+xml" title="我爱自然语言处理 &raquo; Feed" href="http://www.52nlp.cn/feed" />
<link rel="alternate" type="application/rss+xml" title="我爱自然语言处理 &raquo; 评论Feed" href="http://www.52nlp.cn/comments/feed" />
<link rel="alternate" type="application/rss+xml" title="我爱自然语言处理 &raquo; PRML读书会第六章   Kernel Methods评论Feed" href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ad%e7%ab%a0-kernel-methods/feed" />
<link rel='stylesheet' id='yarppWidgetCss-css'  href='http://www.52nlp.cn/wp-content/plugins/yet-another-related-posts-plugin/style/widget.css?ver=4.0.1' type='text/css' media='all' />
<link rel='stylesheet' id='codecolorer-css'  href='http://www.52nlp.cn/wp-content/plugins/codecolorer/codecolorer.css?ver=0.9.9' type='text/css' media='screen' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://www.52nlp.cn/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.52nlp.cn/wp-includes/wlwmanifest.xml" /> 
<link rel='prev' title='PRML读书会第五章  Neural Networks' href='http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%94%e7%ab%a0-neural-networks' />
<link rel='next' title='PRML读书会第七章 Sparse Kernel Machines' href='http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%83%e7%ab%a0-sparse-kernel-machines' />
<meta name="generator" content="WordPress 4.0.1" />
<link rel='canonical' href='http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ad%e7%ab%a0-kernel-methods' />
<link rel='shortlink' href='http://www.52nlp.cn/?p=7282' />
<!-- wp thread comment 1.4.9.4.002 -->
<style type="text/css" media="screen">
.editComment, .editableComment, .textComment{
	display: inline;
}
.comment-childs{
	border: 1px solid #999;
	margin: 5px 2px 2px 4px;
	padding: 4px 2px 2px 4px;
	background-color: white;
}
.chalt{
	background-color: #E2E2E2;
}
#newcomment{
	border:1px dashed #777;width:90%;
}
#newcommentsubmit{
	color:red;
}
.adminreplycomment{
	border:1px dashed #777;
	width:99%;
	margin:4px;
	padding:4px;
}
.mvccls{
	color: #999;
}
			
</style>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } },
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
});
</script><script type="text/javascript" src="http://www.52nlp.cn/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>

<body class="single single-post postid-7282 single-format-standard">
<div id="wrapper" class="hfeed">
	<div id="header">
		<div id="masthead">
			<div id="branding" role="banner">
								<div id="site-title">
					<span>
						<a href="http://www.52nlp.cn/" title="我爱自然语言处理" rel="home">我爱自然语言处理</a>
					</span>
				</div>
				<div id="site-description">I Love Natural Language Processing</div>

										<img src="http://www.52nlp.cn/wp-content/themes/twentytenorg/images/headers/path.jpg" width="940" height="198" alt="" />
								</div><!-- #branding -->

			<div id="access" role="navigation">
			  				<div class="skip-link screen-reader-text"><a href="#content" title="跳至正文">跳至正文</a></div>
								<div class="menu"><ul><li ><a href="http://www.52nlp.cn/">首页</a></li><li class="page_item page-item-2"><a href="http://www.52nlp.cn/about">关于</a></li><li class="page_item page-item-2557 page_item_has_children"><a href="http://www.52nlp.cn/resources">资源</a><ul class='children'><li class="page_item page-item-1271"><a href="http://www.52nlp.cn/resources/wpmatheditor">WpMathEditor</a></li></ul></li></ul></div>
 
				<div class="menu"><ul><li class="page_item page-item-2"></li><li class="page_item page-item-2"><a href="http://coursegraph.com" title="课程图谱" target="_blank">课程图谱</a></li><li class="page_item page-item-2"><a href="http://www.nlpjob.com" title="求职" target="_blank">求职招聘</a></li></ul></div>
			</div><!-- #access -->
		</div><!-- #masthead -->
	</div><!-- #header -->

	<div id="main">

		<div id="container">
			<div id="content" role="main">

			

				<div id="nav-above" class="navigation">
					<div class="nav-previous"><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%94%e7%ab%a0-neural-networks" rel="prev"><span class="meta-nav">&larr;</span> PRML读书会第五章  Neural Networks</a></div>
					<div class="nav-next"><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%83%e7%ab%a0-sparse-kernel-machines" rel="next">PRML读书会第七章 Sparse Kernel Machines <span class="meta-nav">&rarr;</span></a></div>
				</div><!-- #nav-above -->

				<div id="post-7282" class="post-7282 post type-post status-publish format-standard hentry category-pattern-recognition-and-machine-learning-2 category-344 tag-dual tag-gaussian-processes tag-kernel tag-kernel-methods tag-knn tag-mercer tag-pattern-recognition-and-machine-learning tag-prml tag-svm tag-991 tag-1031 tag-344 tag-961 tag-960 tag-1026 tag-1025 tag-995 tag-1037 tag-1035 tag-1034">
					<h1 class="entry-title">PRML读书会第六章   Kernel Methods</h1>

					<div class="entry-meta">
						<span class="meta-prep meta-prep-author">发表于</span> <a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ad%e7%ab%a0-kernel-methods" title="18:04" rel="bookmark"><span class="entry-date">2015年01月28号</span></a> <span class="meta-sep">由</span> <span class="author vcard"><a class="url fn n" href="http://www.52nlp.cn/author/prml" title="查看所有由 prml 发布的文章">prml</a></span>					</div><!-- .entry-meta -->

					<div class="entry-content">
						<p style="text-align: center"><strong>PRML</strong><strong>读书会第六章 </strong> <strong>Kernel Methods</strong></p>
<p style="text-align: center"><strong>主讲人 网络上的尼采</strong></p>
<p style="text-align: center"><strong>（新浪微博:<a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a>）</strong></p>
<p>网络上的尼采(813394698) 9:16:05</p>
<p>今天的主要内容：Kernel的基本知识，高斯过程。边思考边打字，有点慢，各位稍安勿躁。<br />
机器学习里面对待训练数据有的是训练完得到参数后就可以抛弃了，比如神经网络；有的是还需要原来的训练数据比如KNN，SVM也需要保留一部分数据&#8211;支持向量。<br />
很多线性参数模型都可以通过dual representation的形式表达为核函数的形式。所谓线性参数模型是通过非线性的基函数的线性组合来表达非线性的东西，模型还是线性的。比如线性回归模型是y=<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-0.png"><img class="alignnone wp-image-7284" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-0.png" alt="prml6-0" width="66" height="20" /></a>，<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-1.png"><img class="alignnone wp-image-7285" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-1.png" alt="prml6-1" width="45" height="26" /></a>是一组非线性基函数，我们可以通过线性的模型来表达非线性的结构。</p>
<p>核函数的形式：<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-3.png"><img class="alignnone wp-image-7286" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-3.png" alt="prml6-3" width="150" height="26" /></a>，也就是映射后高维特征空间的内积可以通过原来低维的特征得到。因此kernel methods用途广泛。</p>
<p>核函数有很多种，有平移不变的stationary kernels  <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-4.png"><img class="alignnone wp-image-7287" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-4.png" alt="prml6-4" width="160" height="20" /></a>还有仅依赖欧氏距离的径向基核：<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-5.png"><img class="alignnone wp-image-7288" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-5.png" alt="prml6-5" width="160" height="23" /></a><span id="more-7282"></span></p>
<p>非线性转化为线性的形式的好处不言而喻，各种变换推导、闭式解就出来了。 下面推导下线性回归模型的dual representation，有助于我们理解核函数的作用：</p>
<p>根据最小二乘，我们得到下面的目标函数<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-7.png"><img class="alignnone wp-image-7289" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-7.png" alt="prml6-7" width="250" height="43" /></a>，加了L2正则。我们对w求导，令J(w)的梯度等于0，得到以下解：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-8.png"><img class="aligncenter wp-image-7290" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-8.png" alt="prml6-8" width="400" height="63" /></a></p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-9.png"><img class="alignnone wp-image-7291" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-9.png" alt="prml6-9" width="25" height="22" /></a>是个由基函数构成的样本矩阵，<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-10.png"><img class="alignnone wp-image-7292" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-10.png" alt="prml6-10" width="25" height="29" /></a>向量里面的元素由<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-11.png"><img class="alignnone wp-image-7293" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-11.png" alt="prml6-11" width="200" height="40" /></a>组成：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-12.png"><img class="aligncenter wp-image-7294" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-12.png" alt="prml6-12" width="400" height="69" /></a></p>
<p>我们把<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-13.png"><img class="alignnone wp-image-7295" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-13.png" alt="prml6-13" width="60" height="19" /></a>代入最初的J(w)得到：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-14.png"><img class="aligncenter wp-image-7296" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-14.png" alt="prml6-14" width="400" height="47" /></a><br />
咱们用核矩阵K来替换<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-16.png"><img class="alignnone wp-image-7297" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-16.png" alt="prml6-16" width="35" height="21" /></a>，其中矩阵K里面的元素是<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-17.png"><img class="alignnone wp-image-7298" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-17.png" alt="prml6-17" width="200" height="28" /></a><br />
于是得到<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-20.png"><img class="alignnone wp-image-7301" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-20.png" alt="prml6-20" width="250" height="35" /></a><br />
然后<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-21.png"><img class="alignnone wp-image-7302" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-21.png" alt="prml6-21" width="30" height="17" /></a>对<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-10.png"><img class="alignnone wp-image-7292" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-10.png" alt="prml6-10" width="20" height="23" /></a>求导，令其梯度等于0，得到解<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-23.png"><img class="alignnone wp-image-7303" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-23.png" alt="prml6-23" width="120" height="28" /></a><br />
所以原来的线性回归方程就变成了<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-24.png"><img class="alignnone wp-image-7304" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-24.png" alt="prml6-24" width="350" height="30" /></a><br />
K(X)的含义：<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-25.png"><img class="alignnone wp-image-7305" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-25.png" alt="prml6-25" width="380" height="17" /></a>，上面的DUAL形式的含义非常明显，就是根据已知的的训练数据来做预测。至此原来线性回归方程的参数w消失，由核函数来表示回归方程，以上方式把基于特征的学习转换成了基于样本的学习。 这是线性回归的DUAL表示，svm等很多模型都有DUAL表示。<br />
80(850639048) 10:09:50<br />
professor 核函数其实是为了求基函数的内积对吗？<br />
网络上的尼采(813394698) 10:12:57</p>
<p>如果有很多基的话维度势必会很高，计算内积的花销会很大，有些是无限维的，核函数能绕过高维的内积计算，直接用核函数得到内积。</p>
<p>接下来看下核函数的性质及构造方法。核函数的一般形式：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-26.png"><img class="aligncenter wp-image-7306" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-26.png" alt="prml6-26" width="400" height="89" /></a></p>
<p>下面是个简单的例子说明<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-27.png"><img class="alignnone wp-image-7307" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-27.png" alt="prml6-27" width="120" height="31" /></a>为什么是个核函数：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-29.png"><img class="aligncenter wp-image-7308" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-29.png" alt="prml6-29" width="400" height="127" /></a><br />
很明显 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-30.png"><img class="alignnone wp-image-7309" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-30.png" alt="prml6-30" width="100" height="26" /></a> 是个核函数，它能写成核函数的一般形式。</p>
<p>核函数的一个充分必要定理也就是mercer定理：核矩阵是半正定的：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-31.png"><img class="aligncenter wp-image-7310" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-31.png" alt="prml6-31" width="400" height="94" /></a><br />
我们可以通过以下规则用简单的核函数来构造复杂的核函数：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-32.png"><img class="aligncenter wp-image-7311" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-32.png" alt="prml6-32" width="400" height="311" /></a><br />
过会我们讲高斯过程时再举个核函数线性组合的例子。<br />
介绍一个经常用到的径向基核函数，高斯核：<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-36.png"><img class="alignnone wp-image-7313" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-36.png" alt="prml6-36" width="257" height="41" /></a>，这个核函数能把数据映射到无限维的空间：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-37.png"><img class="aligncenter wp-image-7314" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-37.png" alt="prml6-37" width="400" height="96" /></a></p>
<p>中间<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-38.png"><img class="alignnone wp-image-7315" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-38.png" alt="prml6-38" width="100" height="31" /></a>可以展开成无限维的，然后核函数可以表示成内积的形式。<br />
内积的含义就是表示相似性，所以核函数还有其他的用法。比如我们可以通过生成模型来构造核。</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-39.png"><img class="aligncenter wp-image-7316" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-39.png" alt="prml6-39" width="349" height="52" /></a></p>
<p>两个变量的概率都很高相似性就越大，其实这样做就是映射到一维的内积。<br />
我们可以引入离散的隐变量：<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-40.png"><img class="alignnone wp-image-7317" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-40.png" alt="prml6-40" width="243" height="51" /></a><br />
连续的隐变量：<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-41.png"><img class="alignnone wp-image-7318" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-41.png" alt="prml6-41" width="274" height="49" /></a><br />
举个这样做有啥用的例子，我们可以用来比较HMM由同一条隐马尔科夫链生成的两条序列的相似性：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-42.png"><img class="aligncenter wp-image-7319" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-42.png" alt="prml6-42" width="400" height="150" /></a></p>
<p>网络上的尼采(813394698) 10:40:34<br />
接下来讲我们今天的重点Gaussian Processes<br />
牧云(1106207961) 10:41:02</p>
<p>数据海洋(1009129701) 10:41:14<br />
我先再理解，理解这些。<br />
网络上的尼采(813394698) 10:42:41<br />
Gaussian Processes是贝叶斯学派的一个大杀器，用处很广。不同于参数模型，Gaussian Processes认为函数在函数空间里存在一个先验分布。</p>
<p>高斯过程和很多模型是等价的：ARMA (autoregressive moving average) models, Kalman filters, radial basis function networks ，还有特定情况下的神经网络。<br />
现在我们从贝叶斯线性回归自然的引出高斯过程：<br />
前面我们提到的线性回归的形式 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-43.png"><img class="alignnone wp-image-7321" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-43.png" alt="prml6-43" width="100" height="25" /></a><br />
贝叶斯方法为参数加了一个高斯分布 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-44.png"><img class="alignnone wp-image-7322" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-44.png" alt="prml6-44" width="150" height="31" /></a><br />
大家发现了没有，这样做直接导致了函数有个预测分布，并且也是高斯的，因为方程是线性的并且参数是高斯分布。线性的东西和高斯分布总是不分家的。<br />
我们定义向量：y，</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-45.png"><img class="aligncenter wp-image-7323" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-45.png" alt="prml6-45" width="400" height="35" /></a></p>
<p>y就是个多元的高斯分布。<br />
它的每一维 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-46.png"><img class="alignnone wp-image-7324" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-46.png" alt="prml6-46" width="45" height="22" /></a>都是个高斯分布，这也是高斯过程的由来。<br />
y可以表示为 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-47.png"><img class="alignnone wp-image-7325" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-47.png" alt="prml6-47" width="67" height="19" /></a></p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-48.png"><img class="alignnone wp-image-7326" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-48.png" alt="prml6-48" width="300" height="26" /></a></p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-49.png"><img class="alignnone wp-image-7327" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-49.png" alt="prml6-49" width="23" height="18" /></a>是基函数组成的样本矩阵。<br />
高斯过程可以由均值和协方差矩阵完全决定。由于w的均值是0，所以我们也认为高斯过程的均值是0，<br />
剩下的就是根据定义求它的协方差矩阵，很自然地就得出来了：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-50.png"><img class="aligncenter wp-image-7328" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-50.png" alt="prml6-50" width="400" height="85" /></a><br />
矩阵K里的元素<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-51.png"><img class="alignnone wp-image-7329" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-51.png" alt="prml6-51" width="277" height="44" /></a>都是核函数的形式。<br />
选用什么样的核函数也是问题，下面的图是对采用高斯核和指数核的高斯过程的取样，一共取了5条，可以看到两者的区别：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-52.png"><img class="aligncenter wp-image-7330 size-full" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-52.png" alt="prml6-52" width="441" height="129" /></a><br />
接下来我们就用GP来做回归 ：<br />
我们观测的目标值是包含噪音的，噪音是高斯分布。</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-60.png"><img class="aligncenter wp-image-7332" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-60.png" alt="prml6-60" width="186" height="54" /></a></p>
<p>那么根据线性高斯模型的性质，<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-65.png"><img class="alignnone wp-image-7334" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-65.png" alt="prml6-65" width="180" height="31" /></a>，其中<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-66.png"><img class="alignnone wp-image-7335" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-66.png" alt="prml6-66" width="26" height="29" /></a>是噪音的参数<br />
对于向量<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-63.png"><img class="alignnone wp-image-7336" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-63.png" alt="prml6-63" width="120" height="23" /></a>和向量<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-64.png"><img class="alignnone wp-image-7337" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-64.png" alt="prml6-64" width="120" height="23" /></a></p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-68.png"><img class="alignnone wp-image-7338" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-68.png" alt="prml6-68" width="150" height="30" /></a></p>
<p>咱们前面说过了，<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-61.png"><img class="alignnone wp-image-7333" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-61.png" alt="prml6-61" width="30" height="25" /></a>可以表示为<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-62.png"><img class="alignnone wp-image-7339" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-62.png" alt="prml6-62" width="120" height="29" /></a><br />
所以marginal distribution：<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-69.png"><img class="alignnone wp-image-7340" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-69.png" alt="prml6-69" width="266" height="40" /></a><br />
其中矩阵C的元素<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-70.png"><img class="alignnone wp-image-7341" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-70.png" alt="prml6-70" width="255" height="38" /></a>，<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-72.png"><img class="alignnone wp-image-7342" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-72.png" alt="prml6-72" width="35" height="25" /></a>是单位矩阵的元素，其实就是把<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-73.png"><img class="alignnone wp-image-7343" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-73.png" alt="prml6-73" width="38" height="29" /></a>加在了矩阵K的对角线上。这个不难理解，一开始<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-75.png"><img class="alignnone wp-image-7344" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-75.png" alt="prml7-75" width="100" height="25" /></a>，都是高斯的，协方差是两者的相加，噪音每次取都是独立的，所以只在协方差矩阵对角线上有。<br />
现在确定下用什么核的问题，举个例子，下面这个核函数用了高斯核，线性核，以及常数的线性组合，这样做是为了更灵活，过会再讲如何确定里面的这些超参：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-77.png"><img class="aligncenter wp-image-7345" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-77.png" alt="prml6-77" width="400" height="63" /></a></p>
<p>下图是不同的超参对高斯过程的影响：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-78.png"><img class="aligncenter wp-image-7346" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-78.png" alt="prml6-78" width="400" height="292" /></a><br />
解决了核函数的问题，我们再回来，通过前面的结论，不难得出 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-79.png"><img class="alignnone wp-image-7347" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-79.png" alt="prml6-79" width="180" height="31" /></a><br />
如何确定矩阵<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-80.png"><img class="alignnone wp-image-7348" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-80.png" alt="prml6-80" width="43" height="23" /></a>呢，其实我们在原来矩阵<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-81.png"><img class="alignnone wp-image-7349" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-81.png" alt="prml6-81" width="31" height="19" /></a>的基础上补上就行。</p>
<p>k和c比较容易理解：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-82.png"><img class="aligncenter wp-image-7350" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-82.png" alt="prml6-82" width="400" height="21" /></a> <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-83.png"><img class="aligncenter wp-image-7351" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-83.png" alt="prml6-83" width="300" height="43" /></a></p>
<p>咱们的最终目标就是得<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-85.png"><img class="alignnone wp-image-7352" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-85.png" alt="prml6-85" width="68" height="26" /></a>，由于这两个都是高斯分布，用第二章条件高斯分布的公式套一下，其中均值是0：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-89.png"><img class="aligncenter wp-image-7353" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-89.png" alt="prml6-89" width="400" height="85" /></a></p>
<p>就会得到<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-85.png"><img class="alignnone wp-image-7352" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-85.png" alt="prml6-85" width="68" height="26" /></a>的均值和方差：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-86.png"><img class="aligncenter wp-image-7354" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-86.png" alt="prml6-86" width="400" height="77" /></a></p>
<p>可以看出均值和方差都是<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-90.png"><img class="alignnone wp-image-7356" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-90.png" alt="prml6-90" width="46" height="23" /></a>的函数，我们做预测时用均值就行了。<br />
最后一个问题就是高斯过程是由它的协方差矩阵完全决定的，我们如何学习矩阵里面的超参呢？包括我们刚才提到的核函数里面的参数以及噪音的参数。<br />
其实由于高斯分布的原因，我们可以方便的利用log最大似然：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-91.png"><img class="aligncenter wp-image-7357" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-91.png" alt="prml6-91" width="400" height="52" /></a><br />
求最优解时可以用共轭梯度等方法，梯度：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-92.png"><img class="aligncenter wp-image-7358" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-92.png" alt="prml6-92" width="400" height="55" /></a><br />
到这里，用高斯过程做回归就结束了。<br />
有了做回归的基础，咱们再看下如何做分类。<br />
类似逻辑回归，加个sigmoid函数就能做分类了：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-93.png"><img class="aligncenter wp-image-7359" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-93.png" alt="prml6-93" width="400" height="84" /></a><br />
分类与回归不同的是<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-94.png"><img class="alignnone wp-image-7360" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-94.png" alt="prml6-94" width="247" height="47" /></a>是个伯努利分布。<br />
<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-95.png"><img class="alignnone wp-image-7361" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-95.png" alt="prml6-95" width="258" height="47" /></a>这里还和前面一样。</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-96.png"><img class="alignnone wp-image-7362" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-96.png" alt="prml6-96" width="265" height="40" /></a></p>
<p>对于二分类问题，最后我们要得到是：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-97.png"><img class="aligncenter wp-image-7363" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-97.png" alt="prml6-97" width="400" height="45" /></a><br />
但是这个积分是不容易求的，<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-98.png"><img class="alignnone size-full wp-image-7364" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-98.png" alt="prml6-98" width="15" height="14" /></a>是伯努利分布，<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-99.png"><img class="alignnone size-full wp-image-7365" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-99.png" alt="prml6-99" width="31" height="19" /></a>是高斯分布，不是共轭的。求积分的方法有很多，可以用MCMC，也可以用变分的方法。书上用的是Laplace approximation。</p>
<p>今天就到这里吧，我去吃饭，各位先讨论下吧。</p>
<p>上面Gaussian Processes的公式推导虽然有点多，但都是高斯分布所以并不复杂，并且GP在算法实现上也不难。</p>
<p>另外给大家推荐一个机器学习视频的网站，http://blog.videolectures.net/100-most-popular-machine-learning-talks-at-videolectures-net/ 里面有很多牛人比如Jordan的talks，第一个视频就是剑桥的David MacKay讲高斯过程，他的一本书 Information Theory, Inference and Learning Algorithms也很出名。</p>
<p>两栖动物(9219642) 14:35:09<br />
<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-49.png"><img class="alignnone wp-image-7327" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-49.png" alt="prml6-49" width="23" height="18" /></a>是个由基函数构成的矩阵，向量a里面的元素由<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-100.png"><img class="alignnone wp-image-7366" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-100.png" alt="prml6-100" width="200" height="40" /></a>组成。<br />
<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-9.png"><img class="alignnone wp-image-7291" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-9.png" alt="prml6-9" width="25" height="22" /></a>的维度是基函数的个数，an的维度是样本的个数把?<br />
网络上的尼采(813394698) 14:36:44<br />
对<br />
两栖动物(9219642) 14:37:48</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-101.png"><img class="alignnone wp-image-7367" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-101.png" alt="prml6-101" width="381" height="60" /></a></p>
<p>哪这2个怎么后来乘在一起了？维度不是不一样吗？</p>
<p>网络上的尼采(813394698) 14:49:01<br />
@两栖动物 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-9.png"><img class="alignnone wp-image-7291" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-9.png" alt="prml6-9" width="25" height="22" /></a>不是方阵，可以相乘</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-102.png"><img class="alignnone wp-image-7368" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-102.png" alt="prml6-102" width="354" height="29" /></a><br />
明白了吧，另外这个由基函数表示的样本矩阵只在推导里存在。<br />
两栖动物(9219642) 14:56:08<br />
明白了，谢谢</p>
<p>&nbsp;</p>
<p>注：PRML读书会系列文章由 <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a> 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。</p>
<p>PRML读书会讲稿PDF版本以及更多资源下载地址：<a href="http://vdisk.weibo.com/u/1841149974">http://vdisk.weibo.com/u/1841149974</a></p>
<p>本文链接地址：<a href="http://www.52nlp.cn/prml读书会第六章-kernel-methods">http://www.52nlp.cn/prml读书会第六章-kernel-methods</a></p>
<p>&nbsp;</p>
<div class='yarpp-related'>
<p>相关文章:<ol>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%80%e7%ab%a0-sampling-methods" rel="bookmark" title="PRML读书会第十一章  Sampling Methods">PRML读书会第十一章  Sampling Methods </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%83%e7%ab%a0-sparse-kernel-machines" rel="bookmark" title="PRML读书会第七章 Sparse Kernel Machines">PRML读书会第七章 Sparse Kernel Machines </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e5%89%8d%e8%a8%80" rel="bookmark" title="PRML读书会前言">PRML读书会前言 </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%89%e7%ab%a0-linear-models-for-regression" rel="bookmark" title="PRML读书会第三章 Linear Models for Regression">PRML读书会第三章 Linear Models for Regression </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%8c%e7%ab%a0-probability-distributions" rel="bookmark" title="PRML读书会第二章  Probability Distributions">PRML读书会第二章  Probability Distributions </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b9%9d%e7%ab%a0-mixture-models-and-em" rel="bookmark" title="PRML读书会第九章  Mixture Models and EM">PRML读书会第九章  Mixture Models and EM </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%9b%9b%e7%ab%a0-linear-models-for-classification" rel="bookmark" title="PRML读书会第四章 Linear Models for Classification">PRML读书会第四章 Linear Models for Classification </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%80%e7%ab%a0-introduction" rel="bookmark" title="PRML读书会第一章  Introduction">PRML读书会第一章  Introduction </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%94%e7%ab%a0-neural-networks" rel="bookmark" title="PRML读书会第五章  Neural Networks">PRML读书会第五章  Neural Networks </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ab%e7%ab%a0-graphical-models" rel="bookmark" title="PRML读书会第八章  Graphical Models">PRML读书会第八章  Graphical Models </a></li>
</ol></p>
</div>
											</div><!-- .entry-content -->


					<div class="entry-utility">
						此条目发表在 <a href="http://www.52nlp.cn/category/pattern-recognition-and-machine-learning-2" rel="category tag">PRML</a>, <a href="http://www.52nlp.cn/category/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" rel="category tag">机器学习</a> 分类目录，贴了 <a href="http://www.52nlp.cn/tag/dual" rel="tag">DUAL</a>, <a href="http://www.52nlp.cn/tag/gaussian-processes" rel="tag">Gaussian Processes</a>, <a href="http://www.52nlp.cn/tag/kernel" rel="tag">kernel</a>, <a href="http://www.52nlp.cn/tag/kernel-methods" rel="tag">kernel methods</a>, <a href="http://www.52nlp.cn/tag/knn" rel="tag">knn</a>, <a href="http://www.52nlp.cn/tag/mercer%e5%ae%9a%e7%90%86" rel="tag">mercer定理</a>, <a href="http://www.52nlp.cn/tag/pattern-recognition-and-machine-learning" rel="tag">Pattern Recognition And Machine Learning</a>, <a href="http://www.52nlp.cn/tag/prml" rel="tag">PRML</a>, <a href="http://www.52nlp.cn/tag/prml%e8%af%bb%e4%b9%a6%e4%bc%9a" rel="tag">PRML读书会</a>, <a href="http://www.52nlp.cn/tag/svm" rel="tag">svm</a>, <a href="http://www.52nlp.cn/tag/%e5%9f%ba%e5%87%bd%e6%95%b0" rel="tag">基函数</a>, <a href="http://www.52nlp.cn/tag/%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f%e6%9c%ba" rel="tag">支持向量机</a>, <a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" rel="tag">机器学习</a>, <a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b9%a6%e7%b1%8d" rel="tag">机器学习书籍</a>, <a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e8%af%bb%e4%b9%a6%e4%bc%9a" rel="tag">机器学习读书会</a>, <a href="http://www.52nlp.cn/tag/%e6%a0%b8%e5%87%bd%e6%95%b0" rel="tag">核函数</a>, <a href="http://www.52nlp.cn/tag/%e6%a0%b8%e6%96%b9%e6%b3%95" rel="tag">核方法</a>, <a href="http://www.52nlp.cn/tag/%e8%b4%9d%e5%8f%b6%e6%96%af" rel="tag">贝叶斯</a>, <a href="http://www.52nlp.cn/tag/%e8%b4%9d%e5%8f%b6%e6%96%af%e5%ad%a6%e6%b4%be" rel="tag">贝叶斯学派</a>, <a href="http://www.52nlp.cn/tag/%e9%ab%98%e6%96%af%e6%a0%b8" rel="tag">高斯核</a>, <a href="http://www.52nlp.cn/tag/%e9%ab%98%e6%96%af%e8%bf%87%e7%a8%8b" rel="tag">高斯过程</a> 标签。将<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ad%e7%ab%a0-kernel-methods" title="链向 PRML读书会第六章   Kernel Methods 的固定链接" rel="bookmark">固定链接</a>加入收藏夹。											</div><!-- .entry-utility -->
				</div><!-- #post-## -->

				<div id="nav-below" class="navigation">
					<div class="nav-previous"><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%94%e7%ab%a0-neural-networks" rel="prev"><span class="meta-nav">&larr;</span> PRML读书会第五章  Neural Networks</a></div>
					<div class="nav-next"><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%83%e7%ab%a0-sparse-kernel-machines" rel="next">PRML读书会第七章 Sparse Kernel Machines <span class="meta-nav">&rarr;</span></a></div>
				</div><!-- #nav-below -->

				
			<div id="comments">




								<div id="respond" class="comment-respond">
				<h3 id="reply-title" class="comment-reply-title">发表评论 <small><a rel="nofollow" id="cancel-comment-reply-link" href="/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ad%e7%ab%a0-kernel-methods#respond" style="display:none;">取消回复</a></small></h3>
									<form action="http://www.52nlp.cn/wp-comments-post.php" method="post" id="commentform" class="comment-form">
																			<p class="comment-notes">电子邮件地址不会被公开。 必填项已用<span class="required">*</span>标注</p>							<p class="comment-form-author"><label for="author">姓名 <span class="required">*</span></label> <input id="author" name="author" type="text" value="" size="30" aria-required='true' /></p>
<p class="comment-form-email"><label for="email">电子邮件 <span class="required">*</span></label> <input id="email" name="email" type="text" value="" size="30" aria-required='true' /></p>
<p class="comment-form-url"><label for="url">站点</label> <input id="url" name="url" type="text" value="" size="30" /></p>
												<p class="comment-form-comment"><label for="comment">评论</label> <textarea id="comment" name="comment" cols="45" rows="8" aria-required="true"></textarea></p>						<p class="form-allowed-tags">您可以使用这些<abbr title="HyperText Markup Language">HTML</abbr>标签和属性： <code>&lt;a href=&quot;&quot; title=&quot;&quot;&gt; &lt;abbr title=&quot;&quot;&gt; &lt;acronym title=&quot;&quot;&gt; &lt;b&gt; &lt;blockquote cite=&quot;&quot;&gt; &lt;cite&gt; &lt;code&gt; &lt;del datetime=&quot;&quot;&gt; &lt;em&gt; &lt;i&gt; &lt;q cite=&quot;&quot;&gt; &lt;strike&gt; &lt;strong&gt; </code></p>						<p class="form-submit">
							<input name="submit" type="submit" id="submit" value="发表评论" />
							<input type='hidden' name='comment_post_ID' value='7282' id='comment_post_ID' />
<input type='hidden' name='comment_parent' id='comment_parent' value='0' />
						</p>
						<p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="e0f49aba1f" /></p><p style="display: none;"><input type="hidden" id="ak_js" name="ak_js" value="124"/></p><p><input type="hidden" id="comment_reply_ID" name="comment_reply_ID" value="0" /><input type="hidden" id="comment_reply_dp" name="comment_reply_dp" value="0" /></p><div id="cancel_reply" style="display:none;"><a href="javascript:void(0)" onclick="movecfm(null,0,1,null);" style="color:red;">点击取消回复</a></div><script type="text/javascript">
/* <![CDATA[ */
var commentformid = "commentform";
var USERINFO = false;
var atreply = "none";
/* ]]> */
</script>
<script type="text/javascript" src="http://www.52nlp.cn/wp-content/plugins/wp-thread-comment/wp-thread-comment.js.php?jsver=common"></script>
					</form>
							</div><!-- #respond -->
			
</div><!-- #comments -->


			</div><!-- #content -->
		</div><!-- #container -->

﻿
		<div id="primary" class="widget-area" role="complementary">
			<ul class="xoxo">
<!-- begin l_sidebar -->
	<div id="l_sidebar">
<p>卓越网：<a href="http://www.amazon.cn/mn/searchApp?source=garypyang-23&searchType=1&keywords=自然语言处理" title="自然语言处理书籍"target=_blank>自然语言处理书籍</a><br>
<li id="search-3" class="widget-container widget_search"><h3 class="widget-title">站内搜索</h3><form role="search" method="get" id="searchform" class="searchform" action="http://www.52nlp.cn/">
				<div>
					<label class="screen-reader-text" for="s">搜索：</label>
					<input type="text" value="" name="s" id="s" />
					<input type="submit" id="searchsubmit" value="搜索" />
				</div>
			</form></li><li id="text-4" class="widget-container widget_text"><h3 class="widget-title">NLPJob新鲜职位推荐:</h3>			<div class="textwidget"><p></p>
<script src="http://www.nlpjob.com/api/api.php?action=getJobs
&type=0&category=0&count=8&random=1&days_behind=7&response=js" type="text/javascript"></script>

<script type="text/javascript">showJobs('jobber-container', 'jobber-list');</script></div>
		</li><li id="text-3" class="widget-container widget_text"><h3 class="widget-title">52nlp新浪微博</h3>			<div class="textwidget"><p><iframe id="sina_widget_2104931705" style="width:100%; height:500px;" frameborder="0" scrolling="no" src="http://v.t.sina.com.cn/widget/widget_blog.php?uid=2104931705&height=500&skin=wd_01&showpic=1"></iframe></p>
<p><!-- JiaThis Button BEGIN --><br />
<script type="text/javascript" src="http://v3.jiathis.com/code/jiathis_r.js?uid=1340292124103344&move=0&amp;btn=r3.gif" charset="utf-8"></script><br />
<!-- JiaThis Button END --></p>
</div>
		</li><li id="categories-309398091" class="widget-container widget_categories"><h3 class="widget-title">分类目录</h3>		<ul>
	<li class="cat-item cat-item-72"><a href="http://www.52nlp.cn/category/mit-nlp" title="麻省理工学院开放式课程&quot;自然语言处理“的相关翻译文章">MIT自然语言处理</a> (23)
</li>
	<li class="cat-item cat-item-976"><a href="http://www.52nlp.cn/category/pattern-recognition-and-machine-learning-2" >PRML</a> (15)
</li>
	<li class="cat-item cat-item-469"><a href="http://www.52nlp.cn/category/topic-model" >Topic Model</a> (10)
</li>
	<li class="cat-item cat-item-87"><a href="http://www.52nlp.cn/category/wordpress" >wordpress</a> (6)
</li>
	<li class="cat-item cat-item-317"><a href="http://www.52nlp.cn/category/%e4%b8%93%e9%a2%98" >专题</a> (6)
</li>
	<li class="cat-item cat-item-263"><a href="http://www.52nlp.cn/category/chinese-information-processing" >中文信息处理</a> (20)
</li>
	<li class="cat-item cat-item-62"><a href="http://www.52nlp.cn/category/word-segmentation" >中文分词</a> (36)
</li>
	<li class="cat-item cat-item-420"><a href="http://www.52nlp.cn/category/%e5%b9%b6%e8%a1%8c%e7%ae%97%e6%b3%95" >并行算法</a> (1)
</li>
	<li class="cat-item cat-item-268"><a href="http://www.52nlp.cn/category/%e6%8b%9b%e8%81%98" >招聘</a> (4)
</li>
	<li class="cat-item cat-item-560"><a href="http://www.52nlp.cn/category/%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f" >推荐系统</a> (3)
</li>
	<li class="cat-item cat-item-354"><a href="http://www.52nlp.cn/category/%e6%95%b0%e6%8d%ae%e6%8c%96%e6%8e%98" >数据挖掘</a> (2)
</li>
	<li class="cat-item cat-item-241"><a href="http://www.52nlp.cn/category/text-classification" >文本分类</a> (3)
</li>
	<li class="cat-item cat-item-193"><a href="http://www.52nlp.cn/category/maximum-entropy-model" >最大熵模型</a> (7)
</li>
	<li class="cat-item cat-item-344"><a href="http://www.52nlp.cn/category/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" >机器学习</a> (29)
</li>
	<li class="cat-item cat-item-1"><a href="http://www.52nlp.cn/category/machine-translation" >机器翻译</a> (54)
</li>
	<li class="cat-item cat-item-195"><a href="http://www.52nlp.cn/category/%e6%9d%a1%e4%bb%b6%e9%9a%8f%e6%9c%ba%e5%9c%ba" >条件随机场</a> (3)
</li>
	<li class="cat-item cat-item-153"><a href="http://www.52nlp.cn/category/tagging" >标注</a> (13)
</li>
	<li class="cat-item cat-item-885"><a href="http://www.52nlp.cn/category/%e7%a7%91%e5%ad%a6%e8%ae%a1%e7%ae%97" >科学计算</a> (1)
</li>
	<li class="cat-item cat-item-538"><a href="http://www.52nlp.cn/category/%e7%bb%9f%e8%ae%a1%e5%ad%a6" >统计学</a> (10)
</li>
	<li class="cat-item cat-item-126"><a href="http://www.52nlp.cn/category/translation-model" >翻译模型</a> (2)
</li>
	<li class="cat-item cat-item-51"><a href="http://www.52nlp.cn/category/nlp" >自然语言处理</a> (227)
</li>
	<li class="cat-item cat-item-106"><a href="http://www.52nlp.cn/category/computational-linguistics" >计算语言学</a> (39)
</li>
	<li class="cat-item cat-item-22"><a href="http://www.52nlp.cn/category/dictionary" >词典</a> (8)
</li>
	<li class="cat-item cat-item-221"><a href="http://www.52nlp.cn/category/semantics" >语义学</a> (1)
</li>
	<li class="cat-item cat-item-161"><a href="http://www.52nlp.cn/category/semantic-web" >语义网</a> (3)
</li>
	<li class="cat-item cat-item-37"><a href="http://www.52nlp.cn/category/corpus" >语料库</a> (12)
</li>
	<li class="cat-item cat-item-86"><a href="http://www.52nlp.cn/category/language-model" >语言模型</a> (23)
</li>
	<li class="cat-item cat-item-156"><a href="http://www.52nlp.cn/category/speech-recognition" >语音识别</a> (4)
</li>
	<li class="cat-item cat-item-314"><a href="http://www.52nlp.cn/category/%e8%b4%9d%e5%8f%b6%e6%96%af%e6%a8%a1%e5%9e%8b" >贝叶斯模型</a> (1)
</li>
	<li class="cat-item cat-item-110"><a href="http://www.52nlp.cn/category/reprint" >转载</a> (28)
</li>
	<li class="cat-item cat-item-451"><a href="http://www.52nlp.cn/category/%e9%97%ae%e7%ad%94%e7%b3%bb%e7%bb%9f" >问答系统</a> (1)
</li>
	<li class="cat-item cat-item-3"><a href="http://www.52nlp.cn/category/informal-essay" >随笔</a> (63)
</li>
	<li class="cat-item cat-item-60"><a href="http://www.52nlp.cn/category/hidden-markov-model" >隐马尔科夫模型</a> (36)
</li>
		</ul>
</li><li id="archives-2" class="widget-container widget_archive"><h3 class="widget-title">文章归档</h3>		<ul>
	<li><a href='http://www.52nlp.cn/2015/01'>2015年一月</a></li>
	<li><a href='http://www.52nlp.cn/2014/12'>2014年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2014/11'>2014年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2014/09'>2014年九月</a></li>
	<li><a href='http://www.52nlp.cn/2014/07'>2014年七月</a></li>
	<li><a href='http://www.52nlp.cn/2014/06'>2014年六月</a></li>
	<li><a href='http://www.52nlp.cn/2014/05'>2014年五月</a></li>
	<li><a href='http://www.52nlp.cn/2014/04'>2014年四月</a></li>
	<li><a href='http://www.52nlp.cn/2014/01'>2014年一月</a></li>
	<li><a href='http://www.52nlp.cn/2013/12'>2013年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2013/06'>2013年六月</a></li>
	<li><a href='http://www.52nlp.cn/2013/05'>2013年五月</a></li>
	<li><a href='http://www.52nlp.cn/2013/04'>2013年四月</a></li>
	<li><a href='http://www.52nlp.cn/2013/03'>2013年三月</a></li>
	<li><a href='http://www.52nlp.cn/2013/02'>2013年二月</a></li>
	<li><a href='http://www.52nlp.cn/2013/01'>2013年一月</a></li>
	<li><a href='http://www.52nlp.cn/2012/12'>2012年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2012/11'>2012年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2012/10'>2012年十月</a></li>
	<li><a href='http://www.52nlp.cn/2012/09'>2012年九月</a></li>
	<li><a href='http://www.52nlp.cn/2012/08'>2012年八月</a></li>
	<li><a href='http://www.52nlp.cn/2012/07'>2012年七月</a></li>
	<li><a href='http://www.52nlp.cn/2012/06'>2012年六月</a></li>
	<li><a href='http://www.52nlp.cn/2012/05'>2012年五月</a></li>
	<li><a href='http://www.52nlp.cn/2012/04'>2012年四月</a></li>
	<li><a href='http://www.52nlp.cn/2012/03'>2012年三月</a></li>
	<li><a href='http://www.52nlp.cn/2012/01'>2012年一月</a></li>
	<li><a href='http://www.52nlp.cn/2011/12'>2011年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2011/11'>2011年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2011/10'>2011年十月</a></li>
	<li><a href='http://www.52nlp.cn/2011/09'>2011年九月</a></li>
	<li><a href='http://www.52nlp.cn/2011/08'>2011年八月</a></li>
	<li><a href='http://www.52nlp.cn/2011/07'>2011年七月</a></li>
	<li><a href='http://www.52nlp.cn/2011/06'>2011年六月</a></li>
	<li><a href='http://www.52nlp.cn/2011/05'>2011年五月</a></li>
	<li><a href='http://www.52nlp.cn/2011/04'>2011年四月</a></li>
	<li><a href='http://www.52nlp.cn/2011/03'>2011年三月</a></li>
	<li><a href='http://www.52nlp.cn/2011/02'>2011年二月</a></li>
	<li><a href='http://www.52nlp.cn/2011/01'>2011年一月</a></li>
	<li><a href='http://www.52nlp.cn/2010/12'>2010年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2010/11'>2010年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2010/10'>2010年十月</a></li>
	<li><a href='http://www.52nlp.cn/2010/09'>2010年九月</a></li>
	<li><a href='http://www.52nlp.cn/2010/08'>2010年八月</a></li>
	<li><a href='http://www.52nlp.cn/2010/07'>2010年七月</a></li>
	<li><a href='http://www.52nlp.cn/2010/06'>2010年六月</a></li>
	<li><a href='http://www.52nlp.cn/2010/05'>2010年五月</a></li>
	<li><a href='http://www.52nlp.cn/2010/04'>2010年四月</a></li>
	<li><a href='http://www.52nlp.cn/2010/03'>2010年三月</a></li>
	<li><a href='http://www.52nlp.cn/2010/02'>2010年二月</a></li>
	<li><a href='http://www.52nlp.cn/2010/01'>2010年一月</a></li>
	<li><a href='http://www.52nlp.cn/2009/12'>2009年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2009/11'>2009年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2009/10'>2009年十月</a></li>
	<li><a href='http://www.52nlp.cn/2009/09'>2009年九月</a></li>
	<li><a href='http://www.52nlp.cn/2009/08'>2009年八月</a></li>
	<li><a href='http://www.52nlp.cn/2009/07'>2009年七月</a></li>
	<li><a href='http://www.52nlp.cn/2009/06'>2009年六月</a></li>
	<li><a href='http://www.52nlp.cn/2009/05'>2009年五月</a></li>
	<li><a href='http://www.52nlp.cn/2009/04'>2009年四月</a></li>
	<li><a href='http://www.52nlp.cn/2009/03'>2009年三月</a></li>
	<li><a href='http://www.52nlp.cn/2009/02'>2009年二月</a></li>
	<li><a href='http://www.52nlp.cn/2009/01'>2009年一月</a></li>
	<li><a href='http://www.52nlp.cn/2008/12'>2008年十二月</a></li>
		</ul>
</li>		<li id="recent-posts-2" class="widget-container widget_recent_entries">		<h3 class="widget-title">最新文章</h3>		<ul>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e5%9b%9b%e7%ab%a0-combining-models">PRML读书会第十四章 Combining Models</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%89%e7%ab%a0sequential-data">PRML读书会第十三章 Sequential Data</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%ba%8c%e7%ab%a0-continuous-latent-variables">PRML读书会第十二章 Continuous Latent Variables</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%80%e7%ab%a0-sampling-methods">PRML读书会第十一章  Sampling Methods</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e7%ab%a0-approximate-inference">PRML读书会第十章  Approximate Inference</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b9%9d%e7%ab%a0-mixture-models-and-em">PRML读书会第九章  Mixture Models and EM</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ab%e7%ab%a0-graphical-models">PRML读书会第八章  Graphical Models</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%83%e7%ab%a0-sparse-kernel-machines">PRML读书会第七章 Sparse Kernel Machines</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ad%e7%ab%a0-kernel-methods">PRML读书会第六章   Kernel Methods</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%94%e7%ab%a0-neural-networks">PRML读书会第五章  Neural Networks</a>
						</li>
				</ul>
		</li><li id="recentcomments" class="widget-container widget_recentcomments"><h3 class="widget-title">最近评论</h3><ul><li class="rc-navi rc-clearfix"><span class="rc-loading">正在加载...</span></li><li id="rc-comment-temp" class="rc-item rc-comment rc-clearfix"><div class="rc-info"></div><div class="rc-timestamp"></div><div class="rc-excerpt"></div></li><li id="rc-ping-temp" class="rc-item rc-ping rc-clearfix"><span class="rc-label"></span></li></ul></li>			</ul>
		</div><!-- #primary .widget-area -->


		<div id="secondary" class="widget-area" role="complementary">
			<ul class="xoxo">
				<li id="linkcat-103" class="widget-container widget_links"><h3 class="widget-title">NLP相关网站</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://www.aclweb.org/" rel="co-worker" title="The Association for Computational Linguistics" target="_blank">ACL</a></li>
<li><a href="http://aclweb.org/anthology-new/" rel="co-worker" title="A Digital Archive of Research Papers in Computational Linguistics" target="_blank">ACL Anthology</a></li>
<li><a href="http://belobog.si.umich.edu/clair/anthology/index.cgi" rel="colleague" target="_blank">ACL Anthology Network</a></li>
<li><a href="http://aclweb.org/aclwiki/index.php?title=Main_Page" rel="colleague" title="the Wiki of the Association for Computational Linguistics" target="_blank">ACL Wiki</a></li>
<li><a href="http://www.clsp.jhu.edu/" rel="colleague" target="_blank">CLSP</a></li>
<li><a href="http://www.cwbbase.com/" rel="colleague" title="这是一个略具规模的中文语义词库, 也是稍有特色的汉语语义词典" target="_blank">CWB中文词库</a></li>
<li><a href="http://www.euromatrix.net/" rel="colleague" target="_blank">EuroMatrix</a></li>
<li><a href="http://www.freebase.com" rel="colleague" target="_blank">Freebase</a></li>
<li><a href="http://www.clsp.jhu.edu/workshops/" rel="colleague" target="_blank">JHU Workshop</a></li>
<li><a href="http://www.ldc.upenn.edu/" rel="colleague" title="Linguistic Data Consortium" target="_blank">LDC</a></li>
<li><a href="http://www.statmt.org/moses/" rel="colleague" title="A factored phrase-based beam-search decoder for machine translation" target="_blank">Moses</a></li>
<li><a href="http://nlpers.blogspot.com/" rel="colleague" title="国外一个非常不错的自然语言处理博客" target="_blank">nlper</a></li>
<li><a href="http://www.nlpjob.com" target="_blank">NLPJob</a></li>
<li><a href="http://www.powerset.com/" rel="colleague" target="_blank">Powerset</a></li>
<li><a href="http://www.speech.sri.com/projects/srilm/" rel="colleague" title="- The SRI Language Modeling Toolkit" target="_blank">SRILM</a></li>
<li><a href="http://www.statmt.org/" rel="colleague" title="This website is dedicated to research in statistical machine translation" target="_blank">Statistical Machine Translation</a></li>
<li><a href="http://textanalysisonline.com/" target="_blank">Text Analysis</a></li>
<li><a href="http://textminingonline.com/" target="_blank">Text Mining</a></li>
<li><a href="http://textsummarization.net/" target="_blank">Text Summarization</a></li>
<li><a href="http://w3china.org/index.htm" rel="friend" title="致力于促进W3C技术的广泛应用, 传播关于未来Web的知识与技术" target="_blank">中国万维网联盟</a></li>
<li><a href="http://www.cipsc.org.cn/" rel="co-worker" title="Chinese Information Processing Society of China" target="_blank">中国中文信息学会</a></li>
<li><a href="http://www.nlp.org.cn/" rel="colleague" title="中文自然语言处理开放平台" target="_blank">中文自然语言处理开放平台</a></li>
<li><a href="http://www.mt-archive.info/" rel="colleague" title="Repository and bibliography of articles, books and papers on topics" target="_blank">机器翻译档案计划</a></li>
<li><a href="http://www.statmt.org/europarl/" rel="colleague" target="_blank">欧洲议会平行语料库</a></li>
<li><a href="http://www.keenage.com/" title="HowNet" target="_blank">知网</a></li>
<li><a href="http://www.nlpir.org/" rel="friend" title="由张华平博士发起，由北京理工大学网络搜索与挖掘实验室运营，旨在推动NLP(自然语言处理)与IR(信息检索)领域的共享与共赢" target="_blank">自然语言处理与信息检索共享平台</a></li>
<li><a href="http://mitel.ict.ac.cn/" rel="co-worker" title="中科院计算所多语言交互技术实验室" target="_blank">计算所多语言交互技术实验室</a></li>

	</ul>
</li>
<li id="linkcat-2" class="widget-container widget_links"><h3 class="widget-title">友情链接</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://blog.youxu.info/" title="一个计算机专业的 Ph.D. 学生徐宥的个人博客" target="_blank">4G spaces</a></li>
<li><a href="http://blog.52nlp.org" rel="me" title="我爱自然语言处理完全镜像" target="_blank">52nlpblog</a></li>
<li><a href="http://www.52nlp.com" rel="me" title="52nlp的英文站" target="_blank">52nlpcom</a></li>
<li><a href="http://hi.baidu.com/drkevinzhang" rel="friend" title="ICTCLAS 张华平博士的空间" target="_blank">ICTCLAS 张华平博士的空间</a></li>
<li><a href="http://blog.so8848.com/" rel="friend" title="信息检索博客" target="_blank">Information Retrieval Blog</a></li>
<li><a href="http://interop123.com/default.aspx" rel="friend" title="崔晓源师兄关于NET技术的站点" target="_blank">NET互操作技术社区</a></li>
<li><a href="http://bbs.w3china.org/" rel="friend" title="中国万维网联盟讨论区" target="_blank">W3CHINA讨论区</a></li>
<li><a href="http://www.ailab.cn/" rel="friend" target="_blank">人工智能网</a></li>
<li><a href="http://mindhacks.cn/" rel="friend" title="一个很有思想的价值博客!" target="_blank">刘未鹏之Mind Hacks</a></li>
<li><a href="http://www.cnblogs.com/finallyliuyu/" rel="friend" target="_blank">原地转圈的驴子</a></li>
<li><a href="http://xunren.thuir.org/" target="_blank">微博寻人（梁博）</a></li>
<li><a href="http://52opencourse.com" rel="friend" title="我爱公开课，高质量公开课交流平台" target="_blank">我爱公开课</a></li>
<li><a href="http://iregex.org/" rel="friend" target="_blank">我爱正则表达式</a></li>
<li><a href="http://courseminer.com" target="_blank">挖课</a></li>
<li><a href="http://www.flickering.cn/" target="_blank">火光摇曳</a></li>
<li><a href="http://www.sciencenet.cn/u/timy/" rel="friend" title="章成志老师的博客" target="_blank">章成志的博客</a></li>
<li><a href="http://blog.csdn.net/v_JULY_v/" target="_blank">结构之法 算法之道</a></li>
<li><a href="http://www.lingcc.com/" rel="friend" title="关注编译器,虚拟机,编程语言及技术,IT职业和程序员生活" target="_blank">编译点滴</a></li>
<li><a href="http://www.52nlp.org" rel="me" title="52nlp的官方网站" target="_blank">自然语言处理</a></li>
<li><a href="http://www.ieee.org.cn/" rel="friend" title="计算机科学论坛" target="_blank">计算机科学论坛</a></li>
<li><a href="http://coursegraph.com/">课程图谱</a></li>
<li><a href="http://blog.coursegraph.com" rel="friend">课程图谱博客</a></li>

	</ul>
</li>
<li id="meta-4" class="widget-container widget_meta"><h3 class="widget-title">功能</h3>			<ul>
						<li><a href="http://www.52nlp.cn/wp-login.php">登录</a></li>
			<li><a href="http://www.52nlp.cn/feed">文章<abbr title="Really Simple Syndication">RSS</abbr></a></li>
			<li><a href="http://www.52nlp.cn/comments/feed">评论<abbr title="Really Simple Syndication">RSS</abbr></a></li>
<li><a href="https://cn.wordpress.org/" title="基于WordPress，一个优美、先进的个人信息发布平台。">WordPress.org</a></li>			</ul>
</li>			</ul>
		</div><!-- #secondary .widget-area -->

	</div><!-- #main -->

	<div id="footer" role="contentinfo">
		<div id="colophon">



			<div id="site-info">
				<a href="http://www.52nlp.cn/" title="我爱自然语言处理" rel="home">
					我爱自然语言处理				</a>
			</div><!-- #site-info -->

			<div id="site-generator">
								<a href="http://cn.wordpress.org/"
						title="优雅的个人发布平台" rel="generator">
					自豪地采用 WordPress。				</a>
			</div><!-- #site-generator -->

		</div><!-- #colophon -->
	</div><!-- #footer -->

</div><!-- #wrapper -->

<script>
/* <![CDATA[ */
var rcGlobal = {
	serverUrl		:'http://www.52nlp.cn',
	infoTemp		:'%REVIEWER% 在 %POST%',
	loadingText		:'正在加载',
	noCommentsText	:'没有任何评论',
	newestText		:'&laquo; 最新的',
	newerText		:'&laquo; 上一页',
	olderText		:'下一页 &raquo;',
	showContent		:'',
	external		:'',
	avatarSize		:'0',
	avatarPosition	:'left',
	anonymous		:'匿名'
};
/* ]]> */
</script>
<script type='text/javascript' src='http://www.52nlp.cn/wp-content/plugins/akismet/_inc/form.js?ver=3.0.4'></script>
<link rel='stylesheet' id='yarppRelatedCss-css'  href='http://www.52nlp.cn/wp-content/plugins/yet-another-related-posts-plugin/style/related.css?ver=4.0.1' type='text/css' media='all' />
<script type='text/javascript' src='http://www.52nlp.cn/wp-content/plugins/wp-recentcomments/js/wp-recentcomments.js?ver=2.2.7'></script>
	<p align="center"> 本站架设在 <a href="http://www.52nlp.cn/digitalocean%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0">DigitalOcean</a> 上, 采用创作共用版权协议, 要求署名、非商业用途和保持一致. 转载本站内容必须也遵循“署名-非商业用途-保持一致”的创作共用协议.</p>
<!-- Piwik -->
<script type="text/javascript">
  var _paq = _paq || [];
  _paq.push(["trackPageView"]);
  _paq.push(["enableLinkTracking"]);

  (function() {
    var u=(("https:" == document.location.protocol) ? "https" : "http") + "://162.243.252.121/piwik/";
    _paq.push(["setTrackerUrl", u+"piwik.php"]);
    _paq.push(["setSiteId", "5"]);
    var d=document, g=d.createElement("script"), s=d.getElementsByTagName("script")[0]; g.type="text/javascript";
    g.defer=true; g.async=true; g.src=u+"piwik.js"; s.parentNode.insertBefore(g,s);
  })();
</script>
<!-- End Piwik Code -->
</body>
</html>
