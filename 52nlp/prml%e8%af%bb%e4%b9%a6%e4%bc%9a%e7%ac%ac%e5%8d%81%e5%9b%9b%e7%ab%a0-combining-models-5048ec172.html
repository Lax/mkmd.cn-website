<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8" />
<title>PRML读书会第十四章 Combining Models | 我爱自然语言处理</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="stylesheet" type="text/css" media="all" href="http://www.52nlp.cn/wp-content/themes/twentytenorg/style.css" />
<link rel="pingback" href="http://www.52nlp.cn/xmlrpc.php" />
<link rel="alternate" type="application/rss+xml" title="我爱自然语言处理 &raquo; Feed" href="http://www.52nlp.cn/feed" />
<link rel="alternate" type="application/rss+xml" title="我爱自然语言处理 &raquo; 评论Feed" href="http://www.52nlp.cn/comments/feed" />
<link rel="alternate" type="application/rss+xml" title="我爱自然语言处理 &raquo; PRML读书会第十四章 Combining Models评论Feed" href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e5%9b%9b%e7%ab%a0-combining-models/feed" />
<link rel='stylesheet' id='yarppWidgetCss-css'  href='http://www.52nlp.cn/wp-content/plugins/yet-another-related-posts-plugin/style/widget.css?ver=4.0.1' type='text/css' media='all' />
<link rel='stylesheet' id='codecolorer-css'  href='http://www.52nlp.cn/wp-content/plugins/codecolorer/codecolorer.css?ver=0.9.9' type='text/css' media='screen' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://www.52nlp.cn/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.52nlp.cn/wp-includes/wlwmanifest.xml" /> 
<link rel='prev' title='PRML读书会第十三章 Sequential Data' href='http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%89%e7%ab%a0sequential-data' />
<meta name="generator" content="WordPress 4.0.1" />
<link rel='canonical' href='http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e5%9b%9b%e7%ab%a0-combining-models' />
<link rel='shortlink' href='http://www.52nlp.cn/?p=8143' />
<!-- wp thread comment 1.4.9.4.002 -->
<style type="text/css" media="screen">
.editComment, .editableComment, .textComment{
	display: inline;
}
.comment-childs{
	border: 1px solid #999;
	margin: 5px 2px 2px 4px;
	padding: 4px 2px 2px 4px;
	background-color: white;
}
.chalt{
	background-color: #E2E2E2;
}
#newcomment{
	border:1px dashed #777;width:90%;
}
#newcommentsubmit{
	color:red;
}
.adminreplycomment{
	border:1px dashed #777;
	width:99%;
	margin:4px;
	padding:4px;
}
.mvccls{
	color: #999;
}
			
</style>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } },
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
});
</script><script type="text/javascript" src="http://www.52nlp.cn/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>

<body class="single single-post postid-8143 single-format-standard">
<div id="wrapper" class="hfeed">
	<div id="header">
		<div id="masthead">
			<div id="branding" role="banner">
								<div id="site-title">
					<span>
						<a href="http://www.52nlp.cn/" title="我爱自然语言处理" rel="home">我爱自然语言处理</a>
					</span>
				</div>
				<div id="site-description">I Love Natural Language Processing</div>

										<img src="http://www.52nlp.cn/wp-content/themes/twentytenorg/images/headers/path.jpg" width="940" height="198" alt="" />
								</div><!-- #branding -->

			<div id="access" role="navigation">
			  				<div class="skip-link screen-reader-text"><a href="#content" title="跳至正文">跳至正文</a></div>
								<div class="menu"><ul><li ><a href="http://www.52nlp.cn/">首页</a></li><li class="page_item page-item-2"><a href="http://www.52nlp.cn/about">关于</a></li><li class="page_item page-item-2557 page_item_has_children"><a href="http://www.52nlp.cn/resources">资源</a><ul class='children'><li class="page_item page-item-1271"><a href="http://www.52nlp.cn/resources/wpmatheditor">WpMathEditor</a></li></ul></li></ul></div>
 
				<div class="menu"><ul><li class="page_item page-item-2"></li><li class="page_item page-item-2"><a href="http://coursegraph.com" title="课程图谱" target="_blank">课程图谱</a></li><li class="page_item page-item-2"><a href="http://www.nlpjob.com" title="求职" target="_blank">求职招聘</a></li></ul></div>
			</div><!-- #access -->
		</div><!-- #masthead -->
	</div><!-- #header -->

	<div id="main">

		<div id="container">
			<div id="content" role="main">

			

				<div id="nav-above" class="navigation">
					<div class="nav-previous"><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%89%e7%ab%a0sequential-data" rel="prev"><span class="meta-nav">&larr;</span> PRML读书会第十三章 Sequential Data</a></div>
					<div class="nav-next"></div>
				</div><!-- #nav-above -->

				<div id="post-8143" class="post-8143 post type-post status-publish format-standard hentry category-pattern-recognition-and-machine-learning-2 category-344 tag-boosting tag-bootstrap tag-c4-5 tag-cart tag-combining-models tag-conditional-mixture-model tag-id3 tag-pattern-recognition-and-machine-learning tag-prml tag-prmlpdf tag-1101 tag-1107 tag-344 tag-961 tag-960 tag-1103 tag-1104 tag-1099 tag-1105 tag-1111">
					<h1 class="entry-title">PRML读书会第十四章 Combining Models</h1>

					<div class="entry-meta">
						<span class="meta-prep meta-prep-author">发表于</span> <a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e5%9b%9b%e7%ab%a0-combining-models" title="13:38" rel="bookmark"><span class="entry-date">2015年01月31号</span></a> <span class="meta-sep">由</span> <span class="author vcard"><a class="url fn n" href="http://www.52nlp.cn/author/prml" title="查看所有由 prml 发布的文章">prml</a></span>					</div><!-- .entry-meta -->

					<div class="entry-content">
						<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>PRML读书会第十四章 Combining Models<br />
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>主讲人 网神<br />
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 12pt"><strong>（新浪微博: <a href="http://weibo.com/ghtimaq">@豆角茄子麻酱凉面</a>）<br />
</strong></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">网神(66707180) 18:57:18<br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">大家好，今天我们讲一下第14章combining models，这一章是联合模型，通过将多个模型以某种形式结合起来，可以获得比单个模型更好的预测效果。包括这几部分：<br />
committees, 训练多个不同的模型，取其平均值作为最终预测值。<br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">boosting: 是committees的特殊形式，顺序训练L个模型，每个模型的训练依赖前一个模型的训练结果。<br />
决策树：不同模型负责输入变量的不同区间的预测，每个样本选择一个模型来预测，选择过程就像在树结构中从顶到叶子的遍历。<br />
conditional mixture model条件混合模型：引入概率机制来选择不同模型对某个样本做预测，相比决策树的硬性选择，要有很多优势。<br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">本章主要介绍了这几种混合模型。讲之前，先明确一下混合模型与Bayesian model averaging的区别，贝叶斯模型平均是这样的：假设有H个不同模型h，每个模型的先验概率是p(h)，一个数据集的分布是： <img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom1.png" alt="" /><br />
整个数据集X是由一个模型生成的，关于h的概率仅仅表示是由哪个模型来生成的 这件事的不确定性。而本章要讲的混合模型是数据集中，不同的数据点可能由不同模型生成。看后面讲到的内容就明白了。</span><span id="more-8143"></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt"> 首先看committes，committes是一大类，包括boosting，首先将最简单的形式，就是讲多个模型的预测的平均值作为最后的预测。主要讲这么做的合理性，为什么这么做会提高预测性能。从频率角度的概念，bias-variance trade-off可以解释，这个理论在3.5节讲过，我们把这个经典的图copy过来：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom2.png" alt="" /><br />
这个图大家都记得吧，左边一列是对多组数据分别训练得到一个模型，对应一条sin曲线，看左下角这个图，正则参数lamda取得比较小，得到一个bias很小，variance很大的一个模型 。每条线的variance都很大，这样模型预测的错误就比较大，但是把这么多条曲线取一个平均值，得到右下角图上的红色线，红色线跟真实sin曲线也就是蓝色线 基本拟合。所以用平均之后模型来预测，variance准确率就提高了很多，这是直观上来看，接下里从数学公式推导看下：<br />
有一个数据集，用bootstrap方法构造M个不同的训练集bootstrap方法就是从数据集中随机选N个放到训练集中，做M次，就得到M个训练集，M个训练集训练的到M个模型，用<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom3.png" alt="" />表示，那么用committees方法，对于某个x，最终预测值是:<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom4.png" alt="" /><br />
我们来看这个预测值是如何比单个<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom5.png" alt="" />预测值准确的，假设准确的预测模型是h(x)，那么训练得到的y(x)跟h(x)的关系是：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom6.png" alt="" /><br />
后面那一项是模型的error<br />
ZealotMaster(850458544) 19:24:34<br />
能使error趋近于0嘛？<br />
网神(66707180) 19:25:13<br />
模型越好越趋近于0，但很难等于0，这里committes方法就比单个模型更趋近于0<br />
ZealotMaster(850458544) 19:25:28<br />
求证明<br />
网神(66707180) 19:25:39<br />
正在证明，平均平方和错误如下：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom7.png" alt="" /><br />
也就是单个模型的期望error是：<br />
</span><!--more--></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom8.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
如果用M个模型分别做预测，其平均错误是:<br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom9.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
而如果用committes的结果来做预测，其期望错误是：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom10.png" alt="" /><br />
这个<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom11.png" alt="" />跑到了平方的里面，如果假设不同模型的error都是0均值，并且互不相关，也就是：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom12.png" alt="" /><br />
就可以得到：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom13.png" alt="" /><br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">在不同模型error互不相关的假设的下，committes错误是单个模型error的1/M，但实际上，不同模型的error通常是相关的，因此error不会减少这么多，但肯定是小于单个模型的error，接下来讲boosting，可以不考虑那个假设，取得实质的提高.boosting应该是有不同的变种，其中最出名的就是AdaBoost, adaptive boosting. 它可以将多个弱分类器结合，取得很好的预测效果，所谓弱分类器就是，只要比随即预测强一点，大概就是只要准确率超50%就行吧，这是我的理解。<br />
boosting的思想是依次预测每个分类器，每个分类器训练时，对每个数据点加一个权重。训练完一个分类器模型，该模型分错的，再下一个模型训练时，增大权重；分对的，减少权重，具体的算法如下，我把整个算法帖出来，再逐步解释：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom14.png" alt="" /><br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom15.png" alt="" /><br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">大家看下面这个图比较形象：<br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom16.png" alt="" /><span style="font-family: 宋体;font-size: 12pt"><br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">第一步，初始化每个数据点的权重为1/N.接下来依次训练M个分类器，每个分类器训练时，最小化加权的错误函数(14.15)，错误函数看上面贴的算法，从这个错误函数可以看出，权重相同时，尽量让更多的x分类正确，权重不同时，优先让权重大的x分类正确，训练完一个模型后，式(14.16)计算<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom17.png" alt="" />，既分类错误的样本的加权比例. 然后式(14.17)计算:<br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom18.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
只要分类器准确率大于50%，<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom19.png" alt="" />就小于0.5, <img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom20.png" alt="" />就大于0。而且<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom21.png" alt="" />越小(既对应的分类器准确率越高)，<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom22.png" alt="" />就越大，然后用<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom23.png" alt="" />更新每个数据点的权重，即式(14.18)：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom24.png" alt="" /><br />
可以看出，对于分类错误的数据点，<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom25.png" alt="" />大于0，所以exp(a)就大于1，所以权重变大。但是从这个式子看不出，对于分类正确的样本，权重变小。这个式子表明，分类正确的样本，其权重不变 ，因为exp(0)=1.这是个疑问。如此循环，训练完所有模型，最后用式(14.19)做预测:<br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom26.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
从上面过程可以看出，如果训练集合中某个样本在逐步训练每个模型时，一直被分错，他的权重就会一直变大，最后对应的<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom27.png" alt="" />也越来越大，下面看一个图例：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom28.png" alt="" /><br />
图中有蓝红两类样本 ，分类器是单个的平行于轴线的阈值 ，第一个分类器(m=1)把大部分点分对了，但有2个蓝点，3个红点不对，m=2时，这几个错的就变大了，圈越大，对应其权重越大 ，后面的分类器似乎是专门为了这个几个错分点而在努力工作，经过150个分类器，右下角那个图的分割线已经很乱了，看不出到底对不对 ，应该是都已经分对了吧。<br />
网神(66707180) 19:59:59<br />
@ZealotMaster 不知道是否明白点了，大家有啥问题？<br />
ZealotMaster(850458544) 20:00:14<br />
嗯，清晰一些了，这个也涉及over fitting嘛？感觉m=150好乱……<br />
苦瓜炒鸡蛋(852383636) 20:02:23<br />
是过拟合<br />
网神(66707180) 20:02:25<br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">不同的分割线，也就是不同的模型，是主要针对不同的点的，针对哪些点，由模型组合时的系数<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom29.png" alt="" />来影响。<br />
苦瓜炒鸡蛋(852383636) 20:04:50<br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">这是韩家炜 那个数据挖掘书的那一段：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom30.png" alt="" />网神(66707180) 20:04:56<br />
嗯，这章后面也讲到了boosting对某些错分的样本反应太大，一些异常点会对模型造成很大的影响。<br />
</span></p>
<p><span style="color: #333333;font-family: Arial;font-size: 12pt">================================================================<br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">接下来讲boosting的错误函数，我们仔细看下对boosting错误函数的分析，boosting最初用统计学习理论来分析器泛化错误的边界bound，但后来发现这个bound太松，没有意义。实际性能比这个理论边界好得多，后来用指数错误函数来表示。从优化指数损失函数来解释adaboost比较直观，每次固定其他分类器和系数将常量分出来，能推出单分类器的损失函数及系数，再根据常量的形式能得出下一步数据权重的更新方式。指数错误函数如下：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom31.png" alt="" /><br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">其中N是N个样本点，<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom32.png" alt="" />是多个线性分类器的线性组合:<br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom33.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt"><br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom34.png" alt="" />是分类的目标值。我们的目标是训练系数<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom35.png" alt="" />和分类器<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom36.png" alt="" />的参数，使E最小。<br />
最小化E的方法，是先只针对一个分类器进行最小化，而固定其他分类器的参数，例如我们固定<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom37.png" alt="" />和其系数<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom38.png" alt="" />，只针对<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom39.png" alt="" />做优化，这样错误函数E可以改写为：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom40.png" alt="" /><br />
也就是把固定的分类器的部分都当做一个常量：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom41.png" alt="" />，只保留<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom42.png" alt="" />相关的部分。我们用<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom43.png" alt="" />表示<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom44.png" alt="" />分对的数据集，<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom45.png" alt="" />表示分错的数据集，E又可以写成如下形式：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom46.png" alt="" />上式中，因为将数据分成两部分，也就确定了<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom47.png" alt="" />个<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom48.png" alt="" />是否相等，所以消去了这两个变量<br />
看起来清爽点了：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom49.png" alt="" /><br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">这里面后一项是常量，前一项就跟前面boosting的算法里所用的错误函数：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom50.png" alt="" />形式一样了。<br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">上面是将：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom51.png" alt="" />对<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom52.png" alt="" />做最小化得出的结论,即指数错误函数就是boosting里求单个模型时所用的错误函数.类似，也可以得到指数错误函数里的<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom53.png" alt="" />就是boosting里的<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom54.png" alt="" />，确定了<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom55.png" alt="" />根据<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom56.png" alt="" />以及<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom57.png" alt="" /><br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">可以得到，更新w的方法：<br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom58.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
又因为<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom59.png" alt="" />有：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom60.png" alt="" />这又跟boosting里更新数据点权重的方法以一致。<br />
总之，就是想说明，用指数错误函数可以描述boosting的error分析,接下来看看指数错误函数的性质，再贴一下指数错误函数的形式：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom61.png" alt="" /><br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom62.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
其期望error是：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom63.png" alt="" /><br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">然后最所有的y(x)做variational minimization,得到：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom64.png" alt="" /><br />
这是half the log-odds ，看下指数错误函数的图形：</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom65.png" alt="" /><br />
绿色的线是指数错误函数<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom66.png" alt="" />可以看到，对于分错的情况，既z&lt;0时，绿色的线呈指数趋势上升，所以异常点会对训练结果的影响很大。图中红色的线是corss-entropy 错误，是逻辑分类中用的错误函数，对分错的情况，随错误变大，其函数值基本是线性增加的，蓝色线是svm用的错误函数，叫hinge 函数。</p>
<p>&nbsp;</p>
<p><span style="color: #333333;font-family: Arial;font-size: 12pt">================================================================<br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">大家有没有要讨论的？公式比较多，需要推导一下才能理解。接下来讲决策树和条件混合模型。决策树概念比较简单，容易想象是什么样子的，可以认为决策树是多个模型，每个模型负责x的一个区间的预测，通过树形结构来寻找某个x属于哪个区间，从而得到预测值。<br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">决策树有多个算法比较出名，ID3, C4.5, CART，书上以CART为例讲的。<br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">CART叫classification and regression trees先看一个图示：<br />
</span></p>
<p style="background: white"><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom67.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">这个二维输入空间，被划分成5个区间，每个区间的类别分别是A-E，它的决策树如下，很简单明了：<br />
</span></p>
<p style="background: white"><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom68.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">决策树在一些领域应用比较多，最主要的原因是，他有很好的可解释性。那如何训练得到一个合适的决策树呢？也就是如何决定需要哪些节点，节点上选择什么变量作为判断依据，以及判断的阈值是多大。<br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">首先错误函数是残差的平方和，而树的结构则用贪心策略来演化。<br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">开始只有一个节点，也就是根节点，对应整个输入空间，然后将空间划分为2，在多个划分选择之中，选择使残差最小的那个划分，书上提到这个区间划分以及划分点阈值的选择，是用穷举的方法。然后对不同的叶子节点再分别划分子区间。这样树不停长大，何时停止增加节点呢？简单的方法是当残差小于某个值的时候。但经验发现经常再往多做一些划分后，残差又可以大幅降低<br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">所以通常的做法是，先构造一个足够大的树，使叶子节点多到某个数量，然后再剪枝，合并的原则是使叶子尽量减少，同时残差又不要增大。<br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">也就是最小化这个值：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom69.png" alt="" />其中：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom70.png" alt="" /><br />
</span></p>
<p style="background: white"><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom71.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt">是某个叶子负责的那个区间里的所有数据点的预测值的平均值：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom72.png" alt="" /><br />
</span></p>
<p style="background: white"><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom73.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt">是某个数据点的预测值，<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom74.png" alt="" />是叶子的总数，<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom75.png" alt="" />控制残差和叶子数的trade-off，这是剪枝的依据。<br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">以上是针对回归说的，对于分类，剪枝依据仍然是：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom76.png" alt="" /><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">只是Q(T)不再是残差的平方和，而是cross-entropy或Gini index<br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">交叉熵的计算是:<br />
</span></p>
<p style="background: white"><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom77.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
</span></p>
<p style="background: white"><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom78.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt">是第<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom79.png" alt="" />个叶子，也就是第<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom80.png" alt="" />个区间里的数据点，被赋予类别k的比例。<br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">Gini index计算是：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom81.png" alt="" /><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">决策树的优点是直观，可解释性好。缺点是每个节点对输入空间的划分都是根据某个维度来划分的，在坐标空间里看，就是平行于某个轴来划分的，其次，决策树的划分是硬性的，在回归问题里，硬性划分更明显。<br />
</span></p>
<p><span style="color: #333333;font-family: Arial;font-size: 12pt">================================================================<br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">决策树就讲这么多，接下来是conditional mixture models条件混合模型。条件混合模型，我的理解是，将不同的模型依概率来结合，这部分讲了线性回归的条件混合，逻辑回归的条件混合，和更一般性的混合方法mixture of experts，首先看线性回归的混合。<br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">第9章讲过高斯混合模型，其模型是这样的：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom82.png" alt="" /><br />
</span></p>
<p style="background: white;margin-left: 78pt"><span style="font-family: 微软雅黑;font-size: 12pt">这是用多个高斯密度分布来拟合输入空间，就像这种x的分布：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom83.png" alt="" /><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">线性回归混合的实现，可以把这个高斯混合分布扩展成条件高斯分布，模型如下：<br />
</span></p>
<p style="background: white"><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom84.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">这里面，有K个线性回归<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom85.png" alt="" />，其权重参数是<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom86.png" alt="" />，将多个线性回归的预测值确定的概率联合起来，得到最终预测值的概率分布。模型中的参数<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom87.png" alt="" />包括<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom88.png" alt="" /><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom89.png" alt="" />三部分，接下来看如何训练得到这些参数，总体思路是用第9章介绍的EM算法，引入一个隐藏变量<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom90.png" alt="" />，每个训练样本点对应一个<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom91.png" alt="" />，<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom92.png" alt="" />是一个K维的二元向量，<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom93.png" alt="" />,如果第k个模型负责生成第n个数据点，则<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom94.png" alt="" />等于1，否则等于0，这样，我们可以写出log似然函数：<br />
</span></p>
<p style="background: white"><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom95.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">然后用EM算法结合最大似然估计来求各个参数，EM算法首先选择所有参数的初始值，假设是<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom96.png" alt="" /><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">在E步根据这些参数值，可以得到模型k相对于数据点n的后验概率：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom97.png" alt="" /><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">书上提高一个词，这个后验概率又叫做responsibilities，大概是这个数据点n由模型k 负责生成的概率吧，有了这个responsibilities，就可以计算似然函数的期望值了，如下：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom98.png" alt="" /><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">在EM的M步，我们令<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom99.png" alt="" />为固定值，最大化这个期望值<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom100.png" alt="" />,从而求得新的参数<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom101.png" alt="" /><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">首先看<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom102.png" alt="" />，<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom103.png" alt="" />是各个模型的混合权重系数，满足<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom104.png" alt="" />，用拉格朗日乘子法，可以求得:<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom105.png" alt="" /><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">接下来求线性回归的参数<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom106.png" alt="" />，将似然函数期望值的式子里的高斯分布展开，可以得到如下式子：<br />
</span></p>
<p style="background: white"><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom107.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">要求第k个模型的<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom108.png" alt="" />，其他模型的W都对其没有影响，可以统统归做后面的const，这是因为log似然函数，每个模型之间是相加的关系，一求导数，其他模型的项就都消去了。上面的式子是一个加权的最小平方损失函数，每个数据点对应一个权重<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom109.png" alt="" />，这个权重可以看做是数据点的有效精度，将这个式子对<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom110.png" alt="" />求导，可以得到：<br />
</span></p>
<p style="background: white"><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom111.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">最后求得<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom112.png" alt="" /><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">其中<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom113.png" alt="" />，同样，对<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom114.png" alt="" />求导，得到：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom115.png" alt="" /><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">这样的到了所有的参数的新值，再重复E步和M步，迭代求得满意的最终的参数值。<br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">接下来看一个线性回归混合模型EM求参数的图示，两条直线对应两个线性回归模型用EM方法迭代求参数，两条直线最终拟合两部分数据点，中间和右边分别是经过了30轮和50轮迭代，下面那一排，表示每一轮里，每个数据点对应的responsibilities，也就是类k对于数据点n的后验概率：<br />
</span></p>
<p style="background: white"><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom116.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">最终求得的混合模型图示：<br />
</span></p>
<p style="background: white"><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom117.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">接下来讲逻辑回归混合模型，逻辑回归模型本身就定义了一个目标值的概率，所以可以直接用逻辑回归本身作为概率混合模型的组件，其模型如下：<br />
</span></p>
<p style="background: white"><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom118.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">其中<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom119.png" alt="" />，这里面的参数<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom120.png" alt="" />包括<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom121.png" alt="" />两部分。求参数的方法也是用EM，这里就不细讲了，要提一下的是在M步，似然函数不是一个closed-form的形式，不能直接求导来的出参数，需要用IRLS(iterative reweighted least squares)等迭代方法来求，下图是一个逻辑回归混合模型的训练的图示，左图是两个类别的数据，蓝色和红色表示实际的分布，中间图是用一个逻辑回归来拟合的模型，右图是用两个逻辑回归混合模型拟合的图形：<br />
</span></p>
<p style="background: white"><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom122.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">接下来讲mixtures of experts，mixture of experts是更一般化的混合模型，前面讲的两个混合模型，其混合系数是固定值，我们可以让混合系数是输入x的函数即：<br />
</span></p>
<p style="background: white"><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom123.png" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">系数<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom124.png" alt="" />叫做gating函数，组成模型<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom125.png" alt="" />叫做experts，,每个experts可以对输入空间的不同区域建模，对不同区域的输入进行预测，而gating函数则决定一个experts该负责哪个区域。<br />
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">gating函数满足<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0537_PRMLCom126.png" alt="" />，因此可以用softmax函数来作为gating函数，如果expert函数也是线性函数，则整个模型就可以用EM算法来确定。在M步，可能需要用IRLS，来迭代求解，这个模型仍然有局限性，更一般化的模型是hierarchical mixture of experts<br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">也就是混合模型的每个组件又可以是一个混合模型。好了就讲这么多吧，书上第14章的内容都讲完了。</span></p>
<p>注：PRML读书会系列文章由 <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a> 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。</p>
<p>PRML读书会讲稿PDF版本以及更多资源下载地址：<a href="http://vdisk.weibo.com/u/1841149974">http://vdisk.weibo.com/u/1841149974</a></p>
<p>本文链接地址：<a href="http://www.52nlp.cn/prml读书会第十四章-combining-models">http://www.52nlp.cn/prml读书会第十四章-combining-models</a></p>
<div class='yarpp-related'>
<p>相关文章:<ol>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ab%e7%ab%a0-graphical-models" rel="bookmark" title="PRML读书会第八章  Graphical Models">PRML读书会第八章  Graphical Models </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b9%9d%e7%ab%a0-mixture-models-and-em" rel="bookmark" title="PRML读书会第九章  Mixture Models and EM">PRML读书会第九章  Mixture Models and EM </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%89%e7%ab%a0-linear-models-for-regression" rel="bookmark" title="PRML读书会第三章 Linear Models for Regression">PRML读书会第三章 Linear Models for Regression </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%9b%9b%e7%ab%a0-linear-models-for-classification" rel="bookmark" title="PRML读书会第四章 Linear Models for Classification">PRML读书会第四章 Linear Models for Classification </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%83%e7%ab%a0-sparse-kernel-machines" rel="bookmark" title="PRML读书会第七章 Sparse Kernel Machines">PRML读书会第七章 Sparse Kernel Machines </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%94%e7%ab%a0-neural-networks" rel="bookmark" title="PRML读书会第五章  Neural Networks">PRML读书会第五章  Neural Networks </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%80%e7%ab%a0-introduction" rel="bookmark" title="PRML读书会第一章  Introduction">PRML读书会第一章  Introduction </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%89%e7%ab%a0sequential-data" rel="bookmark" title="PRML读书会第十三章 Sequential Data">PRML读书会第十三章 Sequential Data </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e5%89%8d%e8%a8%80" rel="bookmark" title="PRML读书会前言">PRML读书会前言 </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ad%e7%ab%a0-kernel-methods" rel="bookmark" title="PRML读书会第六章   Kernel Methods">PRML读书会第六章   Kernel Methods </a></li>
</ol></p>
</div>
											</div><!-- .entry-content -->


					<div class="entry-utility">
						此条目发表在 <a href="http://www.52nlp.cn/category/pattern-recognition-and-machine-learning-2" rel="category tag">PRML</a>, <a href="http://www.52nlp.cn/category/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" rel="category tag">机器学习</a> 分类目录，贴了 <a href="http://www.52nlp.cn/tag/boosting" rel="tag">boosting</a>, <a href="http://www.52nlp.cn/tag/bootstrap" rel="tag">bootstrap</a>, <a href="http://www.52nlp.cn/tag/c4-5" rel="tag">C4.5</a>, <a href="http://www.52nlp.cn/tag/cart" rel="tag">CART</a>, <a href="http://www.52nlp.cn/tag/combining-models" rel="tag">Combining Models</a>, <a href="http://www.52nlp.cn/tag/conditional-mixture-model" rel="tag">conditional mixture model</a>, <a href="http://www.52nlp.cn/tag/id3" rel="tag">ID3</a>, <a href="http://www.52nlp.cn/tag/pattern-recognition-and-machine-learning" rel="tag">Pattern Recognition And Machine Learning</a>, <a href="http://www.52nlp.cn/tag/prml" rel="tag">PRML</a>, <a href="http://www.52nlp.cn/tag/prml%e8%af%bb%e4%b9%a6%e4%bc%9a" rel="tag">PRML读书会</a>, <a href="http://www.52nlp.cn/tag/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%b3%bb%e5%88%97pdf" rel="tag">PRML读书会系列PDF</a>, <a href="http://www.52nlp.cn/tag/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e8%ae%b2%e7%a8%bf" rel="tag">PRML读书会讲稿</a>, <a href="http://www.52nlp.cn/tag/%e5%86%b3%e7%ad%96%e6%a0%91" rel="tag">决策树</a>, <a href="http://www.52nlp.cn/tag/%e5%88%86%e7%b1%bb%e5%99%a8" rel="tag">分类器</a>, <a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" rel="tag">机器学习</a>, <a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b9%a6%e7%b1%8d" rel="tag">机器学习书籍</a>, <a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e8%af%bb%e4%b9%a6%e4%bc%9a" rel="tag">机器学习读书会</a>, <a href="http://www.52nlp.cn/tag/%e6%9d%a1%e4%bb%b6%e6%b7%b7%e5%90%88%e6%a8%a1%e5%9e%8b" rel="tag">条件混合模型</a>, <a href="http://www.52nlp.cn/tag/%e6%b7%b7%e5%90%88%e6%a8%a1%e5%9e%8b" rel="tag">混合模型</a>, <a href="http://www.52nlp.cn/tag/%e8%81%94%e5%90%88%e6%a8%a1%e5%9e%8b" rel="tag">联合模型</a>, <a href="http://www.52nlp.cn/tag/%e8%b4%9d%e5%8f%b6%e6%96%af%e6%a8%a1%e5%9e%8b%e5%b9%b3%e5%9d%87" rel="tag">贝叶斯模型平均</a>, <a href="http://www.52nlp.cn/tag/%e9%ab%98%e6%96%af%e6%b7%b7%e5%90%88%e6%a8%a1%e5%9e%8b" rel="tag">高斯混合模型</a> 标签。将<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e5%9b%9b%e7%ab%a0-combining-models" title="链向 PRML读书会第十四章 Combining Models 的固定链接" rel="bookmark">固定链接</a>加入收藏夹。											</div><!-- .entry-utility -->
				</div><!-- #post-## -->

				<div id="nav-below" class="navigation">
					<div class="nav-previous"><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%89%e7%ab%a0sequential-data" rel="prev"><span class="meta-nav">&larr;</span> PRML读书会第十三章 Sequential Data</a></div>
					<div class="nav-next"></div>
				</div><!-- #nav-below -->

				
			<div id="comments">




								<div id="respond" class="comment-respond">
				<h3 id="reply-title" class="comment-reply-title">发表评论 <small><a rel="nofollow" id="cancel-comment-reply-link" href="/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e5%9b%9b%e7%ab%a0-combining-models#respond" style="display:none;">取消回复</a></small></h3>
									<form action="http://www.52nlp.cn/wp-comments-post.php" method="post" id="commentform" class="comment-form">
																			<p class="comment-notes">电子邮件地址不会被公开。 必填项已用<span class="required">*</span>标注</p>							<p class="comment-form-author"><label for="author">姓名 <span class="required">*</span></label> <input id="author" name="author" type="text" value="" size="30" aria-required='true' /></p>
<p class="comment-form-email"><label for="email">电子邮件 <span class="required">*</span></label> <input id="email" name="email" type="text" value="" size="30" aria-required='true' /></p>
<p class="comment-form-url"><label for="url">站点</label> <input id="url" name="url" type="text" value="" size="30" /></p>
												<p class="comment-form-comment"><label for="comment">评论</label> <textarea id="comment" name="comment" cols="45" rows="8" aria-required="true"></textarea></p>						<p class="form-allowed-tags">您可以使用这些<abbr title="HyperText Markup Language">HTML</abbr>标签和属性： <code>&lt;a href=&quot;&quot; title=&quot;&quot;&gt; &lt;abbr title=&quot;&quot;&gt; &lt;acronym title=&quot;&quot;&gt; &lt;b&gt; &lt;blockquote cite=&quot;&quot;&gt; &lt;cite&gt; &lt;code&gt; &lt;del datetime=&quot;&quot;&gt; &lt;em&gt; &lt;i&gt; &lt;q cite=&quot;&quot;&gt; &lt;strike&gt; &lt;strong&gt; </code></p>						<p class="form-submit">
							<input name="submit" type="submit" id="submit" value="发表评论" />
							<input type='hidden' name='comment_post_ID' value='8143' id='comment_post_ID' />
<input type='hidden' name='comment_parent' id='comment_parent' value='0' />
						</p>
						<p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="e1079fae4e" /></p><p style="display: none;"><input type="hidden" id="ak_js" name="ak_js" value="174"/></p><p><input type="hidden" id="comment_reply_ID" name="comment_reply_ID" value="0" /><input type="hidden" id="comment_reply_dp" name="comment_reply_dp" value="0" /></p><div id="cancel_reply" style="display:none;"><a href="javascript:void(0)" onclick="movecfm(null,0,1,null);" style="color:red;">点击取消回复</a></div><script type="text/javascript">
/* <![CDATA[ */
var commentformid = "commentform";
var USERINFO = false;
var atreply = "none";
/* ]]> */
</script>
<script type="text/javascript" src="http://www.52nlp.cn/wp-content/plugins/wp-thread-comment/wp-thread-comment.js.php?jsver=common"></script>
					</form>
							</div><!-- #respond -->
			
</div><!-- #comments -->


			</div><!-- #content -->
		</div><!-- #container -->

﻿
		<div id="primary" class="widget-area" role="complementary">
			<ul class="xoxo">
<!-- begin l_sidebar -->
	<div id="l_sidebar">
<p>卓越网：<a href="http://www.amazon.cn/mn/searchApp?source=garypyang-23&searchType=1&keywords=自然语言处理" title="自然语言处理书籍"target=_blank>自然语言处理书籍</a><br>
<li id="search-3" class="widget-container widget_search"><h3 class="widget-title">站内搜索</h3><form role="search" method="get" id="searchform" class="searchform" action="http://www.52nlp.cn/">
				<div>
					<label class="screen-reader-text" for="s">搜索：</label>
					<input type="text" value="" name="s" id="s" />
					<input type="submit" id="searchsubmit" value="搜索" />
				</div>
			</form></li><li id="text-4" class="widget-container widget_text"><h3 class="widget-title">NLPJob新鲜职位推荐:</h3>			<div class="textwidget"><p></p>
<script src="http://www.nlpjob.com/api/api.php?action=getJobs
&type=0&category=0&count=8&random=1&days_behind=7&response=js" type="text/javascript"></script>

<script type="text/javascript">showJobs('jobber-container', 'jobber-list');</script></div>
		</li><li id="text-3" class="widget-container widget_text"><h3 class="widget-title">52nlp新浪微博</h3>			<div class="textwidget"><p><iframe id="sina_widget_2104931705" style="width:100%; height:500px;" frameborder="0" scrolling="no" src="http://v.t.sina.com.cn/widget/widget_blog.php?uid=2104931705&height=500&skin=wd_01&showpic=1"></iframe></p>
<p><!-- JiaThis Button BEGIN --><br />
<script type="text/javascript" src="http://v3.jiathis.com/code/jiathis_r.js?uid=1340292124103344&move=0&amp;btn=r3.gif" charset="utf-8"></script><br />
<!-- JiaThis Button END --></p>
</div>
		</li><li id="categories-309398091" class="widget-container widget_categories"><h3 class="widget-title">分类目录</h3>		<ul>
	<li class="cat-item cat-item-72"><a href="http://www.52nlp.cn/category/mit-nlp" title="麻省理工学院开放式课程&quot;自然语言处理“的相关翻译文章">MIT自然语言处理</a> (23)
</li>
	<li class="cat-item cat-item-976"><a href="http://www.52nlp.cn/category/pattern-recognition-and-machine-learning-2" >PRML</a> (15)
</li>
	<li class="cat-item cat-item-469"><a href="http://www.52nlp.cn/category/topic-model" >Topic Model</a> (10)
</li>
	<li class="cat-item cat-item-87"><a href="http://www.52nlp.cn/category/wordpress" >wordpress</a> (6)
</li>
	<li class="cat-item cat-item-317"><a href="http://www.52nlp.cn/category/%e4%b8%93%e9%a2%98" >专题</a> (6)
</li>
	<li class="cat-item cat-item-263"><a href="http://www.52nlp.cn/category/chinese-information-processing" >中文信息处理</a> (20)
</li>
	<li class="cat-item cat-item-62"><a href="http://www.52nlp.cn/category/word-segmentation" >中文分词</a> (36)
</li>
	<li class="cat-item cat-item-420"><a href="http://www.52nlp.cn/category/%e5%b9%b6%e8%a1%8c%e7%ae%97%e6%b3%95" >并行算法</a> (1)
</li>
	<li class="cat-item cat-item-268"><a href="http://www.52nlp.cn/category/%e6%8b%9b%e8%81%98" >招聘</a> (4)
</li>
	<li class="cat-item cat-item-560"><a href="http://www.52nlp.cn/category/%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f" >推荐系统</a> (3)
</li>
	<li class="cat-item cat-item-354"><a href="http://www.52nlp.cn/category/%e6%95%b0%e6%8d%ae%e6%8c%96%e6%8e%98" >数据挖掘</a> (2)
</li>
	<li class="cat-item cat-item-241"><a href="http://www.52nlp.cn/category/text-classification" >文本分类</a> (3)
</li>
	<li class="cat-item cat-item-193"><a href="http://www.52nlp.cn/category/maximum-entropy-model" >最大熵模型</a> (7)
</li>
	<li class="cat-item cat-item-344"><a href="http://www.52nlp.cn/category/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" >机器学习</a> (29)
</li>
	<li class="cat-item cat-item-1"><a href="http://www.52nlp.cn/category/machine-translation" >机器翻译</a> (54)
</li>
	<li class="cat-item cat-item-195"><a href="http://www.52nlp.cn/category/%e6%9d%a1%e4%bb%b6%e9%9a%8f%e6%9c%ba%e5%9c%ba" >条件随机场</a> (3)
</li>
	<li class="cat-item cat-item-153"><a href="http://www.52nlp.cn/category/tagging" >标注</a> (13)
</li>
	<li class="cat-item cat-item-885"><a href="http://www.52nlp.cn/category/%e7%a7%91%e5%ad%a6%e8%ae%a1%e7%ae%97" >科学计算</a> (1)
</li>
	<li class="cat-item cat-item-538"><a href="http://www.52nlp.cn/category/%e7%bb%9f%e8%ae%a1%e5%ad%a6" >统计学</a> (10)
</li>
	<li class="cat-item cat-item-126"><a href="http://www.52nlp.cn/category/translation-model" >翻译模型</a> (2)
</li>
	<li class="cat-item cat-item-51"><a href="http://www.52nlp.cn/category/nlp" >自然语言处理</a> (227)
</li>
	<li class="cat-item cat-item-106"><a href="http://www.52nlp.cn/category/computational-linguistics" >计算语言学</a> (39)
</li>
	<li class="cat-item cat-item-22"><a href="http://www.52nlp.cn/category/dictionary" >词典</a> (8)
</li>
	<li class="cat-item cat-item-221"><a href="http://www.52nlp.cn/category/semantics" >语义学</a> (1)
</li>
	<li class="cat-item cat-item-161"><a href="http://www.52nlp.cn/category/semantic-web" >语义网</a> (3)
</li>
	<li class="cat-item cat-item-37"><a href="http://www.52nlp.cn/category/corpus" >语料库</a> (12)
</li>
	<li class="cat-item cat-item-86"><a href="http://www.52nlp.cn/category/language-model" >语言模型</a> (23)
</li>
	<li class="cat-item cat-item-156"><a href="http://www.52nlp.cn/category/speech-recognition" >语音识别</a> (4)
</li>
	<li class="cat-item cat-item-314"><a href="http://www.52nlp.cn/category/%e8%b4%9d%e5%8f%b6%e6%96%af%e6%a8%a1%e5%9e%8b" >贝叶斯模型</a> (1)
</li>
	<li class="cat-item cat-item-110"><a href="http://www.52nlp.cn/category/reprint" >转载</a> (28)
</li>
	<li class="cat-item cat-item-451"><a href="http://www.52nlp.cn/category/%e9%97%ae%e7%ad%94%e7%b3%bb%e7%bb%9f" >问答系统</a> (1)
</li>
	<li class="cat-item cat-item-3"><a href="http://www.52nlp.cn/category/informal-essay" >随笔</a> (63)
</li>
	<li class="cat-item cat-item-60"><a href="http://www.52nlp.cn/category/hidden-markov-model" >隐马尔科夫模型</a> (36)
</li>
		</ul>
</li><li id="archives-2" class="widget-container widget_archive"><h3 class="widget-title">文章归档</h3>		<ul>
	<li><a href='http://www.52nlp.cn/2015/01'>2015年一月</a></li>
	<li><a href='http://www.52nlp.cn/2014/12'>2014年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2014/11'>2014年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2014/09'>2014年九月</a></li>
	<li><a href='http://www.52nlp.cn/2014/07'>2014年七月</a></li>
	<li><a href='http://www.52nlp.cn/2014/06'>2014年六月</a></li>
	<li><a href='http://www.52nlp.cn/2014/05'>2014年五月</a></li>
	<li><a href='http://www.52nlp.cn/2014/04'>2014年四月</a></li>
	<li><a href='http://www.52nlp.cn/2014/01'>2014年一月</a></li>
	<li><a href='http://www.52nlp.cn/2013/12'>2013年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2013/06'>2013年六月</a></li>
	<li><a href='http://www.52nlp.cn/2013/05'>2013年五月</a></li>
	<li><a href='http://www.52nlp.cn/2013/04'>2013年四月</a></li>
	<li><a href='http://www.52nlp.cn/2013/03'>2013年三月</a></li>
	<li><a href='http://www.52nlp.cn/2013/02'>2013年二月</a></li>
	<li><a href='http://www.52nlp.cn/2013/01'>2013年一月</a></li>
	<li><a href='http://www.52nlp.cn/2012/12'>2012年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2012/11'>2012年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2012/10'>2012年十月</a></li>
	<li><a href='http://www.52nlp.cn/2012/09'>2012年九月</a></li>
	<li><a href='http://www.52nlp.cn/2012/08'>2012年八月</a></li>
	<li><a href='http://www.52nlp.cn/2012/07'>2012年七月</a></li>
	<li><a href='http://www.52nlp.cn/2012/06'>2012年六月</a></li>
	<li><a href='http://www.52nlp.cn/2012/05'>2012年五月</a></li>
	<li><a href='http://www.52nlp.cn/2012/04'>2012年四月</a></li>
	<li><a href='http://www.52nlp.cn/2012/03'>2012年三月</a></li>
	<li><a href='http://www.52nlp.cn/2012/01'>2012年一月</a></li>
	<li><a href='http://www.52nlp.cn/2011/12'>2011年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2011/11'>2011年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2011/10'>2011年十月</a></li>
	<li><a href='http://www.52nlp.cn/2011/09'>2011年九月</a></li>
	<li><a href='http://www.52nlp.cn/2011/08'>2011年八月</a></li>
	<li><a href='http://www.52nlp.cn/2011/07'>2011年七月</a></li>
	<li><a href='http://www.52nlp.cn/2011/06'>2011年六月</a></li>
	<li><a href='http://www.52nlp.cn/2011/05'>2011年五月</a></li>
	<li><a href='http://www.52nlp.cn/2011/04'>2011年四月</a></li>
	<li><a href='http://www.52nlp.cn/2011/03'>2011年三月</a></li>
	<li><a href='http://www.52nlp.cn/2011/02'>2011年二月</a></li>
	<li><a href='http://www.52nlp.cn/2011/01'>2011年一月</a></li>
	<li><a href='http://www.52nlp.cn/2010/12'>2010年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2010/11'>2010年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2010/10'>2010年十月</a></li>
	<li><a href='http://www.52nlp.cn/2010/09'>2010年九月</a></li>
	<li><a href='http://www.52nlp.cn/2010/08'>2010年八月</a></li>
	<li><a href='http://www.52nlp.cn/2010/07'>2010年七月</a></li>
	<li><a href='http://www.52nlp.cn/2010/06'>2010年六月</a></li>
	<li><a href='http://www.52nlp.cn/2010/05'>2010年五月</a></li>
	<li><a href='http://www.52nlp.cn/2010/04'>2010年四月</a></li>
	<li><a href='http://www.52nlp.cn/2010/03'>2010年三月</a></li>
	<li><a href='http://www.52nlp.cn/2010/02'>2010年二月</a></li>
	<li><a href='http://www.52nlp.cn/2010/01'>2010年一月</a></li>
	<li><a href='http://www.52nlp.cn/2009/12'>2009年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2009/11'>2009年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2009/10'>2009年十月</a></li>
	<li><a href='http://www.52nlp.cn/2009/09'>2009年九月</a></li>
	<li><a href='http://www.52nlp.cn/2009/08'>2009年八月</a></li>
	<li><a href='http://www.52nlp.cn/2009/07'>2009年七月</a></li>
	<li><a href='http://www.52nlp.cn/2009/06'>2009年六月</a></li>
	<li><a href='http://www.52nlp.cn/2009/05'>2009年五月</a></li>
	<li><a href='http://www.52nlp.cn/2009/04'>2009年四月</a></li>
	<li><a href='http://www.52nlp.cn/2009/03'>2009年三月</a></li>
	<li><a href='http://www.52nlp.cn/2009/02'>2009年二月</a></li>
	<li><a href='http://www.52nlp.cn/2009/01'>2009年一月</a></li>
	<li><a href='http://www.52nlp.cn/2008/12'>2008年十二月</a></li>
		</ul>
</li>		<li id="recent-posts-2" class="widget-container widget_recent_entries">		<h3 class="widget-title">最新文章</h3>		<ul>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e5%9b%9b%e7%ab%a0-combining-models">PRML读书会第十四章 Combining Models</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%89%e7%ab%a0sequential-data">PRML读书会第十三章 Sequential Data</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%ba%8c%e7%ab%a0-continuous-latent-variables">PRML读书会第十二章 Continuous Latent Variables</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%80%e7%ab%a0-sampling-methods">PRML读书会第十一章  Sampling Methods</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e7%ab%a0-approximate-inference">PRML读书会第十章  Approximate Inference</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b9%9d%e7%ab%a0-mixture-models-and-em">PRML读书会第九章  Mixture Models and EM</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ab%e7%ab%a0-graphical-models">PRML读书会第八章  Graphical Models</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%83%e7%ab%a0-sparse-kernel-machines">PRML读书会第七章 Sparse Kernel Machines</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ad%e7%ab%a0-kernel-methods">PRML读书会第六章   Kernel Methods</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%94%e7%ab%a0-neural-networks">PRML读书会第五章  Neural Networks</a>
						</li>
				</ul>
		</li><li id="recentcomments" class="widget-container widget_recentcomments"><h3 class="widget-title">最近评论</h3><ul><li class="rc-navi rc-clearfix"><span class="rc-loading">正在加载...</span></li><li id="rc-comment-temp" class="rc-item rc-comment rc-clearfix"><div class="rc-info"></div><div class="rc-timestamp"></div><div class="rc-excerpt"></div></li><li id="rc-ping-temp" class="rc-item rc-ping rc-clearfix"><span class="rc-label"></span></li></ul></li>			</ul>
		</div><!-- #primary .widget-area -->


		<div id="secondary" class="widget-area" role="complementary">
			<ul class="xoxo">
				<li id="linkcat-103" class="widget-container widget_links"><h3 class="widget-title">NLP相关网站</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://www.aclweb.org/" rel="co-worker" title="The Association for Computational Linguistics" target="_blank">ACL</a></li>
<li><a href="http://aclweb.org/anthology-new/" rel="co-worker" title="A Digital Archive of Research Papers in Computational Linguistics" target="_blank">ACL Anthology</a></li>
<li><a href="http://belobog.si.umich.edu/clair/anthology/index.cgi" rel="colleague" target="_blank">ACL Anthology Network</a></li>
<li><a href="http://aclweb.org/aclwiki/index.php?title=Main_Page" rel="colleague" title="the Wiki of the Association for Computational Linguistics" target="_blank">ACL Wiki</a></li>
<li><a href="http://www.clsp.jhu.edu/" rel="colleague" target="_blank">CLSP</a></li>
<li><a href="http://www.cwbbase.com/" rel="colleague" title="这是一个略具规模的中文语义词库, 也是稍有特色的汉语语义词典" target="_blank">CWB中文词库</a></li>
<li><a href="http://www.euromatrix.net/" rel="colleague" target="_blank">EuroMatrix</a></li>
<li><a href="http://www.freebase.com" rel="colleague" target="_blank">Freebase</a></li>
<li><a href="http://www.clsp.jhu.edu/workshops/" rel="colleague" target="_blank">JHU Workshop</a></li>
<li><a href="http://www.ldc.upenn.edu/" rel="colleague" title="Linguistic Data Consortium" target="_blank">LDC</a></li>
<li><a href="http://www.statmt.org/moses/" rel="colleague" title="A factored phrase-based beam-search decoder for machine translation" target="_blank">Moses</a></li>
<li><a href="http://nlpers.blogspot.com/" rel="colleague" title="国外一个非常不错的自然语言处理博客" target="_blank">nlper</a></li>
<li><a href="http://www.nlpjob.com" target="_blank">NLPJob</a></li>
<li><a href="http://www.powerset.com/" rel="colleague" target="_blank">Powerset</a></li>
<li><a href="http://www.speech.sri.com/projects/srilm/" rel="colleague" title="- The SRI Language Modeling Toolkit" target="_blank">SRILM</a></li>
<li><a href="http://www.statmt.org/" rel="colleague" title="This website is dedicated to research in statistical machine translation" target="_blank">Statistical Machine Translation</a></li>
<li><a href="http://textanalysisonline.com/" target="_blank">Text Analysis</a></li>
<li><a href="http://textminingonline.com/" target="_blank">Text Mining</a></li>
<li><a href="http://textsummarization.net/" target="_blank">Text Summarization</a></li>
<li><a href="http://w3china.org/index.htm" rel="friend" title="致力于促进W3C技术的广泛应用, 传播关于未来Web的知识与技术" target="_blank">中国万维网联盟</a></li>
<li><a href="http://www.cipsc.org.cn/" rel="co-worker" title="Chinese Information Processing Society of China" target="_blank">中国中文信息学会</a></li>
<li><a href="http://www.nlp.org.cn/" rel="colleague" title="中文自然语言处理开放平台" target="_blank">中文自然语言处理开放平台</a></li>
<li><a href="http://www.mt-archive.info/" rel="colleague" title="Repository and bibliography of articles, books and papers on topics" target="_blank">机器翻译档案计划</a></li>
<li><a href="http://www.statmt.org/europarl/" rel="colleague" target="_blank">欧洲议会平行语料库</a></li>
<li><a href="http://www.keenage.com/" title="HowNet" target="_blank">知网</a></li>
<li><a href="http://www.nlpir.org/" rel="friend" title="由张华平博士发起，由北京理工大学网络搜索与挖掘实验室运营，旨在推动NLP(自然语言处理)与IR(信息检索)领域的共享与共赢" target="_blank">自然语言处理与信息检索共享平台</a></li>
<li><a href="http://mitel.ict.ac.cn/" rel="co-worker" title="中科院计算所多语言交互技术实验室" target="_blank">计算所多语言交互技术实验室</a></li>

	</ul>
</li>
<li id="linkcat-2" class="widget-container widget_links"><h3 class="widget-title">友情链接</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://blog.youxu.info/" title="一个计算机专业的 Ph.D. 学生徐宥的个人博客" target="_blank">4G spaces</a></li>
<li><a href="http://blog.52nlp.org" rel="me" title="我爱自然语言处理完全镜像" target="_blank">52nlpblog</a></li>
<li><a href="http://www.52nlp.com" rel="me" title="52nlp的英文站" target="_blank">52nlpcom</a></li>
<li><a href="http://hi.baidu.com/drkevinzhang" rel="friend" title="ICTCLAS 张华平博士的空间" target="_blank">ICTCLAS 张华平博士的空间</a></li>
<li><a href="http://blog.so8848.com/" rel="friend" title="信息检索博客" target="_blank">Information Retrieval Blog</a></li>
<li><a href="http://interop123.com/default.aspx" rel="friend" title="崔晓源师兄关于NET技术的站点" target="_blank">NET互操作技术社区</a></li>
<li><a href="http://bbs.w3china.org/" rel="friend" title="中国万维网联盟讨论区" target="_blank">W3CHINA讨论区</a></li>
<li><a href="http://www.ailab.cn/" rel="friend" target="_blank">人工智能网</a></li>
<li><a href="http://mindhacks.cn/" rel="friend" title="一个很有思想的价值博客!" target="_blank">刘未鹏之Mind Hacks</a></li>
<li><a href="http://www.cnblogs.com/finallyliuyu/" rel="friend" target="_blank">原地转圈的驴子</a></li>
<li><a href="http://xunren.thuir.org/" target="_blank">微博寻人（梁博）</a></li>
<li><a href="http://52opencourse.com" rel="friend" title="我爱公开课，高质量公开课交流平台" target="_blank">我爱公开课</a></li>
<li><a href="http://iregex.org/" rel="friend" target="_blank">我爱正则表达式</a></li>
<li><a href="http://courseminer.com" target="_blank">挖课</a></li>
<li><a href="http://www.flickering.cn/" target="_blank">火光摇曳</a></li>
<li><a href="http://www.sciencenet.cn/u/timy/" rel="friend" title="章成志老师的博客" target="_blank">章成志的博客</a></li>
<li><a href="http://blog.csdn.net/v_JULY_v/" target="_blank">结构之法 算法之道</a></li>
<li><a href="http://www.lingcc.com/" rel="friend" title="关注编译器,虚拟机,编程语言及技术,IT职业和程序员生活" target="_blank">编译点滴</a></li>
<li><a href="http://www.52nlp.org" rel="me" title="52nlp的官方网站" target="_blank">自然语言处理</a></li>
<li><a href="http://www.ieee.org.cn/" rel="friend" title="计算机科学论坛" target="_blank">计算机科学论坛</a></li>
<li><a href="http://coursegraph.com/">课程图谱</a></li>
<li><a href="http://blog.coursegraph.com" rel="friend">课程图谱博客</a></li>

	</ul>
</li>
<li id="meta-4" class="widget-container widget_meta"><h3 class="widget-title">功能</h3>			<ul>
						<li><a href="http://www.52nlp.cn/wp-login.php">登录</a></li>
			<li><a href="http://www.52nlp.cn/feed">文章<abbr title="Really Simple Syndication">RSS</abbr></a></li>
			<li><a href="http://www.52nlp.cn/comments/feed">评论<abbr title="Really Simple Syndication">RSS</abbr></a></li>
<li><a href="https://cn.wordpress.org/" title="基于WordPress，一个优美、先进的个人信息发布平台。">WordPress.org</a></li>			</ul>
</li>			</ul>
		</div><!-- #secondary .widget-area -->

	</div><!-- #main -->

	<div id="footer" role="contentinfo">
		<div id="colophon">



			<div id="site-info">
				<a href="http://www.52nlp.cn/" title="我爱自然语言处理" rel="home">
					我爱自然语言处理				</a>
			</div><!-- #site-info -->

			<div id="site-generator">
								<a href="http://cn.wordpress.org/"
						title="优雅的个人发布平台" rel="generator">
					自豪地采用 WordPress。				</a>
			</div><!-- #site-generator -->

		</div><!-- #colophon -->
	</div><!-- #footer -->

</div><!-- #wrapper -->

<script>
/* <![CDATA[ */
var rcGlobal = {
	serverUrl		:'http://www.52nlp.cn',
	infoTemp		:'%REVIEWER% 在 %POST%',
	loadingText		:'正在加载',
	noCommentsText	:'没有任何评论',
	newestText		:'&laquo; 最新的',
	newerText		:'&laquo; 上一页',
	olderText		:'下一页 &raquo;',
	showContent		:'',
	external		:'',
	avatarSize		:'0',
	avatarPosition	:'left',
	anonymous		:'匿名'
};
/* ]]> */
</script>
<script type='text/javascript' src='http://www.52nlp.cn/wp-content/plugins/akismet/_inc/form.js?ver=3.0.4'></script>
<link rel='stylesheet' id='yarppRelatedCss-css'  href='http://www.52nlp.cn/wp-content/plugins/yet-another-related-posts-plugin/style/related.css?ver=4.0.1' type='text/css' media='all' />
<script type='text/javascript' src='http://www.52nlp.cn/wp-content/plugins/wp-recentcomments/js/wp-recentcomments.js?ver=2.2.7'></script>
	<p align="center"> 本站架设在 <a href="http://www.52nlp.cn/digitalocean%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0">DigitalOcean</a> 上, 采用创作共用版权协议, 要求署名、非商业用途和保持一致. 转载本站内容必须也遵循“署名-非商业用途-保持一致”的创作共用协议.</p>
<!-- Piwik -->
<script type="text/javascript">
  var _paq = _paq || [];
  _paq.push(["trackPageView"]);
  _paq.push(["enableLinkTracking"]);

  (function() {
    var u=(("https:" == document.location.protocol) ? "https" : "http") + "://162.243.252.121/piwik/";
    _paq.push(["setTrackerUrl", u+"piwik.php"]);
    _paq.push(["setSiteId", "5"]);
    var d=document, g=d.createElement("script"), s=d.getElementsByTagName("script")[0]; g.type="text/javascript";
    g.defer=true; g.async=true; g.src=u+"piwik.js"; s.parentNode.insertBefore(g,s);
  })();
</script>
<!-- End Piwik Code -->
</body>
</html>
