<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8" />
<title>PRML读书会第五章  Neural Networks | 我爱自然语言处理</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="stylesheet" type="text/css" media="all" href="http://www.52nlp.cn/wp-content/themes/twentytenorg/style.css" />
<link rel="pingback" href="http://www.52nlp.cn/xmlrpc.php" />
<link rel="alternate" type="application/rss+xml" title="我爱自然语言处理 &raquo; Feed" href="http://www.52nlp.cn/feed" />
<link rel="alternate" type="application/rss+xml" title="我爱自然语言处理 &raquo; 评论Feed" href="http://www.52nlp.cn/comments/feed" />
<link rel="alternate" type="application/rss+xml" title="我爱自然语言处理 &raquo; PRML读书会第五章  Neural Networks评论Feed" href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%94%e7%ab%a0-neural-networks/feed" />
<link rel='stylesheet' id='yarppWidgetCss-css'  href='http://www.52nlp.cn/wp-content/plugins/yet-another-related-posts-plugin/style/widget.css?ver=4.0.1' type='text/css' media='all' />
<link rel='stylesheet' id='codecolorer-css'  href='http://www.52nlp.cn/wp-content/plugins/codecolorer/codecolorer.css?ver=0.9.9' type='text/css' media='screen' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://www.52nlp.cn/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.52nlp.cn/wp-includes/wlwmanifest.xml" /> 
<link rel='prev' title='PRML读书会第四章 Linear Models for Classification' href='http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%9b%9b%e7%ab%a0-linear-models-for-classification' />
<link rel='next' title='PRML读书会第六章   Kernel Methods' href='http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ad%e7%ab%a0-kernel-methods' />
<meta name="generator" content="WordPress 4.0.1" />
<link rel='canonical' href='http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%94%e7%ab%a0-neural-networks' />
<link rel='shortlink' href='http://www.52nlp.cn/?p=7236' />
<!-- wp thread comment 1.4.9.4.002 -->
<style type="text/css" media="screen">
.editComment, .editableComment, .textComment{
	display: inline;
}
.comment-childs{
	border: 1px solid #999;
	margin: 5px 2px 2px 4px;
	padding: 4px 2px 2px 4px;
	background-color: white;
}
.chalt{
	background-color: #E2E2E2;
}
#newcomment{
	border:1px dashed #777;width:90%;
}
#newcommentsubmit{
	color:red;
}
.adminreplycomment{
	border:1px dashed #777;
	width:99%;
	margin:4px;
	padding:4px;
}
.mvccls{
	color: #999;
}
			
</style>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } },
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
});
</script><script type="text/javascript" src="http://www.52nlp.cn/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>

<body class="single single-post postid-7236 single-format-standard">
<div id="wrapper" class="hfeed">
	<div id="header">
		<div id="masthead">
			<div id="branding" role="banner">
								<div id="site-title">
					<span>
						<a href="http://www.52nlp.cn/" title="我爱自然语言处理" rel="home">我爱自然语言处理</a>
					</span>
				</div>
				<div id="site-description">I Love Natural Language Processing</div>

										<img src="http://www.52nlp.cn/wp-content/themes/twentytenorg/images/headers/path.jpg" width="940" height="198" alt="" />
								</div><!-- #branding -->

			<div id="access" role="navigation">
			  				<div class="skip-link screen-reader-text"><a href="#content" title="跳至正文">跳至正文</a></div>
								<div class="menu"><ul><li ><a href="http://www.52nlp.cn/">首页</a></li><li class="page_item page-item-2"><a href="http://www.52nlp.cn/about">关于</a></li><li class="page_item page-item-2557 page_item_has_children"><a href="http://www.52nlp.cn/resources">资源</a><ul class='children'><li class="page_item page-item-1271"><a href="http://www.52nlp.cn/resources/wpmatheditor">WpMathEditor</a></li></ul></li></ul></div>
 
				<div class="menu"><ul><li class="page_item page-item-2"></li><li class="page_item page-item-2"><a href="http://coursegraph.com" title="课程图谱" target="_blank">课程图谱</a></li><li class="page_item page-item-2"><a href="http://www.nlpjob.com" title="求职" target="_blank">求职招聘</a></li></ul></div>
			</div><!-- #access -->
		</div><!-- #masthead -->
	</div><!-- #header -->

	<div id="main">

		<div id="container">
			<div id="content" role="main">

			

				<div id="nav-above" class="navigation">
					<div class="nav-previous"><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%9b%9b%e7%ab%a0-linear-models-for-classification" rel="prev"><span class="meta-nav">&larr;</span> PRML读书会第四章 Linear Models for Classification</a></div>
					<div class="nav-next"><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ad%e7%ab%a0-kernel-methods" rel="next">PRML读书会第六章   Kernel Methods <span class="meta-nav">&rarr;</span></a></div>
				</div><!-- #nav-above -->

				<div id="post-7236" class="post-7236 post type-post status-publish format-standard hentry category-pattern-recognition-and-machine-learning-2 category-344 tag-back-propagation tag-bp tag-deep-learning tag-forward-propagation tag-neural-networks tag-pattern-recognition-and-machine-learning tag-prml tag-1021 tag-344 tag-961 tag-960 tag-483 tag-504 tag-1018 tag-513 tag-524 tag-1019 tag-1020">
					<h1 class="entry-title">PRML读书会第五章  Neural Networks</h1>

					<div class="entry-meta">
						<span class="meta-prep meta-prep-author">发表于</span> <a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%94%e7%ab%a0-neural-networks" title="21:33" rel="bookmark"><span class="entry-date">2015年01月27号</span></a> <span class="meta-sep">由</span> <span class="author vcard"><a class="url fn n" href="http://www.52nlp.cn/author/prml" title="查看所有由 prml 发布的文章">prml</a></span>					</div><!-- .entry-meta -->

					<div class="entry-content">
						<p style="text-align: center"><strong>PRML</strong><strong>读书会第五章 Neural Networks</strong></p>
<p style="text-align: center"><strong>主讲人 网神</strong></p>
<p style="text-align: center"><strong>（新浪微博:<a href="http://weibo.com/ghtimaq">@豆角茄子麻酱凉面</a>）</strong></p>
<p>网神(66707180) 18:55:06</p>
<p>那我们开始了啊，前面第3,4章讲了回归和分类问题，他们应用的主要限制是维度灾难问题。今天的第5章神经网络的内容：<br />
1. 神经网络的定义<br />
2. 训练方法：error函数，梯度下降，后向传导<br />
3. 正则化：几种主要方法，重点讲卷积网络</p>
<p>书上提到的这些内容今天先不讲了，以后有时间再讲：BP在Jacobian和Hessian矩阵中求导的应用；<br />
混合密度网络；贝叶斯解释神经网络。</p>
<p>首先是神经网络的定义，先看一个最简单的神经网络，只有一个神经元：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-0.png"><img class="aligncenter wp-image-7237" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-0.png" alt="prml5-0" width="400" height="179" /></a></p>
<p>这个神经元是一个以x1,x2,x3和截距1为输入的运算单元，其输出是：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-1.png"><img class="aligncenter wp-image-7238" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-1.png" alt="prml5-1" width="350" height="22" /></a></p>
<p>其中函数f成为&#8221;激活函数&#8221; , activation function.激活函数根据实际应用确定，经常选择sigmoid函数.如果是sigmoid函数，这个神经元的输入-输出的映射就是一个logistic回归问题。</p>
<p><span id="more-7236"></span></p>
<p>神经网络就是将许多个神经元连接在一起组成的网络，如图：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-2.png"><img class="aligncenter wp-image-7239" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-2.png" alt="prml5-2" width="400" height="361" /></a></p>
<p>神经网络的图跟后面第八章图模型不同， 图模型中，每个节点都是一个随机变量，符合某个分布。神经网络中的节点是确定的值，由与其相连的节点唯一确定。</p>
<p>上图这个神经网络包含三层，左边是输入层，x1&#8230;xd节点是输入节点， x0是截距项。最右边是输出层，y1,&#8230;,yk是输出节点. 中间是隐藏层hidden level, z1,&#8230;,zM是隐藏节点，因为不能从训练样本中观察到他们的值，所以叫隐藏层。</p>
<p>可以把神经网络描述为一系列的函数转换。首先，构造M个输入节点的线性组合：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-5.png"><img class="aligncenter wp-image-7240" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-5.png" alt="prml5-5" width="200" height="59" /></a></p>
<p>上式中，j=1,&#8230;,M，对应M个隐藏节点. 上标(1)表示这是第一层的参数。wji是权重weight,wj0是偏置.  aj 叫做activation. 中文叫激活吧，感觉有点别扭。</p>
<p>把activation aj输入到一个非线性激活函数h(.) <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-6.png"><img class="alignnone wp-image-7241" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-6.png" alt="prml5-6" width="78" height="20" /></a>就得到了隐藏节点的值zj。</p>
<p>在这个从输入层到隐藏层的转换中，这个激活函数不能是线性的，接下来，将隐藏节点的值线性组合得到输出节点的activation：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-8.png"><img class="aligncenter wp-image-7242" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-8.png" alt="prml5-8" width="200" height="61" /></a></p>
<p>每个ak输入一个输出层激活函数<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-9.png"><img class="alignnone wp-image-7243" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-9.png" alt="prml5-9" width="86" height="20" /></a>，就得到了输出值。</p>
<p>这个从隐藏层到输出层的激活函数σ,根据不同应用，有不同的选择，例如回归问题的激活函数是identity函数,既y = a.分类问题的激活函数选择sigmoid函数，multiclass分类选择是softmax函数.</p>
<p>把上面各阶段的计算连起来，神经网络整个的计算过程就是：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-11.png"><img class="aligncenter wp-image-7244" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-11.png" alt="prml5-11" width="400" height="64" /></a></p>
<p>上面计算过程是以2层神经网络为例。实际使用的NN可能有多层，记得听一个报告现在很火的deep learning的隐藏层有5-9层.这个计算过程forward propagation，从前向后计算。与此相反，训练时，会用back propagation从后向前来计算偏导数。</p>
<p>神经网络有很强的拟合能力，可以拟合很多种的曲线，这个图是书上的一个例子，对四种曲线的拟合：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-13.png"><img class="aligncenter wp-image-7245" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-13.png" alt="prml5-13" width="400" height="233" /></a></p>
<p>第一部分神经网络的定义就这么多，大家有没有什么问题啊？</p>
<p>============================讨论=================================</p>
<p>阳阳(236995728) 19:20:43</p>
<p>输入层到隐藏层使用的激活函数与隐藏层到输出层的激活函数是否要一致？</p>
<p>网神(66707180) 19:21:11</p>
<p>两层激活函数不一定一致，输入层到隐藏层 经常是sigmoid函数。 而隐藏层到输出层，根据应用不同，选择不同.</p>
<p>牧云(1106207961) 19:22:30</p>
<p>一般的算法，比如神经网络，都有分类和拟合的功能，分类和拟合有共同点吗？为什么能拟合，这个问题我没有找到地方清晰的解释。</p>
<p>独孤圣者(303957511) 19:23:47</p>
<p>拟合和分类，在我看来实际上是一样的，分类无非是只有2个或多个值的拟合而已。2个值的拟合，被称为二值分类</p>
<p>ant/wxony(824976094) 19:24:30</p>
<p>不是吧，svm做二值分类，就不是以拟合为目标。二值拟合可以用做分类。</p>
<p>独孤圣者(303957511) 19:29:42</p>
<p>可以作为概率来理解吧，我个人认为啊，sigmoid函数，是将分类结果做一个概率的计算。</p>
<p>阳阳(236995728) 19:24:07</p>
<p>输入层到隐藏层 经常是sigmoid函数 那么也就是特征值变成了【0-1】之间的数了？</p>
<p>网神(66707180) 19:25:19</p>
<p>是的，我认为是.机器学习经常需要做特征归一化，也是把特征归一化到[0，1]范围。当然这里不只是归一化。</p>
<p>阳阳(236995728) 19:26:56</p>
<p>这里应该不是归一化的作用@网神</p>
<p>网神(66707180) 19:27:33</p>
<p>嗯，不是归一化，我觉得神经网络的这些做法，没有一个科学的解释。不像SVM每一步都有严谨的理论。</p>
<p>罗杰兔(38900407) 19:29:00</p>
<p>参数W,b是算出来的，还是有些技巧得到的。因为常听人说，神经网络难就难在调试参数上。</p>
<p>阳阳(236995728) 19:29:11</p>
<p>我觉得神经网络也是在学习新的特征 而这些特征比较抽象难于理解@牧云 @网神</p>
<p>网神(66707180) 19:29:48</p>
<p>对，我理解隐藏层的目的就是学习特征</p>
<p>阳阳(236995728) 19:30:24</p>
<p>是的 但是这些特征难于理解较抽象@网神</p>
<p>网神(66707180) 19:30:47</p>
<p>最后一个隐藏层到输出层之间，就是做分类或回归。前面的不同隐藏层，则是提取特征。</p>
<p>独孤圣者(303957511) 19:30:52</p>
<p>隐藏层的目的就是学习特征，这个表示赞同</p>
<p>网神(66707180) 19:31:33</p>
<p>后面讲到卷积网络时，可以很明显感受到，隐藏层的目的就是学习特征。</p>
<p>阳阳(236995728) 19:31:59</p>
<p>但是我不理解为什么隐藏层学习到的特征值介于【0-1】之间是合理的？是归一化的作用？</p>
<p>牧云(1106207961) 19:32:47</p>
<p>归一化映射</p>
<p>Wilbur_中博(1954123) 19:33:59</p>
<p>要归一化也是对每一个输入变量做吧。。各个输入变量都组合在一起了，归一化干嘛。我理解就是一个非线性变换，让神经网络具有非线性能力。sigmoid之外，其他非线性变换用的也蛮多的。和归一化没什么关系。不一定要在[0, 1]之间。</p>
<p>独孤圣者(303957511) 19:39:50</p>
<p>这个解释我很同意，tanh函数也经常替换sigmoid函数，在神经网络中普遍使用。</p>
<p>网神(66707180) 19:39:59</p>
<p>嗯，让NN具有非线性能力是主要原因之一。</p>
<p>牧云(1106207961) 19:40:03</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-14.png"><img class="aligncenter wp-image-7246" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-14.png" alt="prml5-14" width="400" height="208" /></a></p>
<p>这个1要的干嘛</p>
<p>网神(66707180) 19:40:18</p>
<p>这个是偏置项，y = wx+b， 那个+1就是为了把b 合入w,变成　ｙ＝ｗｘ</p>
<p>罗杰兔(38900407) 19:41:36</p>
<p>Ng好象讲b是截距</p>
<p>网神(66707180) 19:43:16</p>
<p>截距和偏执项是一个意思。 放在坐标系上，b就是截距</p>
<p>阳阳(236995728) 19:40:33</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-15.png"><img class="alignnone wp-image-7247" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-15.png" alt="prml5-15" width="300" height="40" /></a>它可以逼近任意的非线性函数吗@网神</p>
<p>网神(66707180) 19:42:13</p>
<p>书上没说可以逼近任意曲线，说的是逼近很多曲线</p>
<p>阳阳(236995728) 19:42:58</p>
<p>它到底是怎么逼近的啊@网神  我想看看多个函数叠加的？</p>
<p>网神(66707180) 19:44:30</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-16.png"><img class="aligncenter wp-image-7248" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-16.png" alt="prml5-16" width="400" height="297" /></a><br />
就是多个函数叠加。这个图上那个抛物线就是最终的曲线，由另外三条曲线叠加而成，另外那三条，每条是一个隐藏节点生成的曲线。</p>
<p>阳阳(236995728) 19:45:05</p>
<p>这三条曲线用的函数形式是不是一样的</p>
<p>zeno(117127143) 19:45:31</p>
<p>理解nn怎么逼近xor函数，就知道怎么非线性的了</p>
<p>HX(458728037) 19:45:56</p>
<p>问一句，你现在讲的是BP网络吧？</p>
<p>网神(66707180) 19:46:07</p>
<p>是BP网络</p>
<p>========================讨论结束===============================</p>
<p>网神(66707180) 19:46:59</p>
<p>接下来是神经网络的训练。神经网络训练的套路与第三，四章相同：确定目标变量的分布函数和似然函数，<br />
取负对数，得到error函数，用梯度下降法求使error函数最小的参数w和b。</p>
<p>NN的应用有：回归、binary分类、K个独立binary分类、multiclass分类。不同的应用决定了不同的激活函数、不同的目标变量的条件分布，不同的error函数.</p>
<p>首先，对于回归问题，输出变量t的条件概率符合高斯分布：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-17.png"><img class="aligncenter wp-image-7250" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-17.png" alt="prml5-17" width="300" height="41" /></a></p>
<p>因此，给定一个训练样本集合，其似然函数是：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-18.png"><img class="aligncenter wp-image-7251" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-18.png" alt="prml5-18" width="300" height="59" /></a></p>
<p>error函数是对似然函数取 负对数，得到：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-19.png"><img class="aligncenter wp-image-7252" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-19.png" alt="prml5-19" width="300" height="59" /></a></p>
<p>最小化这个error函数，只需要最小化第一项，所以回归问题的errro函数定义是：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-20.png"><img class="aligncenter wp-image-7253" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-20.png" alt="prml5-20" width="300" height="58" /></a></p>
<p>回归问题的隐藏层到输出层的激活函数是identity函数，也就是 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-21.png"><img class="alignnone wp-image-7254" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-21.png" alt="prml5-21" width="70" height="23" /></a></p>
<p>ak就是对隐藏层节点的线性组合：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-22.png"><img class="aligncenter wp-image-7255" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-22.png" alt="prml5-22" width="300" height="83" /></a></p>
<p>这样有了error函数，就可以同梯度下降法求极值点.</p>
<p>再说说binary分类和multiclass分类的error函数：</p>
<p>binary分类的目标变量条件分布式伯努利分布：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-24.png"><img class="aligncenter wp-image-7256" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-24.png" alt="prml5-24" width="300" height="42" /></a></p>
<p>这样，通过对似然函数取负对数，得到error函数如下：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-25.png"><img class="aligncenter wp-image-7257" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-25.png" alt="prml5-25" width="300" height="49" /></a></p>
<p>另外binary分类，隐藏层到输出层的激活函数是sigmoid函数。对multiclass问题，也就是结果是K个互斥的类别中的一个.类似思路可以得到其error函数.这里就不说了。书上都有.</p>
<p>知道了error函数，和激活函数，这里先做一个计算，为后面的BP做准备，将E(w)对激活ak求导，可得：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-28.png"><img class="aligncenter wp-image-7258" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-28.png" alt="prml5-28" width="200" height="73" /></a></p>
<p>这里有意思的是，不管是回归、binary分类还是multiclass分类，尽管其error函数不同，这个求导得出的结果都是yk-tk</p>
<p>回归问题这个求导很明显，下式中，y = a, 所以对a求导就是 y &#8211; t.</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-29.png"><img class="aligncenter wp-image-7259" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-29.png" alt="prml5-29" width="300" height="67" /></a></p>
<p>其他错误函数，需要推导一下，这里就不推了。有了error函数，用梯度下降法求其极小值点。</p>
<p>因为输入层到隐藏层的激活函数是非线性函数，所以error函数是非凸函数，所以可能存在很多个局部极值点。</p>
<p>两栖动物(9219642) 20:07:40</p>
<p>你这里说的回归问题是用神经网络拟合随机变量？</p>
<p>网神(66707180) 20:07:56</p>
<p>是的，就是做线性回归要做的事.只是用神经网络的方法，可以拟合更复杂的曲线。这是我的理解。</p>
<p>梯度下降法的思路就是对error函数求导，然后按下式对参数作调整：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-30.png"><img class="aligncenter wp-image-7260" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-30.png" alt="prml5-30" width="300" height="46" /></a></p>
<p>一直到导数为0，就找到了极值点.</p>
<p>这是基本思路，实际上很多算法，如共轭梯度法，拟牛顿法等，可以更有效的寻找极值点。因为梯度下降是机器学习中求极值点的常用方法，这些算法不是本章的内容，就不讲了，有专门的章节讲这些算法。</p>
<p>这里要讲的是error函数对参数w的求导方法，也就是back propagation后向传导法。<br />
可以高效的计算这个导数。</p>
<p>这个导数的计算利用的 求导数的链式法则， <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-31.png"><img class="alignnone wp-image-7261" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-31.png" alt="prml5-31" width="150" height="47" /></a></p>
<p>把E对w的求导，转换成E对activation a的求导 和 a 对w 的求导. 这个地方因为涉及的式子比较多，我在纸上写了一下，我整个贴上吧：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-32.png"><img class="aligncenter wp-image-7262" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-32.png" alt="prml5-32" width="500" height="500" /></a></p>
<p>大家看看，可以讨论一下</p>
<p>阳阳(236995728) 20:21:30</p>
<p>error function是负对数似然函数吧</p>
<p>网神(66707180) 20:21:38</p>
<p>是的</p>
<p>天上的月亮(785011830) 20:22:35</p>
<p>更新hidden层的w，为什么利用偏导的chain rule呢？</p>
<p>网神(66707180) 20:22:49</p>
<p>最主要的结论就是：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-35.png"><img class="wp-image-7264 aligncenter" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-35.png" alt="prml5-35" width="200" height="55" /></a></p>
<p>对于输出层的节点，</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-36.png"><img class="alignnone size-full wp-image-7265" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-36.png" alt="prml5-36" width="31" height="31" /></a> <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-37.png"><img class="alignnone size-full wp-image-7266" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-37.png" alt="prml5-37" width="54" height="31" /></a></p>
<p>对于隐藏层的节点，</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-38.png"><img class="alignnone size-full wp-image-7267" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-38.png" alt="prml5-38" width="31" height="31" /></a> <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-39.png"><img class="alignnone size-full wp-image-7268" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-39.png" alt="prml5-39" width="99" height="31" /></a></p>
<p>E对隐藏层节点w的导数，通过上式，就由输出层节点的导数 求得了.</p>
<p>有了上面求偏导数的方法，整个训练过程就是：<br />
1.输入一个样本Xn,根据forward propagation得到每个hidden单元和output单元的激活值a.<br />
2.计算每个output单元的偏导数<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-40.png"><img class="alignnone wp-image-7269" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-40.png" alt="prml5-40" width="30" height="27" /></a></p>
<p>3.根据反向传导公式，计算每个hidden单元的<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-42.png"><img class="alignnone wp-image-7270" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-42.png" alt="prml5-42" width="25" height="30" /></a></p>
<p>4.计算偏导数。<br />
5.用一定的步长更新参数w。<br />
循环往复，一直找到满意的w。</p>
<p>独孤圣者(303957511) 20:32:09</p>
<p>BP时，是不是每个样本都要反馈一次？</p>
<p>网神(66707180) 20:33:32</p>
<p>应该是每个样本都反馈一次，batch也是.batch形式的每个样本反馈一次，将每个样本的偏导数求和，如下式，得到所有样本的偏导数，然后做一次参数w的调整.</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-43.png"><img class="aligncenter wp-image-7271" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-43.png" alt="prml5-43" width="250" height="93" /></a></p>
<p>接下来就讲正则化，神经网络参数多，训练复杂，所以正则化方法也被研究的很多，书上讲了很多种的正则化方法. 第一个，就是在error函数上，加上正则项。<br />
第三章回归的正则项如下：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-45.png"><img class="aligncenter wp-image-7272" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-45.png" alt="prml5-45" width="350" height="45" /></a></p>
<p>后面那一项对过大的w做惩罚。</p>
<p>但是对神经网络，正则项需要满足 一个缩放不变性，所以正则项的形式如下：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-48.png"><img class="alignnone wp-image-7273 size-full" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-48.png" alt="prml5-48" width="84" height="25" /></a> <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-49.png"><img class="alignnone wp-image-7274 size-full" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-49.png" alt="prml5-49" width="107" height="28" /></a></p>
<p>这个形式怎么推导出来的，大家看书吧，这里不说了。</p>
<p>书上讲到的第二种正则化方法是early stop，我理解的思路就是 在训练集上每调整一次w, 得到新的参数，就在验证集上验证一下。 开始在验证集上的error是逐渐下降的，后来就不下降了，会上升。那么在开始上升之前，就停止训练，那个参数就是最佳情况。</p>
<p>忘了说一个正则化注意对象了，就是隐藏节点的数量M。这个M是预先人为确定的，他的大小决定了整个模型的参数多少。所以是影响 欠拟合还是过拟合的 关键因素。</p>
<p>monica(909117539) 20:42:38</p>
<p>开始上升是出现了过拟合么？节点数量和层数都是怎样选择的呀？</p>
<p>网神(66707180) 20:42:56</p>
<p>开始上升就是过拟合了，是的，在训练集上，error还是下降的，但验证集上已经上升了。节点数量M是人为确定，我想这个数怎么定，是NN调参的重点。</p>
<p>monica(909117539) 20:44:36</p>
<p>：）</p>
<p>网神(66707180) 20:44:38</p>
<p>ML牛的人，叫做 调得一手好参，我觉得这里最能体现。</p>
<p>这个图是不同的M值对拟合情况的影响：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-50.png"><img class="aligncenter wp-image-7275" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-50.png" alt="prml5-50" width="400" height="178" /></a></p>
<p>另外，因为NN又不是凸函数，导致train更麻烦</p>
<p>monica(909117539) 20:47:01</p>
<p>嗯，这个能理解，前面模型太简单，欠拟合了，后面模型太复杂，就过拟合了。</p>
<p>网神(66707180) 20:47:08</p>
<p>需要尝试不同的M值，对于每一个M值，又需要train多次，每次随即初始化w的值，然后尝试找到不同的局部极值点。</p>
<p>上面说了三种正则化方法。接下来，因为神经网络在图像识别中，应用广泛，所以图像识别对正则化有些特殊的需求，就是 平移、缩放、旋转不变性。不知道这个需求在其他应用中，是否存在，例如NLP中，反正书上都是以图像处理为例讲的.</p>
<p>这些不变形，就是图像中的物体位置移动、大小变化、或者一定程度的旋转， 对ouput层的结果应该没有影响。</p>
<p>这里主要讲卷积神经网络。这个东东好像很有用，我看在Ng的deep learning教程中也重点讲了，当图像是大图时，如100&#215;100像素以上，卷积网络可以大大减少参数w的个数.</p>
<p>卷积网络的input层的unit是图像的像素值.卷积网络的hidden层由卷积层和子采样层组成，如图：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-51.png"><img class="aligncenter wp-image-7276" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-51.png" alt="prml5-51" width="400" height="191" /></a></p>
<p>一个卷及网络可能包括多个卷积层和子采样层，如下图有两对卷积/子采样层：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-53.png"><img class="aligncenter wp-image-7277" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-53.png" alt="prml5-53" width="400" height="203" /></a></p>
<p>卷积层的units被划分成多个planes，每个plane是一个feature map，一个feature map里的一个unit只连接input层的一个小区域，一个feature map里的所有unit使用相同的weight值。<br />
卷积层的作用可以认为是提取特征。每个plane就是一个特征滤波器，通过卷积的方式提取图像的一种特征，过滤掉不是本特征的信息。比如提取100种特征，就会有100个plane，上图中，提取6个特征，第二层有6个平面。<br />
一个plane里面的一个unit与图像的一个子区域相连，unit的值表示从该区域提取的一个特征值。共有多少个子区域，plane里就有多少个unit。<br />
以上图为例，输入图像是32&#215;32像素，所以input层有32&#215;32个unit. 取子区域的大小为4&#215;4，，那么共有28&#215;28个子区域。每个子区域提取得到6个特征，对应第二层有6个plane， 每个plane有28&#215;28个unit.</p>
<p>这样一个plane里的一个unit与输入层的4&#215;4个unit相连，有16个权重weight值，这个plane里的其他unit都公用16个weight值.<br />
这样，6个平面，共有16&#215;6=96个weight参数和6个bias参数.</p>
<p>下面说子采样层，上面说道，6个平面，每个平面有28&#215;28个unit，所以第二层共有6x28x28个unit. 这是从图像中提取出来的特征，作为后一层的输入，用于分类或回归。<br />
但实际中，6个特征远远不够，图像也不一定只有32&#215;32像素。假如从96&#215;96像素中提取400个特征，子区域还是4&#215;4，则第二层的unit会有 400 x (96-4) x (96-4) = 300多万。 超过300多万输入的分类器很难训练，也容易过拟合。<br />
这就通过子采样来减少特征。子采样就是把卷积层的一个平面划分成几部分，每个部分取最大值或平均值，作为子采样层的一个unit值.如下图：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-55.png"><img class="aligncenter wp-image-7278" src="http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-55.png" alt="prml5-55" width="400" height="201" /></a></p>
<p>左边表示卷积层的一个plane，右边是这个plane的子采样结果。一个格子表示一个unit。<br />
把左边整个plane分成4部分，互相不重合，每个部分取最大值或平均值，作为右边子采样层的一个unit。这样子采样层一个plane只有4个unit。</p>
<p>通过卷积和子采样：得到的特征作为下一层分类或回归的输入。<br />
主要好处：<br />
a. 这样输入层的各种变形在后面就会变得不敏感。如果有多对 卷积/子采样，每一对就将不变形更深入一层。<br />
b. 减少参数的个数。就这么多了，大家慢慢看。</p>
<p>阳阳(236995728) 21:01:30</p>
<p>关于卷积神经网络的材料有吗？@网神</p>
<p>阿邦(1549614810) 21:01:44</p>
<p>问个问题，cnn怎么做的normalization？</p>
<p>网神(66707180) 21:03:26</p>
<p>卷积网络的材料，我看了书上的，还有Andrew Ng的deep learning里讲的，另外网上搜了一篇文章</p>
<p>这里我贴下链接。</p>
<p>罗杰兔(38900407) 21:04:19</p>
<p>不大明白为什么可以采样，怎么保证采样得到的信息正好是我们需要的</p>
<p>阿邦(1549614810) 21:05:01</p>
<p>采样是max pooling，用于不变性</p>
<p>网神(66707180) 21:05:02</p>
<p>http://ibillxia.github.io/blog/2013/04/06/Convolutional-Neural-Networks/</p>
<p>牧云(1106207961) 21:05:02</p>
<p>采样是随机的吗？</p>
<p>网神(66707180) 21:06:40</p>
<p>传了个deep learning教程。这是Andrew Ng上课的notes, 网上一些人众包翻译的，我觉得翻译的不错。里面讲了神经网络、卷积网络，和其他深度网络的东西。</p>
<p>采样不是随机的。</p>
<p>牧云(1106207961) 21:08:04</p>
<p>卷积提取的特征具有稳定性吗</p>
<p>阿邦(1549614810) 21:08:56</p>
<p>据说要数据量很大才可以</p>
<p>网神(66707180) 21:09:03</p>
<p>卷积层和子采样层主要是 解决 不变性问题 和减少参数w数量问题，所以可以起到泛化作用.<br />
注：PRML读书会系列文章由 <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a> 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。</p>
<p>PRML读书会讲稿PDF版本以及更多资源下载地址：<a href="http://vdisk.weibo.com/u/1841149974">http://vdisk.weibo.com/u/1841149974</a></p>
<p>本文链接地址：<a href="http://www.52nlp.cn/prml读书会第五章-neural-networks">http://www.52nlp.cn/prml读书会第五章-neural-networks</a></p>
<div class='yarpp-related'>
<p>相关文章:<ol>
<li><a href="http://www.52nlp.cn/%e6%96%af%e5%9d%a6%e7%a6%8f%e5%a4%a7%e5%ad%a6%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ac%ac%e5%85%ab%e8%af%be%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e8%a1%a8%e7%a4%baneural-networks-re" rel="bookmark" title="斯坦福大学机器学习第八课“神经网络的表示(Neural Networks: Representation)”">斯坦福大学机器学习第八课“神经网络的表示(Neural Networks: Representation)” </a></li>
<li><a href="http://www.52nlp.cn/%e6%96%af%e5%9d%a6%e7%a6%8f%e5%a4%a7%e5%ad%a6%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ac%ac%e4%b9%9d%e8%af%be%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e5%ad%a6%e4%b9%a0neural-networks-le" rel="bookmark" title="斯坦福大学机器学习第九课“神经网络的学习(Neural Networks: Learning)”">斯坦福大学机器学习第九课“神经网络的学习(Neural Networks: Learning)” </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ab%e7%ab%a0-graphical-models" rel="bookmark" title="PRML读书会第八章  Graphical Models">PRML读书会第八章  Graphical Models </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%9b%9b%e7%ab%a0-linear-models-for-classification" rel="bookmark" title="PRML读书会第四章 Linear Models for Classification">PRML读书会第四章 Linear Models for Classification </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e5%9b%9b%e7%ab%a0-combining-models" rel="bookmark" title="PRML读书会第十四章 Combining Models">PRML读书会第十四章 Combining Models </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%80%e7%ab%a0-introduction" rel="bookmark" title="PRML读书会第一章  Introduction">PRML读书会第一章  Introduction </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%83%e7%ab%a0-sparse-kernel-machines" rel="bookmark" title="PRML读书会第七章 Sparse Kernel Machines">PRML读书会第七章 Sparse Kernel Machines </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ad%e7%ab%a0-kernel-methods" rel="bookmark" title="PRML读书会第六章   Kernel Methods">PRML读书会第六章   Kernel Methods </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e5%89%8d%e8%a8%80" rel="bookmark" title="PRML读书会前言">PRML读书会前言 </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%ba%8c%e7%ab%a0-continuous-latent-variables" rel="bookmark" title="PRML读书会第十二章 Continuous Latent Variables">PRML读书会第十二章 Continuous Latent Variables </a></li>
</ol></p>
</div>
											</div><!-- .entry-content -->


					<div class="entry-utility">
						此条目发表在 <a href="http://www.52nlp.cn/category/pattern-recognition-and-machine-learning-2" rel="category tag">PRML</a>, <a href="http://www.52nlp.cn/category/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" rel="category tag">机器学习</a> 分类目录，贴了 <a href="http://www.52nlp.cn/tag/back-propagation" rel="tag">back propagation</a>, <a href="http://www.52nlp.cn/tag/bp%e7%bd%91%e7%bb%9c" rel="tag">BP网络</a>, <a href="http://www.52nlp.cn/tag/deep-learning" rel="tag">Deep Learning</a>, <a href="http://www.52nlp.cn/tag/forward-propagation" rel="tag">forward propagation</a>, <a href="http://www.52nlp.cn/tag/neural-networks" rel="tag">neural networks</a>, <a href="http://www.52nlp.cn/tag/pattern-recognition-and-machine-learning" rel="tag">Pattern Recognition And Machine Learning</a>, <a href="http://www.52nlp.cn/tag/prml" rel="tag">PRML</a>, <a href="http://www.52nlp.cn/tag/prml%e8%af%bb%e4%b9%a6%e4%bc%9a" rel="tag">PRML读书会</a>, <a href="http://www.52nlp.cn/tag/%e5%8d%b7%e7%a7%af%e7%bd%91%e7%bb%9c" rel="tag">卷积网络</a>, <a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" rel="tag">机器学习</a>, <a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b9%a6%e7%b1%8d" rel="tag">机器学习书籍</a>, <a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e8%af%bb%e4%b9%a6%e4%bc%9a" rel="tag">机器学习读书会</a>, <a href="http://www.52nlp.cn/tag/%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d" rel="tag">梯度下降</a>, <a href="http://www.52nlp.cn/tag/%e6%ad%a3%e5%88%99%e5%8c%96" rel="tag">正则化</a>, <a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0" rel="tag">深度学习</a>, <a href="http://www.52nlp.cn/tag/%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" rel="tag">神经网络</a>, <a href="http://www.52nlp.cn/tag/%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%ad%a6%e4%b9%a0" rel="tag">神经网络学习</a>, <a href="http://www.52nlp.cn/tag/%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%ae%9a%e4%b9%89" rel="tag">神经网络定义</a>, <a href="http://www.52nlp.cn/tag/%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%ae%ad%e7%bb%83" rel="tag">神经网络训练</a> 标签。将<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%94%e7%ab%a0-neural-networks" title="链向 PRML读书会第五章  Neural Networks 的固定链接" rel="bookmark">固定链接</a>加入收藏夹。											</div><!-- .entry-utility -->
				</div><!-- #post-## -->

				<div id="nav-below" class="navigation">
					<div class="nav-previous"><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%9b%9b%e7%ab%a0-linear-models-for-classification" rel="prev"><span class="meta-nav">&larr;</span> PRML读书会第四章 Linear Models for Classification</a></div>
					<div class="nav-next"><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ad%e7%ab%a0-kernel-methods" rel="next">PRML读书会第六章   Kernel Methods <span class="meta-nav">&rarr;</span></a></div>
				</div><!-- #nav-below -->

				
			<div id="comments">




								<div id="respond" class="comment-respond">
				<h3 id="reply-title" class="comment-reply-title">发表评论 <small><a rel="nofollow" id="cancel-comment-reply-link" href="/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%94%e7%ab%a0-neural-networks#respond" style="display:none;">取消回复</a></small></h3>
									<form action="http://www.52nlp.cn/wp-comments-post.php" method="post" id="commentform" class="comment-form">
																			<p class="comment-notes">电子邮件地址不会被公开。 必填项已用<span class="required">*</span>标注</p>							<p class="comment-form-author"><label for="author">姓名 <span class="required">*</span></label> <input id="author" name="author" type="text" value="" size="30" aria-required='true' /></p>
<p class="comment-form-email"><label for="email">电子邮件 <span class="required">*</span></label> <input id="email" name="email" type="text" value="" size="30" aria-required='true' /></p>
<p class="comment-form-url"><label for="url">站点</label> <input id="url" name="url" type="text" value="" size="30" /></p>
												<p class="comment-form-comment"><label for="comment">评论</label> <textarea id="comment" name="comment" cols="45" rows="8" aria-required="true"></textarea></p>						<p class="form-allowed-tags">您可以使用这些<abbr title="HyperText Markup Language">HTML</abbr>标签和属性： <code>&lt;a href=&quot;&quot; title=&quot;&quot;&gt; &lt;abbr title=&quot;&quot;&gt; &lt;acronym title=&quot;&quot;&gt; &lt;b&gt; &lt;blockquote cite=&quot;&quot;&gt; &lt;cite&gt; &lt;code&gt; &lt;del datetime=&quot;&quot;&gt; &lt;em&gt; &lt;i&gt; &lt;q cite=&quot;&quot;&gt; &lt;strike&gt; &lt;strong&gt; </code></p>						<p class="form-submit">
							<input name="submit" type="submit" id="submit" value="发表评论" />
							<input type='hidden' name='comment_post_ID' value='7236' id='comment_post_ID' />
<input type='hidden' name='comment_parent' id='comment_parent' value='0' />
						</p>
						<p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="57b5164308" /></p><p style="display: none;"><input type="hidden" id="ak_js" name="ak_js" value="56"/></p><p><input type="hidden" id="comment_reply_ID" name="comment_reply_ID" value="0" /><input type="hidden" id="comment_reply_dp" name="comment_reply_dp" value="0" /></p><div id="cancel_reply" style="display:none;"><a href="javascript:void(0)" onclick="movecfm(null,0,1,null);" style="color:red;">点击取消回复</a></div><script type="text/javascript">
/* <![CDATA[ */
var commentformid = "commentform";
var USERINFO = false;
var atreply = "none";
/* ]]> */
</script>
<script type="text/javascript" src="http://www.52nlp.cn/wp-content/plugins/wp-thread-comment/wp-thread-comment.js.php?jsver=common"></script>
					</form>
							</div><!-- #respond -->
			
</div><!-- #comments -->


			</div><!-- #content -->
		</div><!-- #container -->

﻿
		<div id="primary" class="widget-area" role="complementary">
			<ul class="xoxo">
<!-- begin l_sidebar -->
	<div id="l_sidebar">
<p>卓越网：<a href="http://www.amazon.cn/mn/searchApp?source=garypyang-23&searchType=1&keywords=自然语言处理" title="自然语言处理书籍"target=_blank>自然语言处理书籍</a><br>
<li id="search-3" class="widget-container widget_search"><h3 class="widget-title">站内搜索</h3><form role="search" method="get" id="searchform" class="searchform" action="http://www.52nlp.cn/">
				<div>
					<label class="screen-reader-text" for="s">搜索：</label>
					<input type="text" value="" name="s" id="s" />
					<input type="submit" id="searchsubmit" value="搜索" />
				</div>
			</form></li><li id="text-4" class="widget-container widget_text"><h3 class="widget-title">NLPJob新鲜职位推荐:</h3>			<div class="textwidget"><p></p>
<script src="http://www.nlpjob.com/api/api.php?action=getJobs
&type=0&category=0&count=8&random=1&days_behind=7&response=js" type="text/javascript"></script>

<script type="text/javascript">showJobs('jobber-container', 'jobber-list');</script></div>
		</li><li id="text-3" class="widget-container widget_text"><h3 class="widget-title">52nlp新浪微博</h3>			<div class="textwidget"><p><iframe id="sina_widget_2104931705" style="width:100%; height:500px;" frameborder="0" scrolling="no" src="http://v.t.sina.com.cn/widget/widget_blog.php?uid=2104931705&height=500&skin=wd_01&showpic=1"></iframe></p>
<p><!-- JiaThis Button BEGIN --><br />
<script type="text/javascript" src="http://v3.jiathis.com/code/jiathis_r.js?uid=1340292124103344&move=0&amp;btn=r3.gif" charset="utf-8"></script><br />
<!-- JiaThis Button END --></p>
</div>
		</li><li id="categories-309398091" class="widget-container widget_categories"><h3 class="widget-title">分类目录</h3>		<ul>
	<li class="cat-item cat-item-72"><a href="http://www.52nlp.cn/category/mit-nlp" title="麻省理工学院开放式课程&quot;自然语言处理“的相关翻译文章">MIT自然语言处理</a> (23)
</li>
	<li class="cat-item cat-item-976"><a href="http://www.52nlp.cn/category/pattern-recognition-and-machine-learning-2" >PRML</a> (15)
</li>
	<li class="cat-item cat-item-469"><a href="http://www.52nlp.cn/category/topic-model" >Topic Model</a> (10)
</li>
	<li class="cat-item cat-item-87"><a href="http://www.52nlp.cn/category/wordpress" >wordpress</a> (6)
</li>
	<li class="cat-item cat-item-317"><a href="http://www.52nlp.cn/category/%e4%b8%93%e9%a2%98" >专题</a> (6)
</li>
	<li class="cat-item cat-item-263"><a href="http://www.52nlp.cn/category/chinese-information-processing" >中文信息处理</a> (20)
</li>
	<li class="cat-item cat-item-62"><a href="http://www.52nlp.cn/category/word-segmentation" >中文分词</a> (36)
</li>
	<li class="cat-item cat-item-420"><a href="http://www.52nlp.cn/category/%e5%b9%b6%e8%a1%8c%e7%ae%97%e6%b3%95" >并行算法</a> (1)
</li>
	<li class="cat-item cat-item-268"><a href="http://www.52nlp.cn/category/%e6%8b%9b%e8%81%98" >招聘</a> (4)
</li>
	<li class="cat-item cat-item-560"><a href="http://www.52nlp.cn/category/%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f" >推荐系统</a> (3)
</li>
	<li class="cat-item cat-item-354"><a href="http://www.52nlp.cn/category/%e6%95%b0%e6%8d%ae%e6%8c%96%e6%8e%98" >数据挖掘</a> (2)
</li>
	<li class="cat-item cat-item-241"><a href="http://www.52nlp.cn/category/text-classification" >文本分类</a> (3)
</li>
	<li class="cat-item cat-item-193"><a href="http://www.52nlp.cn/category/maximum-entropy-model" >最大熵模型</a> (7)
</li>
	<li class="cat-item cat-item-344"><a href="http://www.52nlp.cn/category/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" >机器学习</a> (29)
</li>
	<li class="cat-item cat-item-1"><a href="http://www.52nlp.cn/category/machine-translation" >机器翻译</a> (54)
</li>
	<li class="cat-item cat-item-195"><a href="http://www.52nlp.cn/category/%e6%9d%a1%e4%bb%b6%e9%9a%8f%e6%9c%ba%e5%9c%ba" >条件随机场</a> (3)
</li>
	<li class="cat-item cat-item-153"><a href="http://www.52nlp.cn/category/tagging" >标注</a> (13)
</li>
	<li class="cat-item cat-item-885"><a href="http://www.52nlp.cn/category/%e7%a7%91%e5%ad%a6%e8%ae%a1%e7%ae%97" >科学计算</a> (1)
</li>
	<li class="cat-item cat-item-538"><a href="http://www.52nlp.cn/category/%e7%bb%9f%e8%ae%a1%e5%ad%a6" >统计学</a> (10)
</li>
	<li class="cat-item cat-item-126"><a href="http://www.52nlp.cn/category/translation-model" >翻译模型</a> (2)
</li>
	<li class="cat-item cat-item-51"><a href="http://www.52nlp.cn/category/nlp" >自然语言处理</a> (227)
</li>
	<li class="cat-item cat-item-106"><a href="http://www.52nlp.cn/category/computational-linguistics" >计算语言学</a> (39)
</li>
	<li class="cat-item cat-item-22"><a href="http://www.52nlp.cn/category/dictionary" >词典</a> (8)
</li>
	<li class="cat-item cat-item-221"><a href="http://www.52nlp.cn/category/semantics" >语义学</a> (1)
</li>
	<li class="cat-item cat-item-161"><a href="http://www.52nlp.cn/category/semantic-web" >语义网</a> (3)
</li>
	<li class="cat-item cat-item-37"><a href="http://www.52nlp.cn/category/corpus" >语料库</a> (12)
</li>
	<li class="cat-item cat-item-86"><a href="http://www.52nlp.cn/category/language-model" >语言模型</a> (23)
</li>
	<li class="cat-item cat-item-156"><a href="http://www.52nlp.cn/category/speech-recognition" >语音识别</a> (4)
</li>
	<li class="cat-item cat-item-314"><a href="http://www.52nlp.cn/category/%e8%b4%9d%e5%8f%b6%e6%96%af%e6%a8%a1%e5%9e%8b" >贝叶斯模型</a> (1)
</li>
	<li class="cat-item cat-item-110"><a href="http://www.52nlp.cn/category/reprint" >转载</a> (28)
</li>
	<li class="cat-item cat-item-451"><a href="http://www.52nlp.cn/category/%e9%97%ae%e7%ad%94%e7%b3%bb%e7%bb%9f" >问答系统</a> (1)
</li>
	<li class="cat-item cat-item-3"><a href="http://www.52nlp.cn/category/informal-essay" >随笔</a> (63)
</li>
	<li class="cat-item cat-item-60"><a href="http://www.52nlp.cn/category/hidden-markov-model" >隐马尔科夫模型</a> (36)
</li>
		</ul>
</li><li id="archives-2" class="widget-container widget_archive"><h3 class="widget-title">文章归档</h3>		<ul>
	<li><a href='http://www.52nlp.cn/2015/01'>2015年一月</a></li>
	<li><a href='http://www.52nlp.cn/2014/12'>2014年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2014/11'>2014年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2014/09'>2014年九月</a></li>
	<li><a href='http://www.52nlp.cn/2014/07'>2014年七月</a></li>
	<li><a href='http://www.52nlp.cn/2014/06'>2014年六月</a></li>
	<li><a href='http://www.52nlp.cn/2014/05'>2014年五月</a></li>
	<li><a href='http://www.52nlp.cn/2014/04'>2014年四月</a></li>
	<li><a href='http://www.52nlp.cn/2014/01'>2014年一月</a></li>
	<li><a href='http://www.52nlp.cn/2013/12'>2013年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2013/06'>2013年六月</a></li>
	<li><a href='http://www.52nlp.cn/2013/05'>2013年五月</a></li>
	<li><a href='http://www.52nlp.cn/2013/04'>2013年四月</a></li>
	<li><a href='http://www.52nlp.cn/2013/03'>2013年三月</a></li>
	<li><a href='http://www.52nlp.cn/2013/02'>2013年二月</a></li>
	<li><a href='http://www.52nlp.cn/2013/01'>2013年一月</a></li>
	<li><a href='http://www.52nlp.cn/2012/12'>2012年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2012/11'>2012年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2012/10'>2012年十月</a></li>
	<li><a href='http://www.52nlp.cn/2012/09'>2012年九月</a></li>
	<li><a href='http://www.52nlp.cn/2012/08'>2012年八月</a></li>
	<li><a href='http://www.52nlp.cn/2012/07'>2012年七月</a></li>
	<li><a href='http://www.52nlp.cn/2012/06'>2012年六月</a></li>
	<li><a href='http://www.52nlp.cn/2012/05'>2012年五月</a></li>
	<li><a href='http://www.52nlp.cn/2012/04'>2012年四月</a></li>
	<li><a href='http://www.52nlp.cn/2012/03'>2012年三月</a></li>
	<li><a href='http://www.52nlp.cn/2012/01'>2012年一月</a></li>
	<li><a href='http://www.52nlp.cn/2011/12'>2011年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2011/11'>2011年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2011/10'>2011年十月</a></li>
	<li><a href='http://www.52nlp.cn/2011/09'>2011年九月</a></li>
	<li><a href='http://www.52nlp.cn/2011/08'>2011年八月</a></li>
	<li><a href='http://www.52nlp.cn/2011/07'>2011年七月</a></li>
	<li><a href='http://www.52nlp.cn/2011/06'>2011年六月</a></li>
	<li><a href='http://www.52nlp.cn/2011/05'>2011年五月</a></li>
	<li><a href='http://www.52nlp.cn/2011/04'>2011年四月</a></li>
	<li><a href='http://www.52nlp.cn/2011/03'>2011年三月</a></li>
	<li><a href='http://www.52nlp.cn/2011/02'>2011年二月</a></li>
	<li><a href='http://www.52nlp.cn/2011/01'>2011年一月</a></li>
	<li><a href='http://www.52nlp.cn/2010/12'>2010年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2010/11'>2010年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2010/10'>2010年十月</a></li>
	<li><a href='http://www.52nlp.cn/2010/09'>2010年九月</a></li>
	<li><a href='http://www.52nlp.cn/2010/08'>2010年八月</a></li>
	<li><a href='http://www.52nlp.cn/2010/07'>2010年七月</a></li>
	<li><a href='http://www.52nlp.cn/2010/06'>2010年六月</a></li>
	<li><a href='http://www.52nlp.cn/2010/05'>2010年五月</a></li>
	<li><a href='http://www.52nlp.cn/2010/04'>2010年四月</a></li>
	<li><a href='http://www.52nlp.cn/2010/03'>2010年三月</a></li>
	<li><a href='http://www.52nlp.cn/2010/02'>2010年二月</a></li>
	<li><a href='http://www.52nlp.cn/2010/01'>2010年一月</a></li>
	<li><a href='http://www.52nlp.cn/2009/12'>2009年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2009/11'>2009年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2009/10'>2009年十月</a></li>
	<li><a href='http://www.52nlp.cn/2009/09'>2009年九月</a></li>
	<li><a href='http://www.52nlp.cn/2009/08'>2009年八月</a></li>
	<li><a href='http://www.52nlp.cn/2009/07'>2009年七月</a></li>
	<li><a href='http://www.52nlp.cn/2009/06'>2009年六月</a></li>
	<li><a href='http://www.52nlp.cn/2009/05'>2009年五月</a></li>
	<li><a href='http://www.52nlp.cn/2009/04'>2009年四月</a></li>
	<li><a href='http://www.52nlp.cn/2009/03'>2009年三月</a></li>
	<li><a href='http://www.52nlp.cn/2009/02'>2009年二月</a></li>
	<li><a href='http://www.52nlp.cn/2009/01'>2009年一月</a></li>
	<li><a href='http://www.52nlp.cn/2008/12'>2008年十二月</a></li>
		</ul>
</li>		<li id="recent-posts-2" class="widget-container widget_recent_entries">		<h3 class="widget-title">最新文章</h3>		<ul>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e5%9b%9b%e7%ab%a0-combining-models">PRML读书会第十四章 Combining Models</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%89%e7%ab%a0sequential-data">PRML读书会第十三章 Sequential Data</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%ba%8c%e7%ab%a0-continuous-latent-variables">PRML读书会第十二章 Continuous Latent Variables</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%80%e7%ab%a0-sampling-methods">PRML读书会第十一章  Sampling Methods</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e7%ab%a0-approximate-inference">PRML读书会第十章  Approximate Inference</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b9%9d%e7%ab%a0-mixture-models-and-em">PRML读书会第九章  Mixture Models and EM</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ab%e7%ab%a0-graphical-models">PRML读书会第八章  Graphical Models</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%83%e7%ab%a0-sparse-kernel-machines">PRML读书会第七章 Sparse Kernel Machines</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ad%e7%ab%a0-kernel-methods">PRML读书会第六章   Kernel Methods</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%94%e7%ab%a0-neural-networks">PRML读书会第五章  Neural Networks</a>
						</li>
				</ul>
		</li><li id="recentcomments" class="widget-container widget_recentcomments"><h3 class="widget-title">最近评论</h3><ul><li class="rc-navi rc-clearfix"><span class="rc-loading">正在加载...</span></li><li id="rc-comment-temp" class="rc-item rc-comment rc-clearfix"><div class="rc-info"></div><div class="rc-timestamp"></div><div class="rc-excerpt"></div></li><li id="rc-ping-temp" class="rc-item rc-ping rc-clearfix"><span class="rc-label"></span></li></ul></li>			</ul>
		</div><!-- #primary .widget-area -->


		<div id="secondary" class="widget-area" role="complementary">
			<ul class="xoxo">
				<li id="linkcat-103" class="widget-container widget_links"><h3 class="widget-title">NLP相关网站</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://www.aclweb.org/" rel="co-worker" title="The Association for Computational Linguistics" target="_blank">ACL</a></li>
<li><a href="http://aclweb.org/anthology-new/" rel="co-worker" title="A Digital Archive of Research Papers in Computational Linguistics" target="_blank">ACL Anthology</a></li>
<li><a href="http://belobog.si.umich.edu/clair/anthology/index.cgi" rel="colleague" target="_blank">ACL Anthology Network</a></li>
<li><a href="http://aclweb.org/aclwiki/index.php?title=Main_Page" rel="colleague" title="the Wiki of the Association for Computational Linguistics" target="_blank">ACL Wiki</a></li>
<li><a href="http://www.clsp.jhu.edu/" rel="colleague" target="_blank">CLSP</a></li>
<li><a href="http://www.cwbbase.com/" rel="colleague" title="这是一个略具规模的中文语义词库, 也是稍有特色的汉语语义词典" target="_blank">CWB中文词库</a></li>
<li><a href="http://www.euromatrix.net/" rel="colleague" target="_blank">EuroMatrix</a></li>
<li><a href="http://www.freebase.com" rel="colleague" target="_blank">Freebase</a></li>
<li><a href="http://www.clsp.jhu.edu/workshops/" rel="colleague" target="_blank">JHU Workshop</a></li>
<li><a href="http://www.ldc.upenn.edu/" rel="colleague" title="Linguistic Data Consortium" target="_blank">LDC</a></li>
<li><a href="http://www.statmt.org/moses/" rel="colleague" title="A factored phrase-based beam-search decoder for machine translation" target="_blank">Moses</a></li>
<li><a href="http://nlpers.blogspot.com/" rel="colleague" title="国外一个非常不错的自然语言处理博客" target="_blank">nlper</a></li>
<li><a href="http://www.nlpjob.com" target="_blank">NLPJob</a></li>
<li><a href="http://www.powerset.com/" rel="colleague" target="_blank">Powerset</a></li>
<li><a href="http://www.speech.sri.com/projects/srilm/" rel="colleague" title="- The SRI Language Modeling Toolkit" target="_blank">SRILM</a></li>
<li><a href="http://www.statmt.org/" rel="colleague" title="This website is dedicated to research in statistical machine translation" target="_blank">Statistical Machine Translation</a></li>
<li><a href="http://textanalysisonline.com/" target="_blank">Text Analysis</a></li>
<li><a href="http://textminingonline.com/" target="_blank">Text Mining</a></li>
<li><a href="http://textsummarization.net/" target="_blank">Text Summarization</a></li>
<li><a href="http://w3china.org/index.htm" rel="friend" title="致力于促进W3C技术的广泛应用, 传播关于未来Web的知识与技术" target="_blank">中国万维网联盟</a></li>
<li><a href="http://www.cipsc.org.cn/" rel="co-worker" title="Chinese Information Processing Society of China" target="_blank">中国中文信息学会</a></li>
<li><a href="http://www.nlp.org.cn/" rel="colleague" title="中文自然语言处理开放平台" target="_blank">中文自然语言处理开放平台</a></li>
<li><a href="http://www.mt-archive.info/" rel="colleague" title="Repository and bibliography of articles, books and papers on topics" target="_blank">机器翻译档案计划</a></li>
<li><a href="http://www.statmt.org/europarl/" rel="colleague" target="_blank">欧洲议会平行语料库</a></li>
<li><a href="http://www.keenage.com/" title="HowNet" target="_blank">知网</a></li>
<li><a href="http://www.nlpir.org/" rel="friend" title="由张华平博士发起，由北京理工大学网络搜索与挖掘实验室运营，旨在推动NLP(自然语言处理)与IR(信息检索)领域的共享与共赢" target="_blank">自然语言处理与信息检索共享平台</a></li>
<li><a href="http://mitel.ict.ac.cn/" rel="co-worker" title="中科院计算所多语言交互技术实验室" target="_blank">计算所多语言交互技术实验室</a></li>

	</ul>
</li>
<li id="linkcat-2" class="widget-container widget_links"><h3 class="widget-title">友情链接</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://blog.youxu.info/" title="一个计算机专业的 Ph.D. 学生徐宥的个人博客" target="_blank">4G spaces</a></li>
<li><a href="http://blog.52nlp.org" rel="me" title="我爱自然语言处理完全镜像" target="_blank">52nlpblog</a></li>
<li><a href="http://www.52nlp.com" rel="me" title="52nlp的英文站" target="_blank">52nlpcom</a></li>
<li><a href="http://hi.baidu.com/drkevinzhang" rel="friend" title="ICTCLAS 张华平博士的空间" target="_blank">ICTCLAS 张华平博士的空间</a></li>
<li><a href="http://blog.so8848.com/" rel="friend" title="信息检索博客" target="_blank">Information Retrieval Blog</a></li>
<li><a href="http://interop123.com/default.aspx" rel="friend" title="崔晓源师兄关于NET技术的站点" target="_blank">NET互操作技术社区</a></li>
<li><a href="http://bbs.w3china.org/" rel="friend" title="中国万维网联盟讨论区" target="_blank">W3CHINA讨论区</a></li>
<li><a href="http://www.ailab.cn/" rel="friend" target="_blank">人工智能网</a></li>
<li><a href="http://mindhacks.cn/" rel="friend" title="一个很有思想的价值博客!" target="_blank">刘未鹏之Mind Hacks</a></li>
<li><a href="http://www.cnblogs.com/finallyliuyu/" rel="friend" target="_blank">原地转圈的驴子</a></li>
<li><a href="http://xunren.thuir.org/" target="_blank">微博寻人（梁博）</a></li>
<li><a href="http://52opencourse.com" rel="friend" title="我爱公开课，高质量公开课交流平台" target="_blank">我爱公开课</a></li>
<li><a href="http://iregex.org/" rel="friend" target="_blank">我爱正则表达式</a></li>
<li><a href="http://courseminer.com" target="_blank">挖课</a></li>
<li><a href="http://www.flickering.cn/" target="_blank">火光摇曳</a></li>
<li><a href="http://www.sciencenet.cn/u/timy/" rel="friend" title="章成志老师的博客" target="_blank">章成志的博客</a></li>
<li><a href="http://blog.csdn.net/v_JULY_v/" target="_blank">结构之法 算法之道</a></li>
<li><a href="http://www.lingcc.com/" rel="friend" title="关注编译器,虚拟机,编程语言及技术,IT职业和程序员生活" target="_blank">编译点滴</a></li>
<li><a href="http://www.52nlp.org" rel="me" title="52nlp的官方网站" target="_blank">自然语言处理</a></li>
<li><a href="http://www.ieee.org.cn/" rel="friend" title="计算机科学论坛" target="_blank">计算机科学论坛</a></li>
<li><a href="http://coursegraph.com/">课程图谱</a></li>
<li><a href="http://blog.coursegraph.com" rel="friend">课程图谱博客</a></li>

	</ul>
</li>
<li id="meta-4" class="widget-container widget_meta"><h3 class="widget-title">功能</h3>			<ul>
						<li><a href="http://www.52nlp.cn/wp-login.php">登录</a></li>
			<li><a href="http://www.52nlp.cn/feed">文章<abbr title="Really Simple Syndication">RSS</abbr></a></li>
			<li><a href="http://www.52nlp.cn/comments/feed">评论<abbr title="Really Simple Syndication">RSS</abbr></a></li>
<li><a href="https://cn.wordpress.org/" title="基于WordPress，一个优美、先进的个人信息发布平台。">WordPress.org</a></li>			</ul>
</li>			</ul>
		</div><!-- #secondary .widget-area -->

	</div><!-- #main -->

	<div id="footer" role="contentinfo">
		<div id="colophon">



			<div id="site-info">
				<a href="http://www.52nlp.cn/" title="我爱自然语言处理" rel="home">
					我爱自然语言处理				</a>
			</div><!-- #site-info -->

			<div id="site-generator">
								<a href="http://cn.wordpress.org/"
						title="优雅的个人发布平台" rel="generator">
					自豪地采用 WordPress。				</a>
			</div><!-- #site-generator -->

		</div><!-- #colophon -->
	</div><!-- #footer -->

</div><!-- #wrapper -->

<script>
/* <![CDATA[ */
var rcGlobal = {
	serverUrl		:'http://www.52nlp.cn',
	infoTemp		:'%REVIEWER% 在 %POST%',
	loadingText		:'正在加载',
	noCommentsText	:'没有任何评论',
	newestText		:'&laquo; 最新的',
	newerText		:'&laquo; 上一页',
	olderText		:'下一页 &raquo;',
	showContent		:'',
	external		:'',
	avatarSize		:'0',
	avatarPosition	:'left',
	anonymous		:'匿名'
};
/* ]]> */
</script>
<script type='text/javascript' src='http://www.52nlp.cn/wp-content/plugins/akismet/_inc/form.js?ver=3.0.4'></script>
<link rel='stylesheet' id='yarppRelatedCss-css'  href='http://www.52nlp.cn/wp-content/plugins/yet-another-related-posts-plugin/style/related.css?ver=4.0.1' type='text/css' media='all' />
<script type='text/javascript' src='http://www.52nlp.cn/wp-content/plugins/wp-recentcomments/js/wp-recentcomments.js?ver=2.2.7'></script>
	<p align="center"> 本站架设在 <a href="http://www.52nlp.cn/digitalocean%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0">DigitalOcean</a> 上, 采用创作共用版权协议, 要求署名、非商业用途和保持一致. 转载本站内容必须也遵循“署名-非商业用途-保持一致”的创作共用协议.</p>
<!-- Piwik -->
<script type="text/javascript">
  var _paq = _paq || [];
  _paq.push(["trackPageView"]);
  _paq.push(["enableLinkTracking"]);

  (function() {
    var u=(("https:" == document.location.protocol) ? "https" : "http") + "://162.243.252.121/piwik/";
    _paq.push(["setTrackerUrl", u+"piwik.php"]);
    _paq.push(["setSiteId", "5"]);
    var d=document, g=d.createElement("script"), s=d.getElementsByTagName("script")[0]; g.type="text/javascript";
    g.defer=true; g.async=true; g.src=u+"piwik.js"; s.parentNode.insertBefore(g,s);
  })();
</script>
<!-- End Piwik Code -->
</body>
</html>
