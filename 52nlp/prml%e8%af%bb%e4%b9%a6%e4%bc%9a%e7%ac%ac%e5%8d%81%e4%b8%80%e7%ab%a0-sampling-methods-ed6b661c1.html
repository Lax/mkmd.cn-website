<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8" />
<title>PRML读书会第十一章  Sampling Methods | 我爱自然语言处理</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="stylesheet" type="text/css" media="all" href="http://www.52nlp.cn/wp-content/themes/twentytenorg/style.css" />
<link rel="pingback" href="http://www.52nlp.cn/xmlrpc.php" />
<link rel="alternate" type="application/rss+xml" title="我爱自然语言处理 &raquo; Feed" href="http://www.52nlp.cn/feed" />
<link rel="alternate" type="application/rss+xml" title="我爱自然语言处理 &raquo; 评论Feed" href="http://www.52nlp.cn/comments/feed" />
<link rel="alternate" type="application/rss+xml" title="我爱自然语言处理 &raquo; PRML读书会第十一章  Sampling Methods评论Feed" href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%80%e7%ab%a0-sampling-methods/feed" />
<link rel='stylesheet' id='yarppWidgetCss-css'  href='http://www.52nlp.cn/wp-content/plugins/yet-another-related-posts-plugin/style/widget.css?ver=4.0.1' type='text/css' media='all' />
<link rel='stylesheet' id='codecolorer-css'  href='http://www.52nlp.cn/wp-content/plugins/codecolorer/codecolorer.css?ver=0.9.9' type='text/css' media='screen' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://www.52nlp.cn/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.52nlp.cn/wp-includes/wlwmanifest.xml" /> 
<link rel='prev' title='PRML读书会第十章  Approximate Inference' href='http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e7%ab%a0-approximate-inference' />
<link rel='next' title='PRML读书会第十二章 Continuous Latent Variables' href='http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%ba%8c%e7%ab%a0-continuous-latent-variables' />
<meta name="generator" content="WordPress 4.0.1" />
<link rel='canonical' href='http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%80%e7%ab%a0-sampling-methods' />
<link rel='shortlink' href='http://www.52nlp.cn/?p=7819' />
<!-- wp thread comment 1.4.9.4.002 -->
<style type="text/css" media="screen">
.editComment, .editableComment, .textComment{
	display: inline;
}
.comment-childs{
	border: 1px solid #999;
	margin: 5px 2px 2px 4px;
	padding: 4px 2px 2px 4px;
	background-color: white;
}
.chalt{
	background-color: #E2E2E2;
}
#newcomment{
	border:1px dashed #777;width:90%;
}
#newcommentsubmit{
	color:red;
}
.adminreplycomment{
	border:1px dashed #777;
	width:99%;
	margin:4px;
	padding:4px;
}
.mvccls{
	color: #999;
}
			
</style>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } },
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
});
</script><script type="text/javascript" src="http://www.52nlp.cn/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>

<body class="single single-post postid-7819 single-format-standard">
<div id="wrapper" class="hfeed">
	<div id="header">
		<div id="masthead">
			<div id="branding" role="banner">
								<div id="site-title">
					<span>
						<a href="http://www.52nlp.cn/" title="我爱自然语言处理" rel="home">我爱自然语言处理</a>
					</span>
				</div>
				<div id="site-description">I Love Natural Language Processing</div>

										<img src="http://www.52nlp.cn/wp-content/themes/twentytenorg/images/headers/path.jpg" width="940" height="198" alt="" />
								</div><!-- #branding -->

			<div id="access" role="navigation">
			  				<div class="skip-link screen-reader-text"><a href="#content" title="跳至正文">跳至正文</a></div>
								<div class="menu"><ul><li ><a href="http://www.52nlp.cn/">首页</a></li><li class="page_item page-item-2"><a href="http://www.52nlp.cn/about">关于</a></li><li class="page_item page-item-2557 page_item_has_children"><a href="http://www.52nlp.cn/resources">资源</a><ul class='children'><li class="page_item page-item-1271"><a href="http://www.52nlp.cn/resources/wpmatheditor">WpMathEditor</a></li></ul></li></ul></div>
 
				<div class="menu"><ul><li class="page_item page-item-2"></li><li class="page_item page-item-2"><a href="http://coursegraph.com" title="课程图谱" target="_blank">课程图谱</a></li><li class="page_item page-item-2"><a href="http://www.nlpjob.com" title="求职" target="_blank">求职招聘</a></li></ul></div>
			</div><!-- #access -->
		</div><!-- #masthead -->
	</div><!-- #header -->

	<div id="main">

		<div id="container">
			<div id="content" role="main">

			

				<div id="nav-above" class="navigation">
					<div class="nav-previous"><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e7%ab%a0-approximate-inference" rel="prev"><span class="meta-nav">&larr;</span> PRML读书会第十章  Approximate Inference</a></div>
					<div class="nav-next"><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%ba%8c%e7%ab%a0-continuous-latent-variables" rel="next">PRML读书会第十二章 Continuous Latent Variables <span class="meta-nav">&rarr;</span></a></div>
				</div><!-- #nav-above -->

				<div id="post-7819" class="post-7819 post type-post status-publish format-standard hentry category-pattern-recognition-and-machine-learning-2 category-344 tag-em tag-gibbs-sampling tag-hybrid-monte-carlo tag-markov-chain-monte-carlo tag-mcmc tag-metropolis-hastings tag-pattern-recognition-and-machine-learning tag-prml tag-sampling-methods tag-slice-sampling tag-344 tag-961 tag-960 tag-1081 tag-1082 tag-1073 tag-1074 tag-308 tag-1053 tag-1080">
					<h1 class="entry-title">PRML读书会第十一章  Sampling Methods</h1>

					<div class="entry-meta">
						<span class="meta-prep meta-prep-author">发表于</span> <a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%80%e7%ab%a0-sampling-methods" title="12:40" rel="bookmark"><span class="entry-date">2015年01月31号</span></a> <span class="meta-sep">由</span> <span class="author vcard"><a class="url fn n" href="http://www.52nlp.cn/author/prml" title="查看所有由 prml 发布的文章">prml</a></span>					</div><!-- .entry-meta -->

					<div class="entry-content">
						<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>PRML读书会第十一章 Sampling Methods<br />
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>主讲人 网络上的尼采<br />
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 12pt"><strong>（新浪微博: <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a>）<br />
</strong></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">网络上的尼采(813394698) 9:05:00<br />
今天的主要内容：Markov Chain Monte Carlo，Metropolis-Hastings，Gibbs Sampling，Slice Sampling，Hybrid Monte Carlo。<br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">上一章讲到的平均场是统计物理学中常用的一种思想，将无法处理的复杂多体问题分解成可以处理的单体问题来近似，变分推断便是在平均场的假设约束下求泛函L(Q)极值的最优化问题，好处在于求解过程中可以推出精致的解析解。变分是从最优化的角度通过坐标上升法收敛到局部最优，这一章我们将通过计算从动力学角度见证Markov Chain Monte Carlo收敛到平稳分布。<br />
</span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">先说sampling的原因，因为统计学中经常会遇到对复杂的分布做加和与积分，这往往是intractable的。MCMC方法出现后贝叶斯方法才得以发展，因为在那之前对不可观测变量（包括隐变量和参数）后验分布积分非常困难，对于这个问题上一章变分用的解决办法是通过最优化方法寻找一个和不可观测变量后验分布p(Z|X)近似的分布，这一章我们看下sampling的解决方法，举个简单的例子：比如我们遇到这种形式<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa1.jpg" alt="" />，z是个连续随机变量，p(z)是它的分布，我们求f(z)的期望。如果我们从p(z)中sampling一个数据集z<sup>(l)</sup>，然后再求个平均<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa2.jpg" alt="" />来近似f(z)的期望</span><span style="font-family: 宋体">，</span><span style="font-family: 微软雅黑">so,问题就解决了，关键是如何从p(z)中做无偏的sampling。<br />
为了说明sampling的作用，我们先举个EM的例子，最大似然计算中求分布的积分问题，我们在第九章提到了，完整数据的log似然函数是对隐变量Z的积分：<br />
</span></span><span id="more-7819"></span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa3.jpg" alt="" /><span style="font-family: 宋体;font-size: 12pt"><br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">如果Z是比较复杂的分布，我们就需要对Z进行采样，从而得到：<br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa4.jpg" alt="" /><span style="font-family: 宋体;font-size: 12pt"><br />
</span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">具体就是从Z的后验分布<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa5.jpg" alt="" />中进行采样。</span><span style="font-family: 宋体"><br />
</span></span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">如果我们从贝叶斯的观点，把EM参数theta也当成一个分布的话，有下面一个IP算法:<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa6.jpg" alt="" /><br />
I步，我们无法直接对P(Z|X)取样，我们可以先对P(theta|X)取样<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa7.jpg" alt="" />，然后再对Z的后验分布进行取样：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa8.jpg" alt="" /></span><span style="font-family: 宋体"><br />
</span></span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">P步，利用上一步对P(Z|X)的取样，来确定新的参数分布：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa9.jpg" alt="" /><br />
然后按这个I步和P步的方式迭代。</span><span style="font-family: 宋体"><br />
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">接下来我们讲sampling methods，为了节省时间，两个基本的rejection sampling和Importance sampling不讲了，这两种方法在高维下都会失效，我们直奔主题MCMC（Markov Chain Monte Carlo ）。蒙特卡洛，是个地中海海滨城市，气候宜人，欧洲富人们的聚集地，更重要的是它是世界三大赌城之一，用这个命名就知道这种方法是基于随机的，过会我们会讲到。马尔科夫大神就不用多说了，他当时用自己的名字命名马尔科夫链是预见到这个模型巨大作用。他还有一个师弟叫李雅普洛夫，控制论里面的李雅普洛夫函数说的就是这位，他们的老师叫契比雪夫，都是圣彼得堡学派的。俄国数学家对人类的贡献是无价的orz<br />
最早的MCMC方法是美国科学家在研制原子弹时算积分发明的。我们先介绍一个最基本的Metropolis方法，这种方法的接受率是：<br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa10.jpg" alt="" /><span style="font-family: 宋体;font-size: 12pt"><br />
</span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">但有个要求，就是proposal distribution满足<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa11.jpg" alt="" /></span><span style="font-family: 宋体">。</span><span style="font-family: 微软雅黑">过程很简单，我们先找个比较容易采样的分布即proposal分布，然后从这个分布中取一个样本Z*，如果<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa12.jpg" alt="" />大于1直接接受，如果小于1就接着算出接受率，并且从（0，1）之间取一个随机数和这个接受率做比较来决定是否接受这个样本。过会会在Metropolis-Hastings algorithm方法中具体说。下图是一个简单的例子，对高斯分布做采样，绿线是表示接受的步骤，红线表示拒绝的：</span><span style="font-family: 宋体"><br />
</span></span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa13.jpg" alt="" /><span style="font-family: 宋体;font-size: 12pt"><br />
</span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">讲Metropolis-Hastings方法前，我们先来回顾下马尔科夫链的性质，这个很重要。markov chains最基本的性质就是无后效性，就是这条链的下一个节点的状态由当前节点状态完全决定：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa14.jpg" alt="" /></span><span style="font-family: 宋体"><br />
</span></span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">特定的齐次马尔科夫链可以收敛到平稳分布，也就是经过相当长的一段时间转移，收敛到的分布和初始值无关，转移核起着决定的作用。关于马尔科夫链的收敛，<strong>我们将介绍一个充分条件：detailed balance细致平稳条件。</strong><br />
先介绍两个公式，马尔科夫链节点状态的marginal distribution计算公式，由于无后效性我们可以得到<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa15.jpg" alt="" /></span><span style="font-family: 宋体"><br />
</span></span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">上面公式的加和结合马儿科夫链的状态转移矩阵是比较容易理解。</span><span style="font-family: 宋体"><br />
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">平稳分布的定义就是下面的形式：<br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa16.jpg" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">其中<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa17.jpg" alt="" />是z&#8217;到z的转移概率，上面的公式不难理解，就是转移后分布不再发生变化。<br />
</span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">下面我们给出细致平稳条件的公式：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa18.jpg" alt="" /></span><span style="font-family: 宋体"><br />
</span></span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">满足细致平稳条件就能收敛到平稳分布，下面是推导：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa19.jpg" alt="" /></span><span style="font-family: 宋体"><br />
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">我们把上面细致平稳条件的公式11.40代入公式11.41最左边，从左朝右推导就是平稳分布的公式11.39。细致平稳条件的好处，就是我们能控制马尔科夫链收敛到我们指定的分布<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa20.jpg" alt="" />。以后的Metropolis-Hastings方法及改进都是基于这个基础的。<br />
前面我们提到，Metropolis方法需要先选一个比较容易取样的proposal distribution，从这个分布里取样，然后通过接受率决定是否采用这个样本。一个简单的例子就是对于proposal distribution我们可以采用Gaussian centred on the current state，其实很好理解，就是上一步节点的值可以做下一步节点需要采样的proposal distribution即高斯分布的均值，这样下一步节点的状态由上一步完全决定，这就是一个马尔科夫链。马尔科夫链有了，我们怎么保证能收敛到目标分布呢？就是前面说的细致平稳条件，我们可以通过设置接受率的形式来满足这个条件。Metropolis-Hastings接受率的形式：<br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa21.jpg" alt="" /><span style="font-family: 宋体;font-size: 12pt"><br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa22.jpg" alt="" /><span style="font-size: 12pt"><span style="font-family: 微软雅黑">来自于<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa23.jpg" alt="" />，分布q便是proposal distribution。<br />
范涛@推荐系统(289765648) 10:49:15<br />
Zp 是什么？</span><span style="font-family: 宋体"><br />
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">网络上的尼采(813394698) 10:52:04<br />
Zp是分布中和z无关的部分。<br />
</span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">为了使各位有个形象的理解，我描述一下过程，我们把<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa24.jpg" alt="" />当做高斯分布的均值，方差是固定的。然后从这个分布取一个样本就是<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa25.jpg" alt="" />，如果<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa26.jpg" alt="" />大于1肯定接受，如果小于1，我们便从（0，1）之间取一个随机数，和这个接受率做比较，如果接受率大于这个随机数便接受，反之便拒绝。接受率这么设就能满足细致平稳条件的原因，看这个（11.45）公式：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa27.jpg" alt="" /></span><span style="font-family: 宋体"><br />
</span></span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa28.jpg" alt="" /><span style="font-family: 宋体;font-size: 12pt"><br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">我们把接受率公式11.44代入上面的公式的左边，会推出左右两边就是细致平稳条件的形式，红框部分便是细致平稳条件公式11.40的转移核，书上的公式明显错了，上面的这个是勘误过的。<br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">刚才说了proposal distribution一般采用Gaussian centred on the current state，高斯分布的方差是固定的，其实方差就是步长，如何选择步长这是一个state of the art问题，步子太小扩散太慢，步子太大，拒绝率会很高，原地踏步。书中的一个例子，当用Gaussian centred on the current state作proposal distribution时，步长设为目标高斯分布的最小标准差最合适：<br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa29.jpg" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
下面讲Gibbs Sampling，Gibbs Sampling其实是每次只对一个维度的变量进行采样，固定住其他维度的变量，然后迭代，可以看做是Metropolis-Hastings的特例，它的接受率一直是1.<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa30.jpg" alt="" /><br />
步骤是比较容易理解的，跟上一章的变分法的有相似之处。假设有三个变量的分布<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa31.jpg" alt="" />，<br />
先固定住z2 z3对z1进行采样，<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa32.jpg" alt="" />；<br />
然后固定住z1 z3对z2进行采样，<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa33.jpg" alt="" />；<br />
然后是Z3，<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa34.jpg" alt="" /><br />
如此迭代。<br />
</span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">根据Metropolis-Hastings，它的接受率恒为1。看下面的推导：<br />
<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa35.jpg" alt="" /></span><span style="font-family: 宋体"><br />
</span></span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">因为其他维度是固定不变的，所以<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa36.jpg" alt="" />，代入上式就都约去了，等于1.<br />
最后对于图模型采用gibbs sampling，条件概率<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa37.jpg" alt="" />可以根据马尔科夫毯获得，下面一个是无向图，一个是有向图，蓝色的节点是和要采样的变量有关的其他变量：</span><span style="font-family: 宋体"><br />
</span></span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa38.jpg" alt="" /><span style="font-family: 宋体;font-size: 12pt"><br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">关于更多的gibbs sampling的内容可以看MLAPP，里面有blocked gibbs和collapsed gibbs。<br />
</span></p>
<p>&nbsp;</p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">刚才提到Metropolis-Hastings对步长敏感，针对这个问题，下面介绍两个增加辅助变量的方法，这些方法也是满足细致平稳条件的。先介绍slice sampling，这种方法增加了一个变量U，可以根据分布的特征自动调整步长： <img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa39.jpg" alt="" /><br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">步骤很简单：在<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa40.jpg" alt="" />与<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa41.jpg" alt="" />之间的这段距离随机取个值U，然后通过U画个横线，然后在包含<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa42.jpg" alt="" />并且<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa43.jpg" alt="" />这段横线对z进行随机采样，然后按这种方式迭代。图(b)为了实际中便于操作，有时还需要多出那么一段，因为我们事先不知道目标分布的具体形式，所以包含<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa44.jpg" alt="" />并且<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa45.jpg" alt="" />这段横线没法确定，只能朝外延伸加单位长度进行试，最后会多出来一段，这一点书上并没有介绍详细。<br />
下面介绍The Hybrid Monte Carlo Algorithm （Hamiltonian MCMC）：哈密顿，神童，经典力学三巨头之一，这个算法引入了哈密顿动力系统的概念，计算接受率时考虑的是系统的总能量。Hybrid Monte Carlo 定义了势能和动能两种能量，它们的和便是系统总能量哈密顿量。先看势能，分布可以写成这种形式：<br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa46.jpg" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
E(z)便是系统的势能。<br />
另外增加一个变量，状态变量变化的速率：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa47.jpg" alt="" /><br />
系统的动能便是：<br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa48.jpg" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
总的能量便是：<img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa49.jpg" alt="" /><br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">比如高斯分布的哈密顿量就可表示为：<br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa50.jpg" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">下面这个公式便是Hybrid Monte Carlo 的接受率：<br />
</span></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa51.jpg" alt="" /><span style="font-family: 微软雅黑;font-size: 12pt"><br />
可以证明 这种接受率是满足detailed balance条件的。<br />
ORC(267270520) 12:14:03<br />
推荐一本相关的书 Introducing Monte Carlo Methods with R (use R) ，PS：R做MCMC很方便。<br />
赞尼采讲的很精彩，学习了，嘿嘿<br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">网络上的尼采(813394698) 12:36:37<br />
Markov Chain Monte Carlo In Practice(Gilks)这本书也挺不错。<br />
红烧鱼(403774317) 12:38:06<br />
这本读研的时候生读过，非常实用，随书附带code<br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">网络上的尼采(813394698)<br />
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">最后需要补充的是：判断MCMC的burn-in何时收敛是个问题，koller介绍了两种方法，即同一条链上设置不同的时间窗做比较，另一种同时跑多条链然后作比较。当然也有一条链跑到黑的。<br />
</span></p>
<p>注：PRML读书会系列文章由 <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a> 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。</p>
<p>PRML读书会讲稿PDF版本以及更多资源下载地址：<a href="http://vdisk.weibo.com/u/1841149974">http://vdisk.weibo.com/u/1841149974</a></p>
<p>本文链接地址：<a href="http://www.52nlp.cn/prml读书会第十一章-sampling-methods">http://www.52nlp.cn/prml读书会第十一章-sampling-methods</a></p>
<p><span style="font-family: 微软雅黑;font-size: 9pt"><br />
</span></p>
<div class='yarpp-related'>
<p>相关文章:<ol>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ad%e7%ab%a0-kernel-methods" rel="bookmark" title="PRML读书会第六章   Kernel Methods">PRML读书会第六章   Kernel Methods </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e5%89%8d%e8%a8%80" rel="bookmark" title="PRML读书会前言">PRML读书会前言 </a></li>
<li><a href="http://www.52nlp.cn/lda-math-mcmc-%e5%92%8c-gibbs-sampling2" rel="bookmark" title="LDA-math-MCMC 和 Gibbs Sampling(2)">LDA-math-MCMC 和 Gibbs Sampling(2) </a></li>
<li><a href="http://www.52nlp.cn/lda-math-mcmc-%e5%92%8c-gibbs-sampling1" rel="bookmark" title="LDA-math-MCMC 和 Gibbs Sampling(1)">LDA-math-MCMC 和 Gibbs Sampling(1) </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b9%9d%e7%ab%a0-mixture-models-and-em" rel="bookmark" title="PRML读书会第九章  Mixture Models and EM">PRML读书会第九章  Mixture Models and EM </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%8c%e7%ab%a0-probability-distributions" rel="bookmark" title="PRML读书会第二章  Probability Distributions">PRML读书会第二章  Probability Distributions </a></li>
<li><a href="http://www.52nlp.cn/tears-and-uninitiated-learn-from-natural-language-processing-heros" rel="bookmark" title="“眼泪”与“门外汉”——向自然语言处理的大牛们学习">“眼泪”与“门外汉”——向自然语言处理的大牛们学习 </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e7%ab%a0-approximate-inference" rel="bookmark" title="PRML读书会第十章  Approximate Inference">PRML读书会第十章  Approximate Inference </a></li>
<li><a href="http://www.52nlp.cn/bayesian-modeling-for-language-tutorial-reading" rel="bookmark" title="贝叶斯模型文献阅读指南">贝叶斯模型文献阅读指南 </a></li>
<li><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%9b%9b%e7%ab%a0-linear-models-for-classification" rel="bookmark" title="PRML读书会第四章 Linear Models for Classification">PRML读书会第四章 Linear Models for Classification </a></li>
</ol></p>
</div>
											</div><!-- .entry-content -->


					<div class="entry-utility">
						此条目发表在 <a href="http://www.52nlp.cn/category/pattern-recognition-and-machine-learning-2" rel="category tag">PRML</a>, <a href="http://www.52nlp.cn/category/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" rel="category tag">机器学习</a> 分类目录，贴了 <a href="http://www.52nlp.cn/tag/em" rel="tag">EM</a>, <a href="http://www.52nlp.cn/tag/gibbs-sampling" rel="tag">Gibbs Sampling</a>, <a href="http://www.52nlp.cn/tag/hybrid-monte-carlo" rel="tag">Hybrid Monte Carlo</a>, <a href="http://www.52nlp.cn/tag/markov-chain-monte-carlo" rel="tag">Markov Chain Monte Carlo</a>, <a href="http://www.52nlp.cn/tag/mcmc" rel="tag">MCMC</a>, <a href="http://www.52nlp.cn/tag/metropolis-hastings" rel="tag">Metropolis-Hastings</a>, <a href="http://www.52nlp.cn/tag/pattern-recognition-and-machine-learning" rel="tag">Pattern Recognition And Machine Learning</a>, <a href="http://www.52nlp.cn/tag/prml" rel="tag">PRML</a>, <a href="http://www.52nlp.cn/tag/prml%e8%af%bb%e4%b9%a6%e4%bc%9a" rel="tag">PRML读书会</a>, <a href="http://www.52nlp.cn/tag/sampling-methods" rel="tag">Sampling Methods</a>, <a href="http://www.52nlp.cn/tag/slice-sampling" rel="tag">Slice Sampling</a>, <a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" rel="tag">机器学习</a>, <a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b9%a6%e7%b1%8d" rel="tag">机器学习书籍</a>, <a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e8%af%bb%e4%b9%a6%e4%bc%9a" rel="tag">机器学习读书会</a>, <a href="http://www.52nlp.cn/tag/%e8%92%99%e7%89%b9%e5%8d%a1%e7%bd%97" rel="tag">蒙特卡罗</a>, <a href="http://www.52nlp.cn/tag/%e8%b4%9d%e5%8f%b6%e6%96%af%e6%96%b9%e6%b3%95" rel="tag">贝叶斯方法</a>, <a href="http://www.52nlp.cn/tag/%e9%87%87%e6%a0%b7" rel="tag">采样</a>, <a href="http://www.52nlp.cn/tag/%e9%87%87%e6%a0%b7%e6%96%b9%e6%b3%95" rel="tag">采样方法</a>, <a href="http://www.52nlp.cn/tag/%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e9%93%be%e8%92%99%e7%89%b9%e5%8d%a1%e7%bd%97%e6%96%b9%e6%b3%95" rel="tag">马尔可夫链蒙特卡罗方法</a>, <a href="http://www.52nlp.cn/tag/%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab" rel="tag">马尔科夫</a>, <a href="http://www.52nlp.cn/tag/%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e8%92%99%e7%89%b9%e5%8d%a1%e7%bd%97" rel="tag">马尔科夫蒙特卡罗</a> 标签。将<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%80%e7%ab%a0-sampling-methods" title="链向 PRML读书会第十一章  Sampling Methods 的固定链接" rel="bookmark">固定链接</a>加入收藏夹。											</div><!-- .entry-utility -->
				</div><!-- #post-## -->

				<div id="nav-below" class="navigation">
					<div class="nav-previous"><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e7%ab%a0-approximate-inference" rel="prev"><span class="meta-nav">&larr;</span> PRML读书会第十章  Approximate Inference</a></div>
					<div class="nav-next"><a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%ba%8c%e7%ab%a0-continuous-latent-variables" rel="next">PRML读书会第十二章 Continuous Latent Variables <span class="meta-nav">&rarr;</span></a></div>
				</div><!-- #nav-below -->

				
			<div id="comments">




								<div id="respond" class="comment-respond">
				<h3 id="reply-title" class="comment-reply-title">发表评论 <small><a rel="nofollow" id="cancel-comment-reply-link" href="/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%80%e7%ab%a0-sampling-methods#respond" style="display:none;">取消回复</a></small></h3>
									<form action="http://www.52nlp.cn/wp-comments-post.php" method="post" id="commentform" class="comment-form">
																			<p class="comment-notes">电子邮件地址不会被公开。 必填项已用<span class="required">*</span>标注</p>							<p class="comment-form-author"><label for="author">姓名 <span class="required">*</span></label> <input id="author" name="author" type="text" value="" size="30" aria-required='true' /></p>
<p class="comment-form-email"><label for="email">电子邮件 <span class="required">*</span></label> <input id="email" name="email" type="text" value="" size="30" aria-required='true' /></p>
<p class="comment-form-url"><label for="url">站点</label> <input id="url" name="url" type="text" value="" size="30" /></p>
												<p class="comment-form-comment"><label for="comment">评论</label> <textarea id="comment" name="comment" cols="45" rows="8" aria-required="true"></textarea></p>						<p class="form-allowed-tags">您可以使用这些<abbr title="HyperText Markup Language">HTML</abbr>标签和属性： <code>&lt;a href=&quot;&quot; title=&quot;&quot;&gt; &lt;abbr title=&quot;&quot;&gt; &lt;acronym title=&quot;&quot;&gt; &lt;b&gt; &lt;blockquote cite=&quot;&quot;&gt; &lt;cite&gt; &lt;code&gt; &lt;del datetime=&quot;&quot;&gt; &lt;em&gt; &lt;i&gt; &lt;q cite=&quot;&quot;&gt; &lt;strike&gt; &lt;strong&gt; </code></p>						<p class="form-submit">
							<input name="submit" type="submit" id="submit" value="发表评论" />
							<input type='hidden' name='comment_post_ID' value='7819' id='comment_post_ID' />
<input type='hidden' name='comment_parent' id='comment_parent' value='0' />
						</p>
						<p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="c380dd2246" /></p><p style="display: none;"><input type="hidden" id="ak_js" name="ak_js" value="62"/></p><p><input type="hidden" id="comment_reply_ID" name="comment_reply_ID" value="0" /><input type="hidden" id="comment_reply_dp" name="comment_reply_dp" value="0" /></p><div id="cancel_reply" style="display:none;"><a href="javascript:void(0)" onclick="movecfm(null,0,1,null);" style="color:red;">点击取消回复</a></div><script type="text/javascript">
/* <![CDATA[ */
var commentformid = "commentform";
var USERINFO = false;
var atreply = "none";
/* ]]> */
</script>
<script type="text/javascript" src="http://www.52nlp.cn/wp-content/plugins/wp-thread-comment/wp-thread-comment.js.php?jsver=common"></script>
					</form>
							</div><!-- #respond -->
			
</div><!-- #comments -->


			</div><!-- #content -->
		</div><!-- #container -->

﻿
		<div id="primary" class="widget-area" role="complementary">
			<ul class="xoxo">
<!-- begin l_sidebar -->
	<div id="l_sidebar">
<p>卓越网：<a href="http://www.amazon.cn/mn/searchApp?source=garypyang-23&searchType=1&keywords=自然语言处理" title="自然语言处理书籍"target=_blank>自然语言处理书籍</a><br>
<li id="search-3" class="widget-container widget_search"><h3 class="widget-title">站内搜索</h3><form role="search" method="get" id="searchform" class="searchform" action="http://www.52nlp.cn/">
				<div>
					<label class="screen-reader-text" for="s">搜索：</label>
					<input type="text" value="" name="s" id="s" />
					<input type="submit" id="searchsubmit" value="搜索" />
				</div>
			</form></li><li id="text-4" class="widget-container widget_text"><h3 class="widget-title">NLPJob新鲜职位推荐:</h3>			<div class="textwidget"><p></p>
<script src="http://www.nlpjob.com/api/api.php?action=getJobs
&type=0&category=0&count=8&random=1&days_behind=7&response=js" type="text/javascript"></script>

<script type="text/javascript">showJobs('jobber-container', 'jobber-list');</script></div>
		</li><li id="text-3" class="widget-container widget_text"><h3 class="widget-title">52nlp新浪微博</h3>			<div class="textwidget"><p><iframe id="sina_widget_2104931705" style="width:100%; height:500px;" frameborder="0" scrolling="no" src="http://v.t.sina.com.cn/widget/widget_blog.php?uid=2104931705&height=500&skin=wd_01&showpic=1"></iframe></p>
<p><!-- JiaThis Button BEGIN --><br />
<script type="text/javascript" src="http://v3.jiathis.com/code/jiathis_r.js?uid=1340292124103344&move=0&amp;btn=r3.gif" charset="utf-8"></script><br />
<!-- JiaThis Button END --></p>
</div>
		</li><li id="categories-309398091" class="widget-container widget_categories"><h3 class="widget-title">分类目录</h3>		<ul>
	<li class="cat-item cat-item-72"><a href="http://www.52nlp.cn/category/mit-nlp" title="麻省理工学院开放式课程&quot;自然语言处理“的相关翻译文章">MIT自然语言处理</a> (23)
</li>
	<li class="cat-item cat-item-976"><a href="http://www.52nlp.cn/category/pattern-recognition-and-machine-learning-2" >PRML</a> (15)
</li>
	<li class="cat-item cat-item-469"><a href="http://www.52nlp.cn/category/topic-model" >Topic Model</a> (10)
</li>
	<li class="cat-item cat-item-87"><a href="http://www.52nlp.cn/category/wordpress" >wordpress</a> (6)
</li>
	<li class="cat-item cat-item-317"><a href="http://www.52nlp.cn/category/%e4%b8%93%e9%a2%98" >专题</a> (6)
</li>
	<li class="cat-item cat-item-263"><a href="http://www.52nlp.cn/category/chinese-information-processing" >中文信息处理</a> (20)
</li>
	<li class="cat-item cat-item-62"><a href="http://www.52nlp.cn/category/word-segmentation" >中文分词</a> (36)
</li>
	<li class="cat-item cat-item-420"><a href="http://www.52nlp.cn/category/%e5%b9%b6%e8%a1%8c%e7%ae%97%e6%b3%95" >并行算法</a> (1)
</li>
	<li class="cat-item cat-item-268"><a href="http://www.52nlp.cn/category/%e6%8b%9b%e8%81%98" >招聘</a> (4)
</li>
	<li class="cat-item cat-item-560"><a href="http://www.52nlp.cn/category/%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f" >推荐系统</a> (3)
</li>
	<li class="cat-item cat-item-354"><a href="http://www.52nlp.cn/category/%e6%95%b0%e6%8d%ae%e6%8c%96%e6%8e%98" >数据挖掘</a> (2)
</li>
	<li class="cat-item cat-item-241"><a href="http://www.52nlp.cn/category/text-classification" >文本分类</a> (3)
</li>
	<li class="cat-item cat-item-193"><a href="http://www.52nlp.cn/category/maximum-entropy-model" >最大熵模型</a> (7)
</li>
	<li class="cat-item cat-item-344"><a href="http://www.52nlp.cn/category/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" >机器学习</a> (29)
</li>
	<li class="cat-item cat-item-1"><a href="http://www.52nlp.cn/category/machine-translation" >机器翻译</a> (54)
</li>
	<li class="cat-item cat-item-195"><a href="http://www.52nlp.cn/category/%e6%9d%a1%e4%bb%b6%e9%9a%8f%e6%9c%ba%e5%9c%ba" >条件随机场</a> (3)
</li>
	<li class="cat-item cat-item-153"><a href="http://www.52nlp.cn/category/tagging" >标注</a> (13)
</li>
	<li class="cat-item cat-item-885"><a href="http://www.52nlp.cn/category/%e7%a7%91%e5%ad%a6%e8%ae%a1%e7%ae%97" >科学计算</a> (1)
</li>
	<li class="cat-item cat-item-538"><a href="http://www.52nlp.cn/category/%e7%bb%9f%e8%ae%a1%e5%ad%a6" >统计学</a> (10)
</li>
	<li class="cat-item cat-item-126"><a href="http://www.52nlp.cn/category/translation-model" >翻译模型</a> (2)
</li>
	<li class="cat-item cat-item-51"><a href="http://www.52nlp.cn/category/nlp" >自然语言处理</a> (227)
</li>
	<li class="cat-item cat-item-106"><a href="http://www.52nlp.cn/category/computational-linguistics" >计算语言学</a> (39)
</li>
	<li class="cat-item cat-item-22"><a href="http://www.52nlp.cn/category/dictionary" >词典</a> (8)
</li>
	<li class="cat-item cat-item-221"><a href="http://www.52nlp.cn/category/semantics" >语义学</a> (1)
</li>
	<li class="cat-item cat-item-161"><a href="http://www.52nlp.cn/category/semantic-web" >语义网</a> (3)
</li>
	<li class="cat-item cat-item-37"><a href="http://www.52nlp.cn/category/corpus" >语料库</a> (12)
</li>
	<li class="cat-item cat-item-86"><a href="http://www.52nlp.cn/category/language-model" >语言模型</a> (23)
</li>
	<li class="cat-item cat-item-156"><a href="http://www.52nlp.cn/category/speech-recognition" >语音识别</a> (4)
</li>
	<li class="cat-item cat-item-314"><a href="http://www.52nlp.cn/category/%e8%b4%9d%e5%8f%b6%e6%96%af%e6%a8%a1%e5%9e%8b" >贝叶斯模型</a> (1)
</li>
	<li class="cat-item cat-item-110"><a href="http://www.52nlp.cn/category/reprint" >转载</a> (28)
</li>
	<li class="cat-item cat-item-451"><a href="http://www.52nlp.cn/category/%e9%97%ae%e7%ad%94%e7%b3%bb%e7%bb%9f" >问答系统</a> (1)
</li>
	<li class="cat-item cat-item-3"><a href="http://www.52nlp.cn/category/informal-essay" >随笔</a> (63)
</li>
	<li class="cat-item cat-item-60"><a href="http://www.52nlp.cn/category/hidden-markov-model" >隐马尔科夫模型</a> (36)
</li>
		</ul>
</li><li id="archives-2" class="widget-container widget_archive"><h3 class="widget-title">文章归档</h3>		<ul>
	<li><a href='http://www.52nlp.cn/2015/01'>2015年一月</a></li>
	<li><a href='http://www.52nlp.cn/2014/12'>2014年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2014/11'>2014年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2014/09'>2014年九月</a></li>
	<li><a href='http://www.52nlp.cn/2014/07'>2014年七月</a></li>
	<li><a href='http://www.52nlp.cn/2014/06'>2014年六月</a></li>
	<li><a href='http://www.52nlp.cn/2014/05'>2014年五月</a></li>
	<li><a href='http://www.52nlp.cn/2014/04'>2014年四月</a></li>
	<li><a href='http://www.52nlp.cn/2014/01'>2014年一月</a></li>
	<li><a href='http://www.52nlp.cn/2013/12'>2013年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2013/06'>2013年六月</a></li>
	<li><a href='http://www.52nlp.cn/2013/05'>2013年五月</a></li>
	<li><a href='http://www.52nlp.cn/2013/04'>2013年四月</a></li>
	<li><a href='http://www.52nlp.cn/2013/03'>2013年三月</a></li>
	<li><a href='http://www.52nlp.cn/2013/02'>2013年二月</a></li>
	<li><a href='http://www.52nlp.cn/2013/01'>2013年一月</a></li>
	<li><a href='http://www.52nlp.cn/2012/12'>2012年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2012/11'>2012年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2012/10'>2012年十月</a></li>
	<li><a href='http://www.52nlp.cn/2012/09'>2012年九月</a></li>
	<li><a href='http://www.52nlp.cn/2012/08'>2012年八月</a></li>
	<li><a href='http://www.52nlp.cn/2012/07'>2012年七月</a></li>
	<li><a href='http://www.52nlp.cn/2012/06'>2012年六月</a></li>
	<li><a href='http://www.52nlp.cn/2012/05'>2012年五月</a></li>
	<li><a href='http://www.52nlp.cn/2012/04'>2012年四月</a></li>
	<li><a href='http://www.52nlp.cn/2012/03'>2012年三月</a></li>
	<li><a href='http://www.52nlp.cn/2012/01'>2012年一月</a></li>
	<li><a href='http://www.52nlp.cn/2011/12'>2011年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2011/11'>2011年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2011/10'>2011年十月</a></li>
	<li><a href='http://www.52nlp.cn/2011/09'>2011年九月</a></li>
	<li><a href='http://www.52nlp.cn/2011/08'>2011年八月</a></li>
	<li><a href='http://www.52nlp.cn/2011/07'>2011年七月</a></li>
	<li><a href='http://www.52nlp.cn/2011/06'>2011年六月</a></li>
	<li><a href='http://www.52nlp.cn/2011/05'>2011年五月</a></li>
	<li><a href='http://www.52nlp.cn/2011/04'>2011年四月</a></li>
	<li><a href='http://www.52nlp.cn/2011/03'>2011年三月</a></li>
	<li><a href='http://www.52nlp.cn/2011/02'>2011年二月</a></li>
	<li><a href='http://www.52nlp.cn/2011/01'>2011年一月</a></li>
	<li><a href='http://www.52nlp.cn/2010/12'>2010年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2010/11'>2010年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2010/10'>2010年十月</a></li>
	<li><a href='http://www.52nlp.cn/2010/09'>2010年九月</a></li>
	<li><a href='http://www.52nlp.cn/2010/08'>2010年八月</a></li>
	<li><a href='http://www.52nlp.cn/2010/07'>2010年七月</a></li>
	<li><a href='http://www.52nlp.cn/2010/06'>2010年六月</a></li>
	<li><a href='http://www.52nlp.cn/2010/05'>2010年五月</a></li>
	<li><a href='http://www.52nlp.cn/2010/04'>2010年四月</a></li>
	<li><a href='http://www.52nlp.cn/2010/03'>2010年三月</a></li>
	<li><a href='http://www.52nlp.cn/2010/02'>2010年二月</a></li>
	<li><a href='http://www.52nlp.cn/2010/01'>2010年一月</a></li>
	<li><a href='http://www.52nlp.cn/2009/12'>2009年十二月</a></li>
	<li><a href='http://www.52nlp.cn/2009/11'>2009年十一月</a></li>
	<li><a href='http://www.52nlp.cn/2009/10'>2009年十月</a></li>
	<li><a href='http://www.52nlp.cn/2009/09'>2009年九月</a></li>
	<li><a href='http://www.52nlp.cn/2009/08'>2009年八月</a></li>
	<li><a href='http://www.52nlp.cn/2009/07'>2009年七月</a></li>
	<li><a href='http://www.52nlp.cn/2009/06'>2009年六月</a></li>
	<li><a href='http://www.52nlp.cn/2009/05'>2009年五月</a></li>
	<li><a href='http://www.52nlp.cn/2009/04'>2009年四月</a></li>
	<li><a href='http://www.52nlp.cn/2009/03'>2009年三月</a></li>
	<li><a href='http://www.52nlp.cn/2009/02'>2009年二月</a></li>
	<li><a href='http://www.52nlp.cn/2009/01'>2009年一月</a></li>
	<li><a href='http://www.52nlp.cn/2008/12'>2008年十二月</a></li>
		</ul>
</li>		<li id="recent-posts-2" class="widget-container widget_recent_entries">		<h3 class="widget-title">最新文章</h3>		<ul>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e5%9b%9b%e7%ab%a0-combining-models">PRML读书会第十四章 Combining Models</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%89%e7%ab%a0sequential-data">PRML读书会第十三章 Sequential Data</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%ba%8c%e7%ab%a0-continuous-latent-variables">PRML读书会第十二章 Continuous Latent Variables</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%80%e7%ab%a0-sampling-methods">PRML读书会第十一章  Sampling Methods</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e7%ab%a0-approximate-inference">PRML读书会第十章  Approximate Inference</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b9%9d%e7%ab%a0-mixture-models-and-em">PRML读书会第九章  Mixture Models and EM</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ab%e7%ab%a0-graphical-models">PRML读书会第八章  Graphical Models</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%83%e7%ab%a0-sparse-kernel-machines">PRML读书会第七章 Sparse Kernel Machines</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ad%e7%ab%a0-kernel-methods">PRML读书会第六章   Kernel Methods</a>
						</li>
					<li>
				<a href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%ba%94%e7%ab%a0-neural-networks">PRML读书会第五章  Neural Networks</a>
						</li>
				</ul>
		</li><li id="recentcomments" class="widget-container widget_recentcomments"><h3 class="widget-title">最近评论</h3><ul><li class="rc-navi rc-clearfix"><span class="rc-loading">正在加载...</span></li><li id="rc-comment-temp" class="rc-item rc-comment rc-clearfix"><div class="rc-info"></div><div class="rc-timestamp"></div><div class="rc-excerpt"></div></li><li id="rc-ping-temp" class="rc-item rc-ping rc-clearfix"><span class="rc-label"></span></li></ul></li>			</ul>
		</div><!-- #primary .widget-area -->


		<div id="secondary" class="widget-area" role="complementary">
			<ul class="xoxo">
				<li id="linkcat-103" class="widget-container widget_links"><h3 class="widget-title">NLP相关网站</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://www.aclweb.org/" rel="co-worker" title="The Association for Computational Linguistics" target="_blank">ACL</a></li>
<li><a href="http://aclweb.org/anthology-new/" rel="co-worker" title="A Digital Archive of Research Papers in Computational Linguistics" target="_blank">ACL Anthology</a></li>
<li><a href="http://belobog.si.umich.edu/clair/anthology/index.cgi" rel="colleague" target="_blank">ACL Anthology Network</a></li>
<li><a href="http://aclweb.org/aclwiki/index.php?title=Main_Page" rel="colleague" title="the Wiki of the Association for Computational Linguistics" target="_blank">ACL Wiki</a></li>
<li><a href="http://www.clsp.jhu.edu/" rel="colleague" target="_blank">CLSP</a></li>
<li><a href="http://www.cwbbase.com/" rel="colleague" title="这是一个略具规模的中文语义词库, 也是稍有特色的汉语语义词典" target="_blank">CWB中文词库</a></li>
<li><a href="http://www.euromatrix.net/" rel="colleague" target="_blank">EuroMatrix</a></li>
<li><a href="http://www.freebase.com" rel="colleague" target="_blank">Freebase</a></li>
<li><a href="http://www.clsp.jhu.edu/workshops/" rel="colleague" target="_blank">JHU Workshop</a></li>
<li><a href="http://www.ldc.upenn.edu/" rel="colleague" title="Linguistic Data Consortium" target="_blank">LDC</a></li>
<li><a href="http://www.statmt.org/moses/" rel="colleague" title="A factored phrase-based beam-search decoder for machine translation" target="_blank">Moses</a></li>
<li><a href="http://nlpers.blogspot.com/" rel="colleague" title="国外一个非常不错的自然语言处理博客" target="_blank">nlper</a></li>
<li><a href="http://www.nlpjob.com" target="_blank">NLPJob</a></li>
<li><a href="http://www.powerset.com/" rel="colleague" target="_blank">Powerset</a></li>
<li><a href="http://www.speech.sri.com/projects/srilm/" rel="colleague" title="- The SRI Language Modeling Toolkit" target="_blank">SRILM</a></li>
<li><a href="http://www.statmt.org/" rel="colleague" title="This website is dedicated to research in statistical machine translation" target="_blank">Statistical Machine Translation</a></li>
<li><a href="http://textanalysisonline.com/" target="_blank">Text Analysis</a></li>
<li><a href="http://textminingonline.com/" target="_blank">Text Mining</a></li>
<li><a href="http://textsummarization.net/" target="_blank">Text Summarization</a></li>
<li><a href="http://w3china.org/index.htm" rel="friend" title="致力于促进W3C技术的广泛应用, 传播关于未来Web的知识与技术" target="_blank">中国万维网联盟</a></li>
<li><a href="http://www.cipsc.org.cn/" rel="co-worker" title="Chinese Information Processing Society of China" target="_blank">中国中文信息学会</a></li>
<li><a href="http://www.nlp.org.cn/" rel="colleague" title="中文自然语言处理开放平台" target="_blank">中文自然语言处理开放平台</a></li>
<li><a href="http://www.mt-archive.info/" rel="colleague" title="Repository and bibliography of articles, books and papers on topics" target="_blank">机器翻译档案计划</a></li>
<li><a href="http://www.statmt.org/europarl/" rel="colleague" target="_blank">欧洲议会平行语料库</a></li>
<li><a href="http://www.keenage.com/" title="HowNet" target="_blank">知网</a></li>
<li><a href="http://www.nlpir.org/" rel="friend" title="由张华平博士发起，由北京理工大学网络搜索与挖掘实验室运营，旨在推动NLP(自然语言处理)与IR(信息检索)领域的共享与共赢" target="_blank">自然语言处理与信息检索共享平台</a></li>
<li><a href="http://mitel.ict.ac.cn/" rel="co-worker" title="中科院计算所多语言交互技术实验室" target="_blank">计算所多语言交互技术实验室</a></li>

	</ul>
</li>
<li id="linkcat-2" class="widget-container widget_links"><h3 class="widget-title">友情链接</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://blog.youxu.info/" title="一个计算机专业的 Ph.D. 学生徐宥的个人博客" target="_blank">4G spaces</a></li>
<li><a href="http://blog.52nlp.org" rel="me" title="我爱自然语言处理完全镜像" target="_blank">52nlpblog</a></li>
<li><a href="http://www.52nlp.com" rel="me" title="52nlp的英文站" target="_blank">52nlpcom</a></li>
<li><a href="http://hi.baidu.com/drkevinzhang" rel="friend" title="ICTCLAS 张华平博士的空间" target="_blank">ICTCLAS 张华平博士的空间</a></li>
<li><a href="http://blog.so8848.com/" rel="friend" title="信息检索博客" target="_blank">Information Retrieval Blog</a></li>
<li><a href="http://interop123.com/default.aspx" rel="friend" title="崔晓源师兄关于NET技术的站点" target="_blank">NET互操作技术社区</a></li>
<li><a href="http://bbs.w3china.org/" rel="friend" title="中国万维网联盟讨论区" target="_blank">W3CHINA讨论区</a></li>
<li><a href="http://www.ailab.cn/" rel="friend" target="_blank">人工智能网</a></li>
<li><a href="http://mindhacks.cn/" rel="friend" title="一个很有思想的价值博客!" target="_blank">刘未鹏之Mind Hacks</a></li>
<li><a href="http://www.cnblogs.com/finallyliuyu/" rel="friend" target="_blank">原地转圈的驴子</a></li>
<li><a href="http://xunren.thuir.org/" target="_blank">微博寻人（梁博）</a></li>
<li><a href="http://52opencourse.com" rel="friend" title="我爱公开课，高质量公开课交流平台" target="_blank">我爱公开课</a></li>
<li><a href="http://iregex.org/" rel="friend" target="_blank">我爱正则表达式</a></li>
<li><a href="http://courseminer.com" target="_blank">挖课</a></li>
<li><a href="http://www.flickering.cn/" target="_blank">火光摇曳</a></li>
<li><a href="http://www.sciencenet.cn/u/timy/" rel="friend" title="章成志老师的博客" target="_blank">章成志的博客</a></li>
<li><a href="http://blog.csdn.net/v_JULY_v/" target="_blank">结构之法 算法之道</a></li>
<li><a href="http://www.lingcc.com/" rel="friend" title="关注编译器,虚拟机,编程语言及技术,IT职业和程序员生活" target="_blank">编译点滴</a></li>
<li><a href="http://www.52nlp.org" rel="me" title="52nlp的官方网站" target="_blank">自然语言处理</a></li>
<li><a href="http://www.ieee.org.cn/" rel="friend" title="计算机科学论坛" target="_blank">计算机科学论坛</a></li>
<li><a href="http://coursegraph.com/">课程图谱</a></li>
<li><a href="http://blog.coursegraph.com" rel="friend">课程图谱博客</a></li>

	</ul>
</li>
<li id="meta-4" class="widget-container widget_meta"><h3 class="widget-title">功能</h3>			<ul>
						<li><a href="http://www.52nlp.cn/wp-login.php">登录</a></li>
			<li><a href="http://www.52nlp.cn/feed">文章<abbr title="Really Simple Syndication">RSS</abbr></a></li>
			<li><a href="http://www.52nlp.cn/comments/feed">评论<abbr title="Really Simple Syndication">RSS</abbr></a></li>
<li><a href="https://cn.wordpress.org/" title="基于WordPress，一个优美、先进的个人信息发布平台。">WordPress.org</a></li>			</ul>
</li>			</ul>
		</div><!-- #secondary .widget-area -->

	</div><!-- #main -->

	<div id="footer" role="contentinfo">
		<div id="colophon">



			<div id="site-info">
				<a href="http://www.52nlp.cn/" title="我爱自然语言处理" rel="home">
					我爱自然语言处理				</a>
			</div><!-- #site-info -->

			<div id="site-generator">
								<a href="http://cn.wordpress.org/"
						title="优雅的个人发布平台" rel="generator">
					自豪地采用 WordPress。				</a>
			</div><!-- #site-generator -->

		</div><!-- #colophon -->
	</div><!-- #footer -->

</div><!-- #wrapper -->

<script>
/* <![CDATA[ */
var rcGlobal = {
	serverUrl		:'http://www.52nlp.cn',
	infoTemp		:'%REVIEWER% 在 %POST%',
	loadingText		:'正在加载',
	noCommentsText	:'没有任何评论',
	newestText		:'&laquo; 最新的',
	newerText		:'&laquo; 上一页',
	olderText		:'下一页 &raquo;',
	showContent		:'',
	external		:'',
	avatarSize		:'0',
	avatarPosition	:'left',
	anonymous		:'匿名'
};
/* ]]> */
</script>
<script type='text/javascript' src='http://www.52nlp.cn/wp-content/plugins/akismet/_inc/form.js?ver=3.0.4'></script>
<link rel='stylesheet' id='yarppRelatedCss-css'  href='http://www.52nlp.cn/wp-content/plugins/yet-another-related-posts-plugin/style/related.css?ver=4.0.1' type='text/css' media='all' />
<script type='text/javascript' src='http://www.52nlp.cn/wp-content/plugins/wp-recentcomments/js/wp-recentcomments.js?ver=2.2.7'></script>
	<p align="center"> 本站架设在 <a href="http://www.52nlp.cn/digitalocean%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0">DigitalOcean</a> 上, 采用创作共用版权协议, 要求署名、非商业用途和保持一致. 转载本站内容必须也遵循“署名-非商业用途-保持一致”的创作共用协议.</p>
<!-- Piwik -->
<script type="text/javascript">
  var _paq = _paq || [];
  _paq.push(["trackPageView"]);
  _paq.push(["enableLinkTracking"]);

  (function() {
    var u=(("https:" == document.location.protocol) ? "https" : "http") + "://162.243.252.121/piwik/";
    _paq.push(["setTrackerUrl", u+"piwik.php"]);
    _paq.push(["setSiteId", "5"]);
    var d=document, g=d.createElement("script"), s=d.getElementsByTagName("script")[0]; g.type="text/javascript";
    g.defer=true; g.async=true; g.src=u+"piwik.js"; s.parentNode.insertBefore(g,s);
  })();
</script>
<!-- End Piwik Code -->
</body>
</html>
