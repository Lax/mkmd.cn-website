<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>perf Hacktogram</title>
        <meta name="viewport" content="width=device-width">

        <!-- syntax highlighting CSS -->
        <link rel="stylesheet" href="/blog/css/syntax.css">

        <!-- Custom CSS -->
        <link rel="stylesheet" href="/blog/css/main.css">

<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-7747513-3']);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

    </head>
    <body>

	<div class="nav">
	<p class="navhdr">This Site:</p>
<a href="/index.html">Homepage</a><br>
<a href="/blog/index.html">Blog</a><br>
<a href="/sitemap.html">Full Site Map</a><br>
<a href="/sysperfbook.html">Sys Perf book</a><br>
<a href="/linuxperf.html">Linux Perf</a><br>
<a href="/methodology.html">Perf Methods</a><br>
<a href="/usemethod.html">USE Method</a><br>
<a href="/tsamethod.html">TSA Method</a><br>
<a href="/offcpuanalysis.html">Off-CPU Analysis</a><br>
<a href="/activebenchmarking.html">Active Bench.</a><br>
<a href="/flamegraphs.html">Flame Graphs</a><br>
<a href="/heatmaps.html">Heat Maps</a><br>
<a href="/frequencytrails.html">Frequency Trails</a><br>
<a href="/colonygraphs.html">Colony Graphs</a><br>
<a href="/perf.html">perf Examples</a><br>
<a href="/ktap.html">ktap Examples</a><br>
<a href="/dtrace.html">DTrace Tools</a><br>
<a href="/dtracetoolkit.html">DTraceToolkit</a><br>
<a href="/dtkshdemos.html">DtkshDemos</a><br>
<a href="/guessinggame.html">Guessing Game</a><br>
<a href="/specials.html">Specials</a><br>
<a href="/books.html">Books</a><br>
<a href="/sites.html">Other Sites</a><br>

	</div>

	<div class="recent">
	Recent posts:<br>
	<ul style="padding-left:18px">
	  
		   <li>30 Aug 2014 &raquo;<br>
		   <a href="/blog/2014-08-30/ftrace-the-hidden-light-switch.html">  
		   ftrace: The Hidden Light Switch</a></li>
	  
		   <li>23 Aug 2014 &raquo;<br>
		   <a href="/blog/2014-08-23/linux-perf-tools-linuxcon-na-2014.html">  
		   Linux Performance Tools at LinuxCon North America 2014</a></li>
	  
		   <li>28 Jul 2014 &raquo;<br>
		   <a href="/blog/2014-07-28/execsnoop-for-linux.html">  
		   execsnoop For Linux: See Short-Lived Processes</a></li>
	  
		   <li>25 Jul 2014 &raquo;<br>
		   <a href="/blog/2014-07-25/opensnoop-for-linux.html">  
		   opensnoop For Linux</a></li>
	  
		   <li>23 Jul 2014 &raquo;<br>
		   <a href="/blog/2014-07-23/linux-iosnoop-latency-heat-maps.html">  
		   Linux iosnoop Latency Heat Maps</a></li>
	  
		   <li>16 Jul 2014 &raquo;<br>
		   <a href="/blog/2014-07-16/iosnoop-for-linux.html">  
		   iosnoop For Linux</a></li>
	  
		   <li>13 Jul 2014 &raquo;<br>
		   <a href="/blog/2014-07-13/linux-ftrace-function-counting.html">  
		   Linux ftrace Function Counting</a></li>
	  
		   <li>10 Jul 2014 &raquo;<br>
		   <a href="/blog/2014-07-10/perf-hacktogram.html">  
		   perf Hacktogram</a></li>
	  
		   <li>03 Jul 2014 &raquo;<br>
		   <a href="/blog/2014-07-03/perf-counting.html">  
		   perf Counting</a></li>
	  
		   <li>01 Jul 2014 &raquo;<br>
		   <a href="/blog/2014-07-01/perf-heat-maps.html">  
		   perf Heat Maps</a></li>
	  
		   <li>29 Jun 2014 &raquo;<br>
		   <a href="/blog/2014-06-29/perf-static-tracepoints.html">  
		   perf Static Tracepoints</a></li>
	  
		   <li>22 Jun 2014 &raquo;<br>
		   <a href="/blog/2014-06-22/perf-cpu-sample.html">  
		   perf CPU Sampling</a></li>
	  
		   <li>12 Jun 2014 &raquo;<br>
		   <a href="/blog/2014-06-12/java-flame-graphs.html">  
		   Java Flame Graphs</a></li>
	  
		   <li>09 Jun 2014 &raquo;<br>
		   <a href="/blog/2014-06-09/java-cpu-sampling-using-hprof.html">  
		   Java CPU Sampling Using hprof</a></li>
	  
		   <li>23 May 2014 &raquo;<br>
		   <a href="/blog/2014-05-23/osx-10.9.3-is-toxic.html">  
		   OS X 10.9.3 Recurring Panics</a></li>
	  
		   <li>17 May 2014 &raquo;<br>
		   <a href="/blog/2014-05-17/free-as-in-we-own-your-ip.html">  
		   Free, as in, We Own Your IP</a></li>
	  
		   <li>16 May 2014 &raquo;<br>
		   <a href="/blog/2014-05-16/LISA13-metrics-workshop.html">  
		   LISA13 Metrics Workshop</a></li>
	  
		   <li>11 May 2014 &raquo;<br>
		   <a href="/blog/2014-05-11/strace-wow-much-syscall.html">  
		   strace Wow Much Syscall</a></li>
	  
		   <li>09 May 2014 &raquo;<br>
		   <a href="/blog/2014-05-09/xen-feature-detection.html">  
		   Xen Feature Detection</a></li>
	  
		   <li>07 May 2014 &raquo;<br>
		   <a href="/blog/2014-05-07/what-color-is-your-xen.html">  
		   What Color Is Your Xen?</a></li>
	  
	</ul>
	<a href="/blog/index.html">Blog index</a><br>
	<a href="/blog/about.html">About</a><br>
	<a href="/blog/rss.xml">RSS</a><br>
	</div>

        <div class="site">
          <div class="header">
            <h1 class="title"><a href="/blog/index.html">Brendan Gregg's Blog</a></h1>
            <a class="extra" href="/blog/index.html">home</a>
          </div>

          <h2 class="big">perf Hacktogram</h2>
<p class="meta">10 Jul 2014</p>

<div class="post">
<p>What is the distribution of sent packet sizes for my Linux system?</p>

<pre>
# <b>./perf-stat-hist net:net_dev_xmit len 10</b>
Tracing net:net_dev_xmit, power-of-4, max 16384, for 10 seconds...

            Range          : Count    Distribution
            0              : 0        |                                      |
            1 -> 3         : 0        |                                      |
            4 -> 15        : 0        |                                      |
           16 -> 63        : 6        |#                                     |
           64 -> 255       : 385      |######################################|
          256 -> 1023      : 133      |##############                        |
         1024 -> 4095      : 155      |################                      |
         4096 -> 16383     : 0        |                                      |
        16384 ->           : 0        |                                      |
</pre>

<p>Great! So about half are between 64 and 255 bytes, and the rest are between 256 and 4095 bytes.</p>

<p>How about the requested size of read() syscalls?</p>

<pre>
# <b>time ./perf-stat-hist syscalls:sys_enter_read count 10</b>
Tracing syscalls:sys_enter_read, power-of-4, max 1048576, for 10 seconds...

            Range          : Count    Distribution
            0              : 0        |                                      |
            1 -> 3         : 1361     |#                                     |
            4 -> 15        : 2        |#                                     |
           16 -> 63        : 8        |#                                     |
           64 -> 255       : 60       |#                                     |
          256 -> 1023      : 1933859  |######################################|
         1024 -> 4095      : 59       |#                                     |
         4096 -> 16383     : 146      |#                                     |
        16384 -> 65535     : 21       |#                                     |
        65536 -> 262143    : 554007   |###########                           |
       262144 -> 1048575   : 0        |                                      |
      1048576 ->           : 0        |                                      |

real    0m10.056s
user    0m0.012s
sys     0m0.008s
</pre>

<p>Neat! The most common are in the 256 - 1023 byte range.</p>

<p>This time I added a <tt>time</tt> command, to show that extracting this information from the kernel cost little.</p>

<p>The script I&#39;m using, <a href="https://github.com/brendangregg/perf-tools/blob/master/misc/perf-stat-hist">perf-stat-hist</a>, is demonstrating a custom distribution capability that is bread-and-butter for more advanced tracers like SystemTap, ktap, and DTrace. However, I&#39;m not using those.</p>

<p>I&#39;m using Linux perf_events on the 3.2 kernel. Aka, the <tt>perf</tt> command.</p>

<p>To do <strong>in-kernel histograms</strong>.</p>

<p>Stock, standard, perf_events.</p>

<h2>Via user-space</h2>

<p>Yes, for the current version of perf_events (3.16 and earlier) this is supposed to be impossible. perf_events can do in-kernel tracepoint counts, but anything beyond that requires dumping data to user-space for post-processing, like this:</p>

<pre>
# <b>perf record -e 'syscalls:sys_enter_read' -a sleep 5</b>
[ perf record: Woken up 25 times to write data ]
[ perf record: Captured and wrote 132.355 MB perf.data (~5782677 samples) ]
</pre>

<p>Now I have two problems. This perf.data file has over 5 million entries, which will cost some CPU to process. How much CPU just to read it? Lets dump it using <tt>perf</tt> <tt>script</tt> to /dev/null:</p>

<pre>
window1# <b>time perf script > /dev/null</b>
<i>...hang...</i>
window2# <b>top</b>
<i>...hang...</i>
</pre>

<p>Both windows have frozen. Now I have four problems.</p>

<p>When top finally runs, I can see what&#39;s wrong:</p>

<pre>
# <b>top</b>
top - 23:21:58 up 25 days,  2:56,  2 users,  load average: 1.68, 1.42, 1.09
Tasks: 142 total,   2 running, 136 sleeping,   0 stopped,   4 zombie
Cpu(s): 18.9%us, 54.1%sy,  0.0%ni, 27.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Mem:   3839240k total,  3813020k used,    26220k free,      448k buffers
Swap:        0k total,        0k used,        0k free,   167712k cached

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
27328 root      20   0 3437m <b>3.3g</b>  68m R  100 88.8   0:06.02 <b>perf</b>
   34 root      20   0     0    0    0 S   26  0.0   0:45.04 kswapd0   
26289 root      20   0     0    0    0 S   10  0.0   0:00.04 kworker/u4:0
26290 root      20   0 17344  712  340 R    5  0.0   0:00.90 top        
[...]
</pre>

<p><tt>perf</tt> has not only eaten one CPU, but also exhausted the memory on this system.</p>

<p>perf_events <em>does</em> have an excellent architecture for passing data from the kernel to user-level programs, minimizing overhead and CPU costs. What I&#39;m testing is an extreme case. In many other cases, a <tt>perf report</tt> and <tt>perf</tt> <tt>script</tt> cycle will work fine, and the overhead will be negligible. A perf-stat-hist script using that cycle would merely bucketize the <tt>perf</tt> <tt>script</tt> output and print a report: a trivial program.</p>

<p>I could reduce overheads further by reading the binary perf.data file directly, or even better, calling perf_event_open() and mmap() to read the binary buffer, and process data without a trip via the file system. But there&#39;s also another way...</p>

<h2>The hacktogram</h2>

<p>This is based on <tt>perf</tt> <tt>stat</tt>, which does efficient in-kernel counts. I gave a quick tour of basic <a href="http://www.brendangregg.com/blog/2014-07-03/perf-counting.html">perf Counting</a> capabilities in my previous post.</p>

<p><tt>perf</tt> <tt>stat</tt> lets you instrument the same tracepoint multiple times, with different filters. The trick is to use a tracepoint and filter pair for each histogram bucket. For example:</p>

<pre>
# <b>perf stat -e syscalls:sys_enter_read --filter 'count < 1024' \
    -e syscalls:sys_enter_read --filter 'count >= 1024 && count < 1048576' \
    -e syscalls:sys_enter_read --filter 'count > 1048576' -a sleep 5</b>

 Performance counter stats for 'system wide':

         1,522,160      syscalls:sys_enter_read                      [100.00%]
           401,805      syscalls:sys_enter_read                      [100.00%]
                18      syscalls:sys_enter_read                     

       5.001822069 seconds time elapsed
</pre>

<p>This shows that there were 1,522,160 read() syscalls requesting less than 1 Kbyte, 401,805 requesting between 1 Kbyte and 1 Mbyte, and 18 requesting over 1 Mbyte.</p>

<p>That&#39;s the approach I used in perf-stat-hist. Tracing the same tracepoint multiple times <em>does</em> incur additional overhead, so this approach is not ideal, and can slow my target by up to 50% when using over a dozen tracepoints (buckets). It&#39;s a hack.</p>

<p>As for the variable I&#39;m using, in this case &quot;count&quot;: those come from the tracepoint. See the end of my previous post on <a href="http://www.brendangregg.com/blog/2014-07-03/perf-counting.html">perf Counting</a>, and the contents of the /sys/.../format file.</p>

<h2>Ideal</h2>

<p>What would be ideal is for <tt>perf</tt> <tt>stat</tt> to provide a histogram option. Eg:</p>

<pre>
# <b>perf stat -e syscalls:sys_enter_read --hist "pow2 count"</b>
</pre>

<p>For a power-of-2 histogram of the count variable.</p>

<p>I think it&#39;s likely perf_events will get this capability in the future, especially thanks to recent kernel developments (more on this soon). So my perf-stat-hist workaround has a limited lifespan.</p>

<p>For more on perf_events, see my <a href="/perf.html">perf_events examples</a> page and the <a href="https://perf.wiki.kernel.org/index.php/Main_Page"</a>perf_events wiki</a>.</p>

</div>



<br><hr>
<div id="disqus_thread"></div>
<script type="text/javascript">
var disqus_shortname = 'brendangregg';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


          <div class="footer">
            <div class="contact">
                Copyright 2014 Brendan Gregg.<br><a href="/blog/about.html">About this blog</a>
            </div>
          </div>
        </div>

    </body>
</html>
